\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{indentfirst}
\usepackage{enumitem}
\usepackage{microtype}
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}

\setlist{parsep=0pt,listparindent=\parindent}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcounter{lemmaCounter}
\newcounter{theoremCounter}
\newcounter{definitionCounter}

\newenvironment{shortlemma}{\refstepcounter{lemmaCounter}
\noindent\textbf{Lemma~\thelemmaCounter.}\em}

\newenvironment{shorttheorem}{\refstepcounter{theoremCounter}
\noindent\textbf{Theorem~\thetheoremCounter.}\em}

\newenvironment{shortdefinition}{\refstepcounter{definitionCounter}
\noindent\textbf{Definition~\thedefinitionCounter.}\em}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\makeatletter
\let\oldabs\abs
\let\oldceil\ceil 
\let\oldfloor\floor
\def\abs{\@ifstar{\oldabs}{\oldabs*}}
\def\ceil{\@ifstar{\oldceil}{\oldceil*}}
\def\floor{\@ifstar{\oldfloor}{\oldfloor*}}
\makeatother

\newcommand{\N}{\mathbf{N}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\paren}[1]{\left( #1 \right)}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\exc}[2][Abbott]{\item \textbf{#1, Exercise #2.}}
\newcommand{\lep}[1][L]{#1et $\epsilon > 0$ be arbitrary}
\newcommand{\seq}[1]{\left(#1\right)_{n=1}^\infty}
\newcommand{\LIM}[1]{\text{LIM}_{n \to \infty} #1}

\let\oldlog\log
\let\oldmax\max
\let\oldmin\min
\let\oldsin\sin
\let\oldcos\cos
\renewcommand{\log}[1]{\oldlog \left( #1 \right)}
\renewcommand{\max}[1]{\oldmax \left( #1 \right)}
\renewcommand{\min}[1]{\oldmin \left( #1 \right)}
\renewcommand{\sin}[1]{\oldsin \left( #1 \right)}
\renewcommand{\cos}[1]{\oldcos \left( #1 \right)}


\title{Real Analysis Exercises}
\author{Eduardo Freire}
\date{April 2021}

\begin{document}

\maketitle

\begin{enumerate}
	\exc[Tao]{5.4.1} For every real
	number $x$, exactly one of the following three statements is true: $(a)\, x$ is zero; $(b)\, x$ is positive; $(c)\, x$ is negative.
				      	      
	\begin{proof}
		First we show that at least one of $a$, $b$ or $c$ is true. Let $x$ be an arbitrary real number. If $x=0$ we are done. Otherwise, we need to show that either $b$ or $c$ is true. Since $x \neq 0$, it can be written as $\text{LIM}_{n \to \infty} a_n$ where $(a_n)_{n=1}^{\infty}$ is a Cauchy sequence that is bounded away from zero. Then, there is some $c > 0$ such that $|a_n| \geq c$ for all $n$. Also, there is some $N \geq 1$ such that $|a_N - a_n| \leq c/2$ for all $n \geq N$, since the sequence is Cauchy, $c/2 > 0$ and $N \geq N$. Since the sequence is bounded away from zero, none of its terms are zero. Therefore we can split the problem in two cases, $a_N > 0$ and $a_N < 0$.
							      		      	 
		Case 1 ($a_N > 0$): If we can show that $a_n \geq c/2 > 0$ for all $n \geq N$ we would almost be done, since we could then define a new sequence $(b_n)_{n=1}^\infty$ where $b_n := c/2$ if $n < N$ and $b_n := a_n$ if $n \geq N$, which is clearly positively bounded away from zero and equivalent to $(a_n)_{n=1}^{\infty}$. So, assume for the sake of contradiction that $a_n < c/2$ for some $n \geq N$. Then, $-a_n > -c/2$, therefore $a_N - a_n > a_N-c/2 \geq c/2 > 0$. But then $|a_N - a_n| = a_N - a_n \leq c/2$. Thus we have show that $c/2 < a_N-a_n \leq c/2$, a contradiction. This means that $a_n \geq c/2$ for all $n \geq N$, and we are done.
							      		      	 
		Case 2 ($a_N < 0$): Similarly to case one, we assume for the sake of contradiction that $a_n > -c/2$. Since $-a_N \geq c$, $a_n - a_N > c/2 > 0$. But then $|a_n - a_N| = a_n - a_N \leq c/2$, so we have show that $c/2 < a_n-a_N \leq c/2$, a contradiction. Therefore, for all $n \geq N$, $a_n \leq -c/2$. Now we can define a new sequence $(b_n)_{n=1}^\infty$ where $b_n := -c/2$ if $n < N$ and $b_n := a_n$ if $n \geq N$, which is clearly negatively bounded away from zero and equivalent to $(a_n)_{n=1}^{\infty}$.
							      		      	  
		Now, we show that at most one of $a$, $b$ or $c$ must be true. We do that by contradiction, in three separate cases.
							      		      	  
		Case 1 ($a$ and $b$ are true): Since x is positive, it can be written as $x = \text{LIM}_{n \to \infty} a_n$ where $(a_n)_{n=1}^{\infty}$ is positively bounded away from zero. In other words, there is some $c > 0$ such that $a_n \geq c$ for all $n \geq 1$. But since $x = 0$, $(a_n)_{n=1}^{\infty}$ is equivalent to zero, which means that there is some $N \geq 1$ such that $|a_N| = a_N \leq c/2 < c$, a contradiction.
							      		      	  
		Case 2 ($a$ and $c$ are true): Since x is negative, it can be written as $x = \text{LIM}_{n \to \infty} a_n$ where $(a_n)_{n=1}^{\infty}$ is negatively bounded away from zero. In other words, there is some $-c < 0$ such that $a_n \leq -c$ for all $n \geq 1$. But since $x = 0$, $(a_n)_{n=1}^{\infty}$ is equivalent to zero, which means that there is some $N \geq 1$ such that $|a_N| = -a_N \leq c/2$. But then $a_N \geq -c/2 > -c$, a contradiction.
							      		      	
		Case 3 ($b$ and $c$ are true): Since $x$ is both positive and negative, we have that $x = \text{LIM}_{n \to \infty} a_n = \text{LIM}_{n \to \infty} b_n$, where $(a_n)_{n=1}^{\infty}$ is positively bounded away from zero and $(b_n)_{n=1}^{\infty}$ is negatively bounded away from zero. This means that there there exists some $c_1, c_2 > 0$ such that $a_n \geq c_1$ and $-b_m \geq c_2$ for all $n,m \geq 1$. Then $a_n - b_m \geq c_1 + c_2 > 0$. However, since the sequences are equivalent, there is some $N \geq 1$ such that
		\begin{equation*}
			|a_N - b_N| = a_N - b_N \leq \frac{c_1+c_2}{2} < c_1 + c_2
			\, .
		\end{equation*}
							      		      	
		This is a contradiction, since we have already shown that $a_N - b_N \geq c_1 + c_2$ 
							      		      	 
	\end{proof}
	
				      	      
	\exc[Tao]{5.5.2} Let $E$ be a nonempty subset of $\R$, let $n \geq 1$ be an integer, and
	let $L<K$ be integers. Suppose that $K/n$ is an upper bound for $E$, but that
	$L/n$ is not an upper bound for $E$. Without using the Least Upper Bound Theorem, show that there exists an integer $L < m \leq K$ such that $m/n$ is an upper bound for $E$, but that $(m - 1)/n$ is not an upper bound for $E$.
				      	      
	\begin{proof}
		We will say a real number $w$ is U.B whenever $w$ is an upper bound for $E$.
							      		      	
							      		      	
		Suppose for sake of contradiction that there is no integer $L < m \leq K$ such that $m/n$ is U.B but that $(m - 1)/n$ is not U.B. This implies that if $m/n$ is U.B, $(m - 1)/n$ must also be U.B (as long as $L < m \leq K$).
		Let $P(t)$ be the statement "$L < K-t \implies$ Both $(K-t)/n$ and $(K-t-1)/n$ are U.B". We will prove $P(t)$ holds for all natural $t$ by induction. First, we need to show that $L < K \implies$ Both $K/n$ and $(K-1)/n$ are U.B. $K/n$ is U.B by assumption, and since $L < K \leq K$, $(K - 1)/n$ also has to be an U.B, again by assumption. Now assume $P(t)$. We need to show that $L < K-t-1 \implies$ Both $(K-t-1)/n$ and $(K-t-2)/n$ are U.B. If $ L \geq K-t-1$, $P(t+1)$ is vacuously true, so we assume $K \geq K-t-1 > L$. Notice that $K-t-1 > L \implies K-t > L$. By the induction hypothesis, $(K-t-1)/n$ is U.B, but this also means that $(K-t-2)/n$ is U.B, as we wanted to show.
							      		      	
		Now, since $K>L$ we have that $K \geq L+1$, which means that $K = L+1+c$ for some natural number $c$. Then $P(c)$ holds and also $L < K- c = L + 1 \leq K$. Therefore $(K-c-1)/n = L/n$ must be U.B, a contradiction.
	\end{proof}
				      	      
	\exc[Tao]{5.4.5} Given any two real numbers $x<y$, we can find a rational number $q$ such that $x<q<y$.
	\begin{proof}
		By the Archimedean Property, there is a positive integer $b$ such that $(y-x)b > 1 > 0$. Since $x-y$ is positive and their product with $b$ is also positive, it follows that $b > 0$. By Exercise 5.4.3, there is an integer $a-1$ such that $a-1 \leq bx < a$. By the definition of $b$, we have that $bx < by - 1$. Then, $a-1 \leq bx < by-1 \implies a < by$. Now we have $bx < a < by$. Since $b > 0$, we can divide this inequality by $b$ resulting in $x < a/b < y$. We can define $q := a/b$ and since $a$ and $b$ are integers (with $b \neq$ 0) we are done.
	\end{proof}
				      	      
	\exc[Tao]{5.4.8} Let $(a_n)_{n=0}^\infty$ be a Cauchy sequence of rationals, and let $x$ be
	a real number. Show that if $a_n \leq x$ for all $n \geq 1$, then $\text{LIM}_{n \to \infty} a_n \leq x$
	Similarly, show that if $a_n \geq x$ for all $n \geq 1$, then $\text{LIM}_{n \to \infty} a_n \geq x$.
				      	      
	\begin{proof}
		Assume $a_n \leq x$ for all $n \geq 1$. For sake of contradiction, assume that $\text{LIM}_{n \to \infty} a_n > x$. In that case, we can find a rational $q$ such that $\text{LIM}_{n \to \infty} a_n > q > x \geq a_n$. Since $q > a_n$ for all $n \geq 1$, Corollary 5.4.10 asserts that $q \geq \text{LIM}_{n \to \infty} a_n$. Now, we have that $\text{LIM}_{n \to \infty} a_n > q \geq \text{LIM}_{n \to \infty} a_n$, a contradiction. A very similar proof follows when $x \leq a_n$.
	\end{proof}
				      	      
	\exc{1.3.2}
	\begin{enumerate}
		\item A real number $s$ is the \textit{greatest lower bound} for a set $A \subseteq \R$ if and only if it meets the following criteria:
		      \begin{enumerate}
		      	\item $s$ is a lower bound for $A$;
		      	\item If $b$ is a lower bound for $A$, then $s \geq b$.
		      \end{enumerate}
		\item We want to show that if $s \in \R$ is a lower bound for a set $A \subseteq \R$, then $s = \inf A$ if and only if for every $\epsilon > 0$, there exists an element $a \in A$ such that $s + \epsilon > a$.
		      \begin{proof}
		      	For the forward direction, assume $s = \inf A$. Since $s + \epsilon > s$, it is not a lower bound for $A$. Therefore, there must be some $a \in A$ such that $s + \epsilon > a$.
		      			      			      		      	      		      	      	    
		      	Conversely, assume $s$ is a lower bound for $A$, with the property that for every $\epsilon > 0$ there is some $a \in A$ such that $ s + \epsilon > a$. Let $b$ be a lower bound for $A$. Assume for sake of contradiction that $b > s$. then we can choose $\epsilon = b - s > 0$ and we have that $b = s + \epsilon > a$, which means that $b$ is not a lower bound for $A$, a contradiction. Therefore $b \leq s$, thus $s = \inf A$.
		      \end{proof}
	\end{enumerate}
				      	      
	\pagebreak
	\exc{1.3.3}
				      	      
	\begin{enumerate}
		\item First, we show that $s := \sup B \geq m$ for any $m$ that is a lower bound for $A$. Since $m$ is a lower bound for $A$, $m \in B$. But, since $s$ is an upper bound for $B$, $s \geq m$. Now we need to show that given any $a \in A$, $s \leq a$. Let $b \in B$ be arbitrary. By Lemma 1.3.7, for every choice of $\epsilon > 0$, $s-\epsilon < b$. Since $b$ is a lower bound for $A$, we have that $s-\epsilon < b \leq a$, therefore $s \leq a$. 
		      		      		      	      	      	      	          
		\item The previous item gives a general process for finding the greatest lower bound of any nonempty $A \subseteq \R$ that is bounded below. Namely, construct a set $B = \{b \in \R : b \text{ is a lower bound for }A\}$. Then ,since $A$ is bounded below, $B$ is nonempty. $B$ is also bounded above, since any $a \in A$, is an upper bound for $B$. This follows because if $b \in B$, then $b$ is a lower bound for $A$, therefore $a \geq b$. Then, the Axiom of Completeness guarantees that $s := \sup B$ exists, and we have already shown that this means $s = \inf A$. This means that the existence of the greatest lower bound is a corollary of Completeness, hence needs not be asserted by it.
		      		      		      	      	      	      	          
		\item Let $A \subseteq \R$ be nonempty and bounded below. Then, define the set $-A = \{-x: x \in A\}$. $-A$ is clearly nonempty. Also, let $m$ be a lower bound for $A$ and pick an $x \in -A$. Then, $x = -a$ for some $a \in A$. But then $a \geq m \implies x=-a \leq -m$. This shows that $-m$ is an upper bound for $-A$, therefore $-A$ is both nonempty and bounded above. Then, by the Axiom of Completeness, there exists an $s := \sup -A$. We want to show that $-s = \inf A$. We have that $s \geq x = -a$. Then, $-s \leq a$ thus $-s$ is a lower bound for $A$. Now, let $b \in \R$ be a lower bound for $A$. Then, $-b$ is an upper bound of $-A$, therefore $-b \geq \sup -A = s$. This means that $b \leq -s$, and $-s$ is indeed the greatest lower bound of $A$.
	\end{enumerate}
				      	      
	\exc{1.3.4}
				      	      
	Since $B \subseteq A$, it follows that if $b \in B$, then $b \in A$. Lets pick one such $b$. Then we must have $b \leq \sup B$, but also $b \leq \sup A$
	, since $b$ is an element of $A$. This means that $\sup A$ must be an upper bound of $B$, therefore $\sup B \leq \sup A$.
				      	      
	\exc{1.3.5}
				      	      
	\begin{enumerate}
		\item Let $x$ be an element of $c + A$. Then, there is some $a \in A$ such that $x = c + a$. Then, since $\sup A \geq a$ we have $c + \sup A \geq c + a = x$, therefore $c + \sup A$ is an upper bound for $c + A$. Now, let $b$ be an upper bound of $c + A$. Then, $b \geq x = c+a$, which means that $b - c \geq a$, so $b-c$ is an upper bound for $A$. This means that $b-c \geq \sup A$, thus $b \geq c + \sup A$, and $\sup (c+A) = c + \sup A$, as desired.
		      		      		      	      	      	      	          
		\item If $c = 0$, then the only element of $c A$ is $0 = \sup c A = c \sup A$. Then, we can assume $c > 0$ and proceed very similarly to the previous item. Let $x$ be an element of $c A$. The, there is some $a \in A$ such that $x = c a$. Then, since $\sup A \geq a$ we have $c \sup A \geq c a = x$, therefore $c \sup A$ is an upper bound for $c A$. Now, let $b$ be an upper bound of $c A$. The, $b \geq x = c A$, which means that $b/c \geq a$, so $b/c$ is an upper bound of $A$. This means that $b/c \geq \sup A$, thus $b \geq c \sup A$, and $\sup (c A) = c \sup A$, as desired.
		      		      		      	      	      	      	          
		\item If $c < 0$, then $\sup(c A) = c\inf(A)$.
	\end{enumerate}
				      	      
	\exc{1.3.6}
				      	      
	\begin{enumerate}
		\item $3$, $1$.
		\item $1$, $0$.
		\item $1/3$, $1/2$
		\item $9$, $1/9$
	\end{enumerate}
				      	      
	\exc{1.3.7}
				      	      
	Let $b$ also be an upper bound for $A$. Then $b \geq a$, therefore $a = \sup A$
				      	      
	\exc{1.3.8}
	Let $\epsilon = \sup B - \sup A$. Since $\sup B > \sup A$, $\epsilon$ is positive, which means there is an element $b \in B$ such that $\sup A = \sup B - \epsilon < b$. Now, let $a$ be an element of $A$. Then, $a \leq \sup A < b$, therefore $b$ is an upper bound for $A$.
				      	      
	\exc{1.3.9}
	\begin{enumerate}
		\item True.
		\item False. $L = 2$ and $A = (0, 2)$.
		\item False. $A = (0, 1)$, $B = (1, 2).$
		\item True.
		\item False. $A = B = (0, 1).$
	\end{enumerate}
				      	       
	\exc{1.4.1}
				      	       
	If $b > 0$, then $a < 0 < b$ hence we can set $r := 0$ since $0 \in \Q$. Now, assume $b \leq 0$. Then, $0 \leq -b < -a$ so we can choose an $r \in \Q$ such that $-b < r < -a$, therefore $a < -r < b$ and we are done ($-r$ is also rational). 
				      	       
	\exc{1.4.2}
	Since $a,b \in \Q$, there are integers $r_1, r_2, q_1,q_2$ such that $a = r_1/q_1$ and $b = r_2/q_2$.
				      	        
	\begin{enumerate}
		\item \begin{equation*}
		      a + b = \frac{r_1}{q_1} + \frac{r_2}{q_2} = \frac{r_1 q_2 + r_2 q_1}{q_1 q_2} 
		\end{equation*}
							      		      	      
		\begin{equation*}
			ab = \frac{r_1 r_2}{q_1 q_2}
		\end{equation*}
							      		      	      
		Since $r_1 q_2 + r_2 q_1$ and $q_1 q_2 \neq 0$ are integers, $a+b$ and $ab$ are rational numbers.
							      		      	      
		\item Assume $a + t \in \Q$. Then, 
		      \begin{equation*}
		      	a + t = \frac{r_1}{q_1} + t = \frac{r_3}{q_3}
		      \end{equation*}
		      		      		      	      	      	      	            
		      for some $r_3, q_3 \in \Z$ with $q_3 \neq 0$. But then,
		      \begin{equation*}
		      	t = \frac{r_3}{q_3} +  \frac{-r_1}{q_1}
		      \end{equation*}
		      		      		      	      	      	      	            
		      which is a sum of rational numbers, therefore also rational, a contradiction. Since $\R$ is closed under addition, $t$ must be irrational.
		      		      		      	      	      	      	            
		\item $\mathbb{I}$ is not closed under addition or multiplication. If $t$ is an irrational number and $q$ is rational, then $s := q-t$ is also irrational. However, $t + s = q$ is a rational number, therefore two irrationals can sum to a rational. Also, if we instead set $s := q/t$, then $t s = q$, so irrationals are also not closed under multiplication.
	\end{enumerate}
				      	        
	\exc{1.4.3}
	Since $a < b$, $a - \sqrt{2} < b - \sqrt{2}$, and we can find a rational number $q$ such that $a - \sqrt{2} < q < b - \sqrt{2}$. We then have $a < q + \sqrt{2} < b$. Since $q \in Q$, $q + \sqrt{2}$ must be irrational.
				      	      
	\exc{1.4.4}
	Let $A := \{1/n : n \in \N\}$.
	Assume there is some $n \in \N$ such that $1/n < 0$. Multiplying by $n$ makes the contradiction clear, so $0$ is a lower bound for $A$. Now, let $b$  also be a lower bound for $A$ and assume $b > 0$. Then, we can use the Archimedean Property of $\R$ to find some $n \in \N$ such that $1/n < b$. But this contradicts the fact that $b$ is a lower bound for $A$, therefore we must have $b \leq 0$, which proves $0 = \inf A$.
				      	        
	\exc{1.4.5} 
	Assume that $\bigcap_{n=1}^\infty (0, 1/n) \neq \emptyset$. Then, there must be some $x \in \R$ such that for all $n \in \N$, $0 < x < 1/n$. But then we can use the Archimedean Property to find a natural $m$ such that $x > 1/m$, which is a contradiction. Therefore $\bigcap_{n=1}^\infty (0, 1/n) = \emptyset$.
				      	        
	\exc{1.4.6}
	The following has already been shown:
	\begin{equation*}
		(\alpha-\frac{1}{n})^2 > \alpha^2 - \frac{2\alpha}{n} \, .
	\end{equation*}
	Now, choose $n_0 \in \N$ such that
	\begin{equation*}
		\frac{1}{n_0} < \frac{\alpha^2-2}{2\alpha} \, .
	\end{equation*}
	It follows that 
	\begin{equation*}
		(\alpha-\frac{1}{n_0})^2 > \alpha^2-\frac{2\alpha}{n_0} > 2 > t^2
	\end{equation*}
	for any $t \in T$. Since $\alpha = \sup T$, it follows that there is some $r \ in T$ such that $\alpha - 1/n_0 < r$. But, since $(\alpha-1/n_0)^2 > r^2$ and $\alpha - 1/n_0 > 0$, we have $\alpha - 1/n_0 > r$, a contradiction.
				      	        
	\exc{1.4.7}
	Let $C_1 = \{n \in \N : f(n) \in A\}$ and $C_{k+1} = C_k \backslash \{n_k\}$ for all $k \in \N$, where $n_k := \min{C_k}$. Then, let $g : \N \rightarrow A$ be defined as $g(k) = f(n_k)$. First we prove a couple of useful lemmas.
				      	        
	\begin{shortlemma} \label{lem147_1}
		For all natural $k$, $n_{k+1} > n_k$. Also, $a \neq b \implies n_a \neq n_b$.
	\end{shortlemma} \begin{proof}
	In the first part, since $n_{k+1} \in C_{k+1}$, we have that $n_{k+1} \in \{n \in C_k : n \neq n_k\}$, therefore $n_{k+1} \in C_k$, so $n_{k+1} > n_k$, since $n_k$ is the minimum of $C_k$ and $n_{k+1} \neq n_k$.
				      	        
	Now, assume $a,b \in \N$ and $a \neq b$. Without loss of generality, also assume that $a > b$. Then $n_a > n_b$, therefore $n_a \neq n_b$.
	\end{proof}
					  
	\begin{shortlemma} \label{lem147_2}
		For every $L \in \N$ such that $f(L) \in A$, there is some $k \in \N$ such that $n_k = L$.
	\end{shortlemma}
	\begin{proof}
		Let $L$ be a natural number such that $f(L) \in A$. Then, $L \in C_1$. Now, assume for sake of contradiction that there is no natural $k$ such that $n_k = L$. This means that for all $k \in \N$, we can find a $c_k \in C_k$ such that $L > c_k$, since $L$ cannot be the minimum of $C_k$. Now, pick a $c_L \in C_L$ such that $L > c_L$. Then, $c_L$ must be grater than or equal to the minimum of $C_L$, namely $n_L$. Now, we can use Lemma \ref{lem147_1} to see that $L > n_L > n_{L-1} > \dots > n_1 > 0$. This shows that there are $L$ natural numbers strictly between $0$ and $L$, which is a contradiction.
	\end{proof}
					  
	Now, we show that $g$ is onto. Let $a$ be an element of $A$. Since $f$ is onto and $A \subseteq B$, there must be a natural number $L$ such that $f(L) = a$. Then, by Lemma \ref{lem147_2}, we have a $k \in \N$ such that $n_k = L$. Now, we have that $f(L) = f(n_k) = g(k) = a$, as we wanted to show.
					  
	Next, we show that $g$ is one-to-one. Assume that $g(k_1) = g(k_2)$, for $k_1, k_2 \in \N$. This means that $f(n_{k_1}) = f(n_{k_2})$, and since $f$ is one-to-one, it must be that $n_{k_1} = n_{k_2}$. By Lemma \ref{lem147_1}, this can only happen when $k_1 = k_2$, and we are done.
					  
	\exc{1.4.8}
	\begin{enumerate}
		\item $B_2$ is a subset of $A_1$, therefore it is countable or finite. First, assume it is countable. Then, there must be two functions $f : \N \rightarrow A_1$ and $g : \N \rightarrow B_2$ which are 1-1 and onto. Then, we can define another function $h : \N \rightarrow A_1 \cup B_2$ as following:
		      \begin{equation*}
		      	h(n) = \begin{cases}
		      	f(\frac{n-1}{2}) & \text{n is odd} \\
		      	g(\frac{n}{2}), & \text{n is even}
		      	\end{cases}
		      \end{equation*}
		      		      		      	      	      	      	        
		      To show that $h$ is 1-1, assume that $a \neq b$ for natural numbers $a$ and $b$. If they are both odd, then $h(a) = f((a-1)/2)$ and $h(b) = f((b-1)/2)$. Since $(a-1)/2 \neq (b-1)/2$ and $f$ is 1-1, we must have $h(a) \neq h(b)$.
		      A very similar argument follows if $a$ and $b$ are both even. The final case is $a$ is odd and $b$ is even. Then, $h(a) = f((n-1)/2)$ and $h(b) = g(b/2)$. Since $f((n-1)/2) \in A_1$ and $g(b/2) \in B_2$, it must be the case that $h(a) \neq h(b)$, since $g(b/2) \notin A_1$. Now, we need to show that $h$ is onto. Let $t \in A_1 \cup B_2$. We must find an $n \in \N$ such that $h(n) = t$. First, assume $t \in A_1$. Since $f$ is onto, there is a $k \in \N$ such that $f(k) = t$. Setting $n := 2k + 1$, we have that $h(n)=f(k)=t$. Next, assume $t \in B_2$. Since $g$ is onto, there is a $k \in B_2$ such that $g(k) = t$. Setting $n := 2k$, we have that $h(n)=g(k)=t$.
		      This shows that $A_1 \cup B_2 \sim \N$ is countable whenever $B_2$ is countable.
		      		      		      	      	      	      	        
		      Next, we informally discuss why the theorem holds if $B_2$ is finite. In this case, we can find a bijection $g: \{1, 2, 3, \dots, m\} \rightarrow B_2$, where $m$ is the cardinality of $B_2$. Now, we define a new function $h: \N \rightarrow A_1 \cup B_2$ as following: 
		      \begin{equation*}
		      	h(n) = \begin{cases}
		      	g(n) & n \leq m \\
		      	f(n-m) & n > m
		      	\end{cases}
		      \end{equation*}
		      where $f : \N \rightarrow A_1$ is a bijection. Now pick two numbers $a, b \in \N$ and assume $a \neq b$. Without loss of generality, we can make $a > b$. If $a > b > m$, then $h(a) = f(a-m)$ and $f(b-m) = h(b)$. Since $f$ is 1-1, $h(a) \neq h(b)$. Next, if $b \leq m$,  then $h(b) = g(b)$. In the case where $a \leq m$, we also have $h(a) = g(a) \neq g(b)$. Otherwise, $h(a) = f(a-m) \in A_1$, and, since $h(b) \notin A_1$, $h(a) \neq h(b)$, therefore $h$ is 1-1.
		      		      		      	      	      	      	        
		      Now, we need to show that $h$ is onto. Let $t \in A_1 \cup B_2$. We must find an $n \in \N$ such that $h(n) = t$. First, assume $t \in A_1$. Since $f$ is onto, there is a $k \in \N$ such that $f(k) = t$. Setting $n := m + k$, we have that $h(n) = f(k) = t$. Next, assume $t \in B_2$. Since $g$ is onto, there is a $k \in B_2$ such that $g(k) = t$. Setting $n := k$, we have that $h(n)=g(k)=t$, since $k \in B_2 \implies k \leq m$.
		      		      		      	      	      	      	        
		      The more general statement in (i) follows easily by applying induction to the statement just proved.
		      		      		      	      	      	      	        
		\item We can use induction to show that $\bigcup_{n=1}^m A_n$ is countable for any particular $m \in \N$, which only consists of a finite ($m$) number of unions, not infinite.
		      		      		      	      	      	      	        
		\item Each one of the columns has a countable number of elements, and there are countably many columns. By matching each $a_m \in A_n$ with the $n$th column, and $m$th row, we have created a bijection between the unions of all the $A_n$ and the natural numbers.
	\end{enumerate}
				      	          
	\exc{1.4.9}
				      	          
	\begin{enumerate}
		\item Let $f : A \rightarrow B$ be a bijection. Since for every $b \in B$ there is a unique $a \in A$ such that $f(a) = b$, we can define a new function $g: B \rightarrow A$ where $g(b) = g(f(a)) = a$. To show that $g$ is 1-1, let $g(b_1) = g(b_2)$, where $b_1,b_2 \in B$. Then, we can find $a_1, a_2 \in A$ such that $f(a_1) = b_1$ and $f(a_2) = b_2$. Then, $g(f(a_1)) = g(f(a_2))$ and by the definition of $g$ this means that $f(a_1) = f(a_2)$, therefore $a_1 = a_2$. Now, $b_1 = f(a_1) = f(a_2) = b_2$, so $g$ is 1-1. Next, let $a \in A$. We must find a $b \in B$ such that $g(b) = a$. All we have to do is set $b := f(a)$, and we are done. Since $g$ is a bijection between $B$ and $A$, it follows that $B \sim A$.
		      		      		      	      	      	      	              
		\item Let $f : A \rightarrow B$ and $g : B \rightarrow C$ be bijections and $h : A \rightarrow C$ be a function such that for every $a \in A$, $h(a) = g(f(a))$. This can be done since $f(a) \in B$ and $g(f(a)) \in C$. To show that $h$ is 1-1, let $h(a_1) = h(a_2)$. This means that $g(f(a_1)) = g(f(a_2))$, then $f(a_1) = f(a_2)$ and finally $a_1 = a_2$. Now, pick a $c \in C$.  Since $g$ is onto, there is a $b \in B$ such that $g(b) = c$, and since $f$ is onto, there is an $a \in A$ such that $f(a)=b$. Then $g(f(a)) = h(a) = c$, which shows $h$ is onto. Since $h$ is also 1-1, it follows that $A \sim C$.
	\end{enumerate}
				      	          
	\exc{1.4.10}
	Let $S_n := \{S \subseteq \N : \text{The cardinality of } S = n\}$ for every $n \in \N$. Then, the set of all finite subsets of $\N$ is $U = \bigcup_{n=1}^\infty S_n$. If we can show that each $S_n$ is countable, Theorem 1.4.13 guarantees $U$ is also countable.
				      	          
	Define $T_{1,m} = S_1$ and $T_{n+1,m} = \{\{m\} \cup s : s \in S_n, m \notin s\}$ for all $n, m \in \N$. We claim that \begin{equation*}
	S_n = \bigcup_{m=1}^{\infty} T_{n,m} \, .
	\end{equation*}
					    
	This is clearly true when $n = 1$, so we now show that the equality holds for all $n > 1$. To see that, let $x \in \bigcup_{m=1}^{\infty} T_{n,m}$. Then $x \in T_{n, m}$ for some $m \in \N$. This means that $x = \{m\} \cup s$, where $s \in S_{n-1}$, thus $x \subseteq \N$. Since $m \notin s$, the cardinality of $x$ is $n$, so $x \in S_n$. Now, we must show that $x \in S_n \implies x \in \bigcup_{m=1}^{\infty} T_{n,m}$, and we will call this statement $P(n)$. Assume $x \in S_n$. If $n = 1$, we have already seen that the equality in question holds, so $P(1)$ is true. Now assume $P(n)$. Also, let $y \in S_{n+1}$. Then, the cardinality of $y$ is $n+1$. Next, we can use the fact that $y \subseteq \N$ to see that $y = \{m\} \cup s$ for some $s \in S_n$ and some natural $m \notin s$. But this means that $y \in T_{n+1, m}$, so $P(n+1)$ holds.
					    
	Now, lets show by induction that $T_n$ is countable. $T_{1,m} = S_1$ is easily seen to be countable by defining a function $v : \N \rightarrow T_{1,m} $ such that $v(n) = \{n\}$. Now, assume $T_{n,m}$ is countable and define the set $A_m := \{a \in \N : m \notin f(a)\}$. By Theorem 1.4.13, $S_n$ is also countable, therefore there is a bijection $f : \N \rightarrow S_n$. Define a function $g : A\rightarrow T_{n+1, m}$ such that $g(a) = \{m\} \cup f(a)$, and let $a_1, a_2 \in A$ with $a_1 \neq a_2$. Then, $f(a_1) \neq f(a_2)$, and since $m \notin f(a_1), f(a_2)$, we have that $\{m\} \cup f(a_1) \neq \{m\} \cup f(a_1)$, thus $g(a_1) \neq g(a_2)$ so $g$ is 1-1. Now, let $t \in T_{n+1, m}$. Then, $t = \{m\} \cup s$ where $m \notin s$. Since $f$ is onto, there is some $n \in \N$ such that $f(n) = s$. Then, $g(n) = \{m\} \cup s = t$, so $g$ is onto. This shows that $A \sim T_{n+1, m}$. Since $A \subseteq \N$ and $A$ is not finite, it must be countable, therefore every $T_{n, m}$ is also countable, and Theorem 1.4.13 can be used to see that this results in every $S_n$ being countable, as we wanted to show.
					    
	\exc{1.4.11}
	\begin{enumerate}
		\item $f : (0,1) \rightarrow S$, $f(x) = (x, 1/2)$.
		      		      		      	      	      	      	              
		\item For every $x \in \R$ if there is a decimal expansion of $x$ that ends in a tail of nines, we can instead choose one that ends in a tail of zeros, and we will call this the unique expansion of $x$ (when $x$ does not end in a tail of nines the expansion is already unique). Then, let $(x_1, x_2) \in S$. We can expand $x_1$ and $x_2$ uniquely as follows:
		      		      		      	      	      	      	              
		      \begin{gather}
		      	\nonumber x_1 = 0.d_1 d_2 d_3 \dots \\
		      	\nonumber x_2 = 0.e_1 e_2 e_3 \dots
		      \end{gather}
		      Then, define the function $f : S \rightarrow (0, 1)$ as following: 
		      \begin{equation*}
		      	f((x_1, x_2)) = 0.d_1e_1d_2e_2 \dots
		      \end{equation*}
		      		      		      	      	      	      	             
		      $f$ is 1-1, but not onto. Consider for example $x = 0.898989\dots$. Notice that every other digit is a $9$, so in order for $f$ to map some ordered pair to $x$, one of the elements of the pair would have to be $0.999\dots = 1$, which is not in the domain of $f$.
	\end{enumerate}
				      	          
	\exc{1.5.11} (Switched to second edition here)
				      	          
	\begin{enumerate}
		\item Since $g$ maps $B'$ onto $A'$, for every $a \in A'$ there is a $b \in B'$ such that $g(b) = a$. Since $g$ is also 1-1, this $b$ is unique. Then, we can define a function $g^{-1} : A' \rightarrow B'$ such that $g^{-1}(a) = b$. To show that $g^{-1}$ is onto, let $y \in B'$ be arbitrary. Then $g(y) = x$ for some $x \in A'$, which by the definition of $g^{-1}$ means that $g^{-1}(x) = y$ so $g^{-1}$ is onto. Next, we show that $g{-1}$ is 1-1 by letting $g^{-1}(x_1) = g^{-1}(x_2)$ for some $x_1, x_2 \in A'$. Then, we can find $y_1, y_2 \in B'$ such that $g(y_1) = x_1$ and $g(y_2) = x_2$. Then, $g^{-1}(g(y_1)) = g^{-1}(g(y_2))$, in other words, $g(y_1) = g(y_2)$, which means $y_1 = y_2$ since $g$ is 1-1. Then, $g(y_1) = x_1 = g(y_2) = x_2$, and $g^{-1}$ is 1-1 and onto.
		      		      		      	      	      	      	              
		      Now, let $h : X \rightarrow Y$ be such that 
		      \begin{equation*}
		      	h(x) = \begin{cases}
		      	f(x) & x \in A \\ 
		      	g^{-1}(x) & x \in A'
		      	\end{cases}
		      \end{equation*}
		      		      		      	      	      	      	              
		      for every $x \in X$. Now, assume $a \neq b$ for $a, b \in X$. If $a$ and $b$ are elements of $A$, then $h(a) \neq h(b)$, since $f$ is 1-1. Also, if $a,b \in A'$, then $h(a) \neq h(b)$ since $g^{-1}$ is 1-1. The last case is $a \in A$ and $b \in A'$, then $h(a)=f(a) \in B$, and $h(b)=g^{-1}(b) \in B'$, and we can use the fact that $A'$ and $B'$ are disjoint to see that $h(a) \neq h(b)$, so $h$ is 1-1. Now, let $y \in Y$. If $y \in B$, then there is some $a \in A$ such that $f(a)=h(a)=y$, since $f$ maps $A$ onto $B$. Also, if $y \in B'$, then there is some $a' \in A'$ such that $g^{-1}(a') = y$, since $g^{-1}$ is onto.
	\end{enumerate}
				      	          
	\exc{1.6.1}
	The function $(1-2x)/((2x-1)^2-1)$ maps $(0, 1)$ to $\R$ both 1-1 and onto, therefore $\R \sim (0, 1)$, and since $\sim$ is an equivalence relation, $\R$ is uncountable $\iff (0, 1)$ is uncountable. 
				      	          
	\exc{1.6.2}
	\begin{enumerate}
		\item If $a_{11} = 2$, then $b_1 = 3$, and if $a_{11} \neq 2$, $b_1 = 2$. In both cases, $a_{11} \neq b_1$. Since $f(1) = .a_{11} a_{12} \dots$, $x$ and $f(1)$ differ in at least one decimal place, therefore they are not equal.
		      		      		      	      	      	      	              
		\item If $a_{nn} = 2$, then $b_n = 3$, and if $a_{nn} \neq 2$, $b_n = 2$. In both cases, $a_{nn} \neq b_1$. Since $f(n) = .a_{n 1} \dots a_{nn} a_{n n+1} \dots$, $x$ and $f(n)$ differ in at least one decimal place , therefore they are not equal.
		      		      		      	      	      	      	              
		\item We assumed that every real number is included in the list, therefore there is some $n \in \N$ such that $x = f(n)$. However, we have also shown that this cannot be the case, which is a contradiction. Therefore, our assumption that $(0, 1)$ must be false, and $(0, 1)$ is uncountable.
	\end{enumerate}
				      	          
	\exc{1.6.3}
				      	          
	\begin{enumerate}
		\item We cannot apply the same argument to $\Q$ because even though every rational number has a decimal expansion, it is not true that every decimal expansion corresponds to a rational number. Therefore, the number $x = .b_1b_2 \dots$ created is only guaranteed to be a real number, so we cannot use the fact that $x$ is not in the list to get a contradiction. Instead, this argument shows that the number $x$ must be irrational.
		      		      		      	      	      	      	              
		\item We used the fact that if $x, y \in \R$ and the $n$th digit of $x$ is not equal to the $n$th digit of $y$, then $x \neq y$. However, $0.499\dots = 0.5$, and their first digits (after the decimal point) are different. Fortunately, this only happens when one of $x$ has a decimal expansion that terminates, and $y$ can be written with repeating nines (or vice-versa), and this is never the case with the real number $x$ that we constructed, since its only digits are $2$ and $3$. 
	\end{enumerate}
				      	          
	\exc{1.6.4}
	Assume that $S$ is countable. Then, there is a function $f : \N \rightarrow S$ which is 1-1 and onto. Now, let $(a_n)$ be a sequence such that \begin{equation*}
	a_n = \begin{cases}
	0 & f(n)_n = 1 \\ 
	1 & f(n)_n = 0
	\end{cases}
	\end{equation*}
	where $f(n)_n$ represents the $n$th entry in the sequence $f(n)$. Since $a_n$ is a sequence of only zeros and ones, $(a_n) \in S$. Since $f$ is onto, this means that there is some $k \in \N$ such that $f(k) = (a_n)$. However, we know that $f(k)_k \neq a_k$, therefore $f(k) \neq (a_n)$, a contradiction. This means that $S$ is not countable. Since $S$ is also infinite, $S$ is uncountable.
					    
	\exc{1.6.5}
				      	          
	\begin{enumerate}
		\item $P(A) = \{\emptyset, \{a\}, \{b\}, \{c\}, \{a, b\}, \{a, c\}, \{b, c\}, \{a, b, c\}\}$.
		      		      		      	      	      	      	              
		\item If $A$ has $1$ element $a$, then $P(A) = \{\emptyset, \{a\}\}$ has $2^1 = 2$ elements. Now assume that if $A$ has $n$ elements $P(A)$ has $2^n$ elements. Let $B$ have $n + 1$ elements, and $b \in B$. The set $B' = B \backslash \{b\}$, has cardinality $n$, so $P(B')$ has $2^n$ elements. But every element of $P(B)$ is either an element of $P(B')$ or the union of one of the elements of $B'$ with $b$. Therefore, $P(B)$ has $2^n + 2^n = 2^{n+1}$ elements.
	\end{enumerate}
				      	          
	\exc{1.6.6}
				      	          
	\begin{enumerate}
		\item \begin{equation*}
		      f(x) = \begin{cases}
		      \emptyset & x = a \\
		      \{a\} & x = b \\
		      \{b\} & x = c
		\end{cases}
		\end{equation*}
		\begin{equation*}
			g(x) = \begin{cases}
			\{a\} & x = a \\
			\{b\} & x = b \\
			\{c\} & x = c
			\end{cases}
		\end{equation*}
							      		      	        
		\item \begin{equation*}
		      g(x) = \begin{cases}
		      \{1\} & x = 1 \\
		      \{2\} & x = 2 \\
		      \{3\} & x = 3 \\ 
		      \{4\} & x = 4
		\end{cases}
		\end{equation*}
							      		      	        
		\item Since there are more elements in $P(C)$ than $C$, a mapping from $C \rightarrow P(C)$ always "runs out of" elements from $C$ before mapping all to all of the elements in $P(C)$.
	\end{enumerate}
				      	          
	\exc{1.6.8}
				      	          
	\begin{enumerate}
		\item By the definition of $B$, $a'$ is some element of $A$ such that $a' \notin f(a') = B$. Since we assumed $a' \in B$, this is a contradiction.
		      		      		      	      	      	      	              
		\item Since $a' \notin B$ and $a' \in A$, it must be the case that $a' \in f(a') = B$, a contradiction.
	\end{enumerate}
				      	          
	\exc{1.6.9}
	Let $A \in P(\N)$ be an arbitrary subset of the naturals. Then, define the function $f : P(\N) \rightarrow S$ such that
	\begin{equation*}
		f(A)_n = \begin{cases}
		0 & n \notin A \\
		1 & n \in A
		\end{cases}
	\end{equation*}
	where $S$ is the set of all sequences of $0'$s and $1'$s discussed in Exercise 1.6.4, and $f(A)_n$ stands for the $n$th term of the sequence $f(A)$. Since $S$ is uncountable, if we can show that $f$ is 1-1 and onto, then $P(\N) \sim S$, which is uncountable. Assume that $f(X) = f(Y)$ for some $X, Y \subseteq \N$. This means that for all $n \in \N$ $f(X)_n = f(Y)_n$. Now, pick an arbitrary $n \in X$. Then, $f(X)_n = 1 = f(Y)_n$, which means $n$ must also be an element of $Y$. A very similar argument follows if you first pick an $n \in Y$. This means that $n \in X \iff n \in Y$, so $X = Y$ and $f$ is 1-1. Now, let $s \in S$ be arbitrary. To show that $f$ is onto, we must find some $A \subseteq \N$ such that $f(A) = s$. To do that let $A = \{a \in \N:s_a = 1\}$. Then $f(A)_n = 1$ means that $n \in A$, which only happens if $s_n$ is also equal to $1$, so $f(A_n) = s_n$ in this case. Finally, if $f(A)_n = 0$, then $n \notin A$, so $s_n \neq 1$, which can only happen if $s_n = 0 = f(A)_n$, therefore $f(A) = s$ and $f$ is onto. 
				      	          
	We have shown that $P(\N) \sim S$, but our goal was to show that $P(\N) \sim \R$. We do this by showing that $S \sim (0, 1)$. Since $(0, 1) \sim \R$ and $\sim$ is an equivalence relation this automatically gives our wanted result. To do that, let $x \in (0, 1)$ be a real number. We are interested in the binary representation of $x$, namely \begin{equation*}
	x = 0.a_1 a_2 a_3 \dots
	\end{equation*}
	where the $a_n$ are either $0$ or $1$. Also, we require that the binary expansion never terminates in $1'$s. Then, the function $f : (0, 1) \rightarrow S$ such that $f(x)_n = a_n$ is easily seen to be 1-1, but it is not onto, since sequences that terminate in $1$'s will not be "reached" by the function. However, by the Schröder–Bernstein Theorem finding a 1-1 function from $g : S \rightarrow (0, 1)$ is enough for our purposes. To do this, let $g(A)_n = A_n$, where $g(A)_n$ represents the $n$th digit in the decimal expansion of a real number in the interval $(0, 1)$. $g$ is clearly 1-1, so we are done.
					    
	\exc{1.6.10}
				      	          
	\begin{enumerate}
		\item Let $F$ be the set of all functions from $\{0, 1\}$ to $\N$. Then, define $g : \N^2 \rightarrow F$ such that $g((a, b))$ is a function $f : \{0, 1\} \rightarrow \N^2$ such that \begin{equation*}
		      f(x) = \begin{cases}
		      a & x = 0 \\
		      b & x = 1
		\end{cases}
		\end{equation*}
		$g$ is easily seen to be 1-1 and onto, so $F \sim \N^2 \sim \N$, therefore $F$ is countable.
							      		      	        
		\item Let $F$ now be the set of all $f : \N \rightarrow \{0, 1\}$. Now let the function $g : F \rightarrow S$ be such that $g(f)_n = f(n)$ for every $n \in \N$ and every $f \in F$, where $S$ is the set of all sequences of $0'$s and $1'$s discussed in Exercise 1.6.4. Again, $g$ is easily seen to be a bijection, so $F \sim S \sim \R$, therefore $F$ is uncountable.
	\end{enumerate}
				      	          
	\exc{2.2.1}
	The sequence $f(n) = (-1)^n$ verconges to $0$ and $1$, but does not converge. This definition describes bounded sequences.
				      	          
	\exc{2.2.4}
				      	          
	\begin{enumerate}
		\item $f(n) = (-1)^n$.
		\item There is no such sequence. To see that, let $(a_n)$ be a sequence such that for every $N \in \N$ there is some $n \geq N$ such that $a_n = 1$ which also converges to some real number $L$. Now, assume $L \neq 1$. Since $(a_n)$ converges, there is some $M \in \N$ such that for all $m \geq M$ $|a_m - L| < |1-L|/2$, since $|1-L|/2 > 0$. By the construction of $(a_n)$, we can pick an $m \geq M$ such that $a_m = 1$. Then, we have $|1-L| < |1-L|/2$ which implies $1 < 1/2$, a contradiction. Therefore $(a_n)$ must converge to $1$.
		      		      		      	      	      	      	              
		\item $(0, 1, 0, 1, 1, 0, 1, 1, 1, 0, \dots)$.
	\end{enumerate}
				      	          
	\exc{2.2.5}
				      	              
	\begin{enumerate}
		\item We claim that $\lim a_n = 0$. Let $\epsilon > 0$ be arbitrary. Choose a natural number $N > 5$. Notice that whenever $n \geq N > 5$, $1 > 5/n \geq 0$, which means $0 = [[a_n]]$, therefore $|a_n - 0| = 0 < \epsilon$.
		      		      		      	      	      	      	                  
		\item We claim that $\lim a_n = 1$. Let $\epsilon > 0$ be arbitrary. Choose a natural number $N > 6$. Notice that whenever $n \geq N > 6$, $2 > (12+4n)/(3n) \geq 1$, which means $1 = [[a_n]]$, therefore $|a_n - 1| = 0 < \epsilon$.
	\end{enumerate}
				      	              
	\exc{2.2.6}
	Assume $a \neq b$. Then, there are naturals $N_1, N_2$ such that for every $n_1 \geq N_1$ and every $n_2 \geq N_2$, we have $|a_{n_1} - a| < |a-b|/2$ and $|a_{n_2} - b| < |a-b|/2$. By letting $N = \max{N_1, N_2}$, it is then true that for every $n \geq N$ $|a_n - a| < |a-b|/2$ and $|a_n - b| < |a-b|/2$. Adding both of these equations, we have $|a_n - a| + |a_n - b| < |a-b|$, which contradicts the triangle inequality, so we must have $a = b$.
				      	              
	\exc{2.2.7}
				      	              
	\begin{enumerate}
		\item The sequence $(-1)^n$ is frequently in $\{1\}$.
		\item Definition (i) is stronger, a sequence that is eventually in a set is also frequently in the set.
		      		      		      	      	      	      	                  
		\item A sequence $(a_n)$ converges to $a$ if, given any $\epsilon$-neighborhood $V_\epsilon(a)$ of $a$, the sequence is eventually in $V_\epsilon(a)$.
		      		      		      	      	      	      	                  
		\item The sequence $(1,2,1,2,1\dots)$ is not eventually in $(1.9, 2.1)$. However, any sequence with an infinite number of $2'$s is frequently in $(1.9, 2.1)$, since $2$ is in this set. 
	\end{enumerate}
				      	              
	\exc{2.2.8}
				      	              
	\begin{enumerate}
		\item Yes.
		\item Yes.
		\item The sequence $(0, 1, 0, 1, 1, 0, 1,1,1, 0 \dots)$ is a counterexample.
		\item A sequence is not zero-heavy if for all $M \in \N$ there exists $N \in \N$ such that for all $n$ satisfying $N \leq n \leq N + M$ we have $x_n \neq 0$.
	\end{enumerate}
				      	              
	\exc{2.3.1}
	\begin{enumerate}
		\item  Let $\epsilon > 0$ be arbitrary. Since $(x_n) \rightarrow 0$, there is an $N \in \N$ such that for all $n \geq N$, $x_n < \epsilon^2$. Then, $\sqrt{x_n} = |\sqrt{x_n} - 0| < \epsilon$, so $(\sqrt{x_n}) \rightarrow 0$.
		      		      		      	      	      	      	                  
		\item Since item (a) already proves the case where $x = 0$, we can assume $x > 0$. Now, let $\epsilon > 0$ be arbitrary. Since $(x_n) \rightarrow x$, there is an $N \in \N$ such that for all $n \geq N$ $|x_n-x| < \epsilon \sqrt{x}$. In that case, \begin{equation*}
		      |\sqrt{x_n}-\sqrt{x}| = |\sqrt{x_n}-\sqrt{x}| \frac{\sqrt{x_n}+ \sqrt{x}}{\sqrt{x_n}+ \sqrt{x}} = \frac{|x_n-x|}{\sqrt{x_n}+\sqrt{x}} \leq \frac{|x_n-x|}{\sqrt{x}} < \epsilon
		\end{equation*}
		and we are done.
	\end{enumerate}
				      	              
	\exc{2.3.2}
				      	              
	\begin{enumerate}
		\item Let $\epsilon > 0$ be arbitrary. Since $(x_n) \rightarrow 2$, we can choose a natural number $N$ such that $|x_n-2| < 3\epsilon/2$ for all $n \geq N$. Then, \begin{equation*}
		      \abs{\frac{2x_n-1}{3}-1} = \frac{2}{3}|x_n-2| < \epsilon \, .
		\end{equation*}
							      		      	            
		\item Let $\epsilon > 0$ be arbitrary. Since $(x_n) \rightarrow 2$, we can choose a natural number $N_1$ such that $|x_n-2| < 2\epsilon$ for all $n \geq N_1$. We can also find a natural $N_2$ such that $\abs{2-x_n} < 1$, for all $n \geq N_2$, which implies $|x_n| > 1$. Let $N := \max{N_1, N_2}$. Then, for all $n \geq N$, we have \begin{equation*}
		      \abs{\frac{1}{x_n}-\frac{1}{2}} = \abs{\frac{2-x_n}{2x_n}} < \abs{\frac{x_n-2}{2}} < \epsilon.
		\end{equation*}
	\end{enumerate}
	
	\begin{shortlemma} \label{lem_convergEquiv}
	    If $(x_n), (y_n) \rightarrow L$ for some real number $L$, then for every $\epsilon > 0$ there is some natural $N$ such that $\abs{x_n-y_n} < \epsilon$ for all $n \geq N$.
	\end{shortlemma}
	\begin{proof}
    \lep \space and use the fact that both the sequences converge to find $N_1, N_2 \in \N$ such that $\abs{x_n - L} < \epsilon/2$ for all $n \geq N_1$ and $\abs{y_m-L} < \epsilon/2$ for all $m \geq N_2$. Setting $N := \max{N_1, N_2}$ we have $\abs{x_n-L} < \epsilon/2$ $\abs{y_n-L} < \epsilon/2$ for all $n \geq N$. Summing the two inequalities, we get $\abs{x_n-L} + \abs{y_n-L} < \epsilon$, and we can use the triangle inequality to see that $\abs{x_n-y_n} \leq \abs{x_n-L} + \abs{y_n-L} < \epsilon$, as we wanted to show.
	\end{proof}
				      	              
	\exc{2.3.3}
	\lep. The convergence of $(x_n)$ and $(y_n)$ to $l$, together with Lemma \ref{lem_convergEquiv} imply that we can choose $N \in \N$ such that $\abs{x_n-l}, \abs{x_n-z_n} < \epsilon/2$. Also, we have 
	\begin{equation*}
	    x_n \leq y_n \leq z_n \implies 0 \leq y_n-x_n \leq z_n-x_n \implies 
	    \abs{y_n-x_n} \leq \abs{z_n-x_n} < \frac{\epsilon}{2}.
	\end{equation*} Then, $\abs{y_n-l} \leq \abs{y_n-x_n} + \abs{x_n-l} < \epsilon/2 + \epsilon/2 = \epsilon$, which means $(y_n) \to l$, as we wanted to show.
	 
	\exc{2.3.4}
	\begin{enumerate}
		\item Applying the Algebraic Limit Theorem several times, we have: \begin{gather}
		      \nonumber \lim (\frac{1+2a_n}{1+3a_n-4a_{n}^2}) = \frac{\lim (1 + 2a_n)}{\lim (1 + 3a_n-4a_n^2)} = \\
		      \nonumber \frac{\lim (1) + 2\lim(a_n)}{\lim(1) + 3\lim(a_n) - 4 \lim(a_n) \lim(a_n)} = \frac{1}{1} = 1 \, .
		\end{gather}
							      		        
		\item \begin{equation*}
		      \frac{(a_n+2)^2-4}{a_n} = \frac{a_n (a_n+4)}{a_n} = a_n + 4
		\end{equation*}
		Then, \begin{equation*}
		\lim (\frac{(a_n+2)^2-4}{a_n}) = \lim (a_n) + \lim(4) = 4 \, .
		\end{equation*}
							      		        
		\item \begin{equation*}
		      \lim(\frac{\frac{2}{a_n}+3}{\frac{1}{a_n}+5}) = \lim (\frac{3a_n + 2}{5a_n + 1}) = 2 \, .
		\end{equation*}
	\end{enumerate}
				      
	\exc{2.3.5}
	Assume $(z_n) \rightarrow L$, for some real number $L$. We must show that both $(x_n)$ and $(y_n)$ are also convergent. Let $\epsilon > 0$ be arbitrary. There exists a natural number $N$ such that for all $n \geq N$ we have $\abs{z_n - L} < \epsilon$. Since $n \geq N \implies 2n-1 \geq N$, we also have $\abs{z_{2n-1} - L} < \epsilon$ for $n \geq N$. Similarly, $n \geq N \implies 2n \geq N$, therefore $\abs{z_{2n}-L} < \epsilon$. Therefore, for all $n \geq N$ we have both $\abs{x_n - L} < \epsilon$ and $\abs{y_n - L} < \epsilon$, since $z_{2n-1} = x_n$ and $z_{2n} = y_n$, so all three sequences converge to $L$.
			        
	For the converse, we assume $(x_n), (y_n) \rightarrow L$ for some real number $L$, and we must show $(z_n)$ also converges, in particular, we will show $(z_n) \rightarrow L$. Since $\abs{a} \geq 0$ for any real $a$, we have $\abs{y_n-L} = \abs{z_{2n} - L} \leq \abs{x_n-y_n} + \abs{y_n-L}$ for all natural $n$. Also, we can use the triangle inequality to see that $\abs{x_n-L} = \abs{z_{2n-1}- L} \leq \abs{x_n-y_n} + \abs{y_n - L}$. Now, let $\epsilon > 0$ be arbitrary. Lemma \ref{lem_convergEquiv} lets us choose a natural $N$ such that $\abs{x_n-y_n} < \epsilon/2$ and $\abs{y_n-L} < \epsilon/2$ for all $n \geq N$. Using the two inequalities just mentioned, we then have $\abs{z_{2n} - L} \leq \epsilon$ and $\abs{z_{2n-1} - L} < \epsilon$. This shows that $\abs{z_m - L} < \epsilon$ for all $m \geq 2N-1$, so $(z_n) \rightarrow 0$.
			        
			        
	\exc{2.3.6}
	First, notice that
	\begin{equation*}
		\lim (1/n) = 0 \implies \lim(1+\sqrt{1+\frac{2}{n}}) = 2 \implies \lim (\frac{-2}{1+\sqrt{1+ \frac{2}{n}}}) = -1 \, .
	\end{equation*}
	Also, 
	\begin{equation*}
		b_n = n-\sqrt{n^2+2n} \cdot \frac{n+\sqrt{n^2+2n}}{n+\sqrt{n^2+2n}} = \frac{-2n}{n+\sqrt{n^2+2n}} = \frac{-2}{1+\sqrt{1+\frac{2}{n}}} ~ .
	\end{equation*}
			        
	Combining both results we have $\lim(b_n) = -1$.
			        
	\exc{2.3.7}
	\begin{enumerate}
		\item $x_n = n$ and $y_n = -n$.
		      		      		                  
		\item This is impossible. To see this, assume that $(x_n+y_n)$ and $(x_n)$ are convergent, while $(y_n)$ is not. By the Algebraic Limit Theorem, we have $\lim(y_n)=\lim((x_n+y_n)-x_n) = \lim(x_n+y_n)-\lim(x_n)$, so $(y_n)$ converges, a contradiction.
		      		      		                  
		\item $(1, 1/2, 1/3, 1/4 \dots)$.
		      		      		                  
		\item This is not possible. Assume for contradiction that $(a_n)$ is unbounded, $(b_n)$ is convergent and $(a_n-b_n)$ is bounded. By Theorem 2.3.2, there is a real number $M$ such that $M \geq \abs{b_n}$ for all $n$. By our initial assumption, there is also a real $L$ such that $L \geq \abs{a_n-b_n}$ for all $n$. Then, we have $L \geq \abs{a_n-b_n} \geq \abs{a_n} - \abs{b_n} \geq \abs{a_n} - M$, which means $L+M \geq \abs{a_n}$ for all $n$, which contradicts the assumption that $(a_n)$ was not bounded.
		      		      		                  
		\item $(a_n) = (0,0,0, \dots)$, $(b_n) = (1,2,3, \dots)$.
	\end{enumerate}
			        
	\exc{2.3.8}
			    
	\begin{enumerate}
		\item Assume $(x_n) \rightarrow x$. First, we use induction to show that \begin{equation*}
		      \lim (x_n^k) = x ^ k
		\end{equation*}
		for all natural $k$. The case $k = 1$ is trivial, so we assume the equality holds for $k$ and seek to show that it also holds for $k+1$. Applying the Algebraic Limit Theorem, we have $\lim(x_n^{k+1}) = \lim(x_n^k x_n) = x^k x = x^{k+1}$, as we wanted to show.
						            
		Now, let $p$ be a polynomial. We can write \begin{equation*}
		p(z) = \sum\limits_{i=0}^k a_i z^i
		\end{equation*}
		for every real $z$, some natural $k$ and a sequence of real numbers $(a_i)$. Then, we can use induction and the Algebraic Limit Theorem very similarly to the previous paragraph to see that \begin{equation*}
		\lim (p(x_n)) = \sum\limits_{i=0}^k a_i \lim(x_n^i) =\sum\limits_{i=0}^k a_i x^i = p(x)
		\end{equation*}
		therefore $p(x_n) \rightarrow p(x)$.
						            
		\item Let $(x_n)$ be the sequence where $x_n = 1/n$ for all natural $n$, and $f : {x_1, x_2, \dots} \rightarrow {0, 1}$ be such that \begin{equation*}
		      f(z) = \begin{cases}
		      0 & z \neq 0 \\
		      1 & z = 0
		\end{cases}
		\end{equation*}
						            
		Then, $\lim f(x_n) = \lim (0) = 0$ and $f(\lim x_n) = f(0) = 1$. Therefore, $\lim (f(x_n)) \neq f(\lim(x_n))$.
	\end{enumerate}
			        
	\exc{2.3.9}
	\begin{enumerate}
		\item Let $\epsilon > 0$ be arbitrary. Since $(a_n)$ is bounded, there is a real number $M \neq 0$ such that $M \geq \abs{a_n}$ for all natural $n$. Also, since $(b_n) \rightarrow 0$, there is a natural $N$ such that $\abs{b_n} \leq \epsilon/M$ for all $n \geq N$. Then, for all $n \geq N$ we have $\abs{a_n b_n} = \abs{a_n} \abs{b_n} \leq M \abs{b_n} < \epsilon$, so $(a_n b_n) \rightarrow 0$.
		      		      		      	        
		      We cannot use the Algebraic Limit Theorem to prove this since $(a_n)$ might not be convergent, even though it is bounded.
		      		      		      	        
		\item If $(b_n) \rightarrow b \neq 0$, then $(a_b b_n)$ converges $\iff (a_n)$ converges. The converse direction is a special case of the statement of the Algebraic Limit Theorem. In the other direction, notice that $a_n = (a_n b_n) / b_n$, so, $\lim ((a_n b_n) / b_n) = \lim(a_n b_n) / b = \lim (a_n)$, therefore $(a_n)$ converges.
		      		      		      	        
		\item Assume $\lim (a_n) = 0$ and $\lim (b_n) = b$. Since $(a_n)$ is convergent it is also bounded, therefore (a) guarantees that $\lim(a_b b_n) = 0 = \lim(a_n) \lim(b_n)$.
	\end{enumerate}
				    
	\exc{2.3.10}
	\begin{enumerate}
		\item $a_n = n$ and $b_n = n$ for all $n \in \N$ is a counterexample, since $\lim (a_n - b_n) = 0$ and neither $\lim(a_n)$ nor $\lim(b_n)$ exist.
		      		      		      	     
		\item Let $\epsilon > 0$ be arbitrary. Choose a natural number $N$ such that $\abs{b_n - b} < \epsilon$ for all $n \geq N$. Since $\abs{b_n} - \abs{b} \leq \abs{b_n - b}$ and $\abs{b} - \abs{b_n} \leq \abs{b_n - b}$, we have $\abs{\abs{b_n}-\abs{b}} \leq \abs{b_n-b} < \epsilon$ for all $n \geq N$ so $\abs{b_n} \rightarrow \abs{b}$.
		      		      		      	     
		\item By Theorem 2.3.3, $\lim ((b_n-a_n) + a_n) = \lim(b_n) = \lim (b_n - a_n) + \lim(a_n) = a$.
		      		      		      	     
		\item Let $\epsilon > 0$ be arbitrary. Choose an $N \in \N$ such that $\abs{a_n} < \epsilon$ for all $n \geq N$. Then, $0 \leq \abs{b_n-b} \leq a_n = \abs{a_n} < \epsilon$, so $\abs{b_n - b} < \epsilon$ for all $n \geq N$, therefore $(b_n) \rightarrow b$.
	\end{enumerate}
				 
	\exc{2.3.11}
	\begin{enumerate}
		\item Assume $(x_n) \rightarrow x$ and let $\epsilon > 0$ be arbitrary. Notice that \begin{equation*}
		      \abs{y_n-x} = \abs{\left (\sum\limits_{k=1}^{n} \frac{x_k}{n}\right)-x} = \abs{\frac{1}{n} \sum\limits_{k=1}^{n} x_k-x} \leq \frac{1}{n} \sum\limits_{k=1}^n \abs{x_k-x}
		\end{equation*} for all natural $n$. Choose $N_1 \in \N$ such that $\abs{x_n-x} < \epsilon/4$ for all natural $n \geq N$. Then, we can write \begin{equation*}
		\abs{y_n-x} \leq \sum\limits_{k=1}^{N_1-1} \frac{\abs{x_k-x}}{n} + \sum\limits_{k=N_1}^{n} \frac{\abs{x_k-x}}{n} ~ .
		\end{equation*} Now, use the fact that the first term converges to $0$ to choose a natural number $N_2$ such that \begin{equation*}
		\sum\limits_{k=1}^{N_1-1} \frac{\abs{x_k-x}}{n} < \frac{\epsilon}{2}
		\end{equation*} for all $n \geq N_2$. By letting $N := \max{N_1, N_2}$, we can write \begin{equation*}
		\abs{y_n-x} \leq \frac{\epsilon}{2} + \sum\limits_{k=N_1}^{n} \frac{\abs{x_k-x}}{n} \leq \frac{\epsilon}{2} + \sum\limits_{k=N_1}^{n} \frac{\epsilon}{4n}
		\end{equation*} for all $n \geq N$. Notice that \begin{equation*}
		\sum\limits_{k=N_1}^{n} \frac{\epsilon}{4n} = \frac{n-N_1+1}{n} \cdot \frac{\epsilon}{4}
		\end{equation*} and, since $(n-N_1 + 1) / n < 2$ for all $n \geq N_1$, \begin{equation*}
		\sum\limits_{k=N_1}^{n} \frac{\epsilon}{4n} < 2 \cdot \frac{\epsilon}{4} = \frac{\epsilon}{2} ~.
		\end{equation*} Finally, \begin{equation*}
		\abs{y_n-x} \leq \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
		\end{equation*} for all $n \geq N$, which means $(y_n) \rightarrow (x_n)$.
							     
		\item If for all naturals $n$ \begin{equation*}
		      x_n := \begin{cases}
		      0 & n \text{ is odd} \\
		      1 & n \text{ is even} \text{ ,}
		\end{cases}
		\end{equation*} then it is not hard to see that \begin{equation*}
		y_n = \begin{cases}
		\frac{n-1}{2n} & n \text{ is odd} \\
		\frac{1}{2} & n \text{ is even} ~ .
		\end{cases}
		\end{equation*} Therefore, $(y_n)$ is the "shuffled" sequence of $a_n = (n-1)/(2n)$ and $b_n = 1/2$, in the sense of Exercise 2.3.5. Notice that $\lim((n-1)/(2n)) = \lim(1/2 - 1/n) = 1/2 = \lim(a_n) = \lim(b_n)$, and by what was shown on Exercise 2.3.5 $(y_n)$ must converge, even though $(x_n)$ diverges.
	\end{enumerate}
				 
	\exc{2.3.12}
				 
	\begin{enumerate}
		\item True. For every $b \in B$ and every $n \in \N$ we have $a_n \geq B$, which implies $a \geq b$, by the Order Limit Theorem.
		      		      		      	     
		\item First, we show that every $a_n$ being in the complement of $(0, 1)$ implies the existence of some $N \in \N$ such that $a_n \geq 1$ for all $n \geq N$ or $a_n \leq 0$ for all $n \geq N$, as long as $a \neq 0$. Assume $a > 0$. Then, there is some $N \in \N$ such that $\abs{a-a_n} < a/2$. Now, assume for contradiction that there is some $m \geq N$ such that $a_m \leq 0$. Then, $\abs{a-a_m} = a-a_m < a/2$, which means $a_m > a/2 > 0$, a contradiction. For the case $a < 0$, choose $N \in \N$ such that $\abs{a_n-a} < 1-a$ for all $n \geq N$. Assume for contradiction that there is some $m \geq N$ such that $a_m \geq 1$. Then, $\abs{a_m-a} = a_m-a < 1-a$, which means $a_m < 1$, a contradiction. 
		      		      		      	     
		      If $a = 0$, then $a$ is already in the complement of $(0, 1)$, so assume $a \neq 0$. If $a > 0$, we have shown that there is some $n \geq N$ such that all $a_n \geq 1$, which, by a slightly modified version of the Order Limit Theorem, implies $a \geq 1$, so $a$ is in the complement of $(0, 1)$, and a similar argument follows when $a < 0$.
		      		      		      	     
		\item We have already shown that given any two real numbers, there is a rational number strictly between them. Therefore, we can make the sequence $(a_n)$ by choosing each $a_n$ such that $\sqrt{2} < a_n < \sqrt{2} + 1/n$ and $a_n \in \Q$. Every $a_n$ is rational by construction, but we claim $(a_n) \rightarrow \sqrt{2}$, which is irrational. To see this, let $\epsilon > 0$ be arbitrary and choose $N \in \N$ such that $N > 1/\epsilon$. Then, for every $n \geq N$, $a_n < \sqrt{2}+1/n < \sqrt{2}+\epsilon$, therefore $0 < a_n-\sqrt{2} = \abs{a_n-\sqrt{2}} < \epsilon$ for all $n \geq N$, so $(a_n) \rightarrow \sqrt{2}.$
	\end{enumerate} 
				 
	\item \begin{shortlemma} \label{CauchyBounded}
	      Every Cauchy sequence is bounded.
	\end{shortlemma}
	\begin{proof}
		Let $(a_n)$ be a Cauchy sequence. Choose $N \in \N$ such that
		
		\noindent $\abs{a_n-a_m} < 1$ for all $n \geq N$. Then, $\abs{a_n} - \abs{a_N} \leq \abs{a_n-a_N} < 1$, therefore $\abs{a_n} < \abs{a_N} + 1$ for all $n \geq N$. Since every finite sequence is bounded, there is some real number $M_1$ such that $M_1 \geq \abs{a_n}$ for every $n < N$, so if we define $M := \max{M_1, \abs{a_N} + 1}$ we will have $M \geq \abs{a_n}$ for every natural $n$, therefore $(a_n)$ is bounded by $M$.  
	\end{proof}
	\item \begin{shorttheorem} \label{thm_convergCauchy}
        Every convergent sequence is Cauchy.
	\end{shorttheorem}
				 
	\begin{proof}
		First, assume $(a_n) \rightarrow L$ for some $L \in \R$. Let $\epsilon > 0$ be arbitrary and choose $N \in \N$ such that $\abs{a_t - L} \leq \epsilon/2$ for all $t \geq N$. Then, for all $n,m \geq N$ we have $\abs{a_n - a_m} = \abs{a_n - L + L - a_m} \leq \abs{a_n-L} + \abs{a_m - L} < \epsilon/2 + \epsilon/2 = \epsilon$, so $(a_n)$ is Cauchy.
	\end{proof}
			
	\exc{2.3.13} \begin{enumerate}
	\item Since \begin{equation*}
	      a_{mn} = \frac{1}{1+\frac{n}{m}} ~ ,
	\end{equation*} we have \begin{equation*}
	\lim_{n \to \infty} \left(\lim_{m \to \infty} a_{mn} \right) = \lim_{n \to \infty} 1 = 1,
	\end{equation*} whereas \begin{equation*}
	\lim_{m \to \infty} \left(\lim_{n \to \infty} a_{mn} \right) = \lim_{m \to \infty} 0 = 0.
	\end{equation*}
			    
	\item \begin{enumerate}
	\item Let $a_{mn} = 1/(m+n)$. We claim that \begin{equation*}
	      \lim_{m, n \to \infty} a_{mn} = 0.
	\end{equation*} \lep \space and choose $N \in \N$ such that $N > 1/(2\epsilon)$. Then, \begin{equation*}
	\abs{\frac{1}{m+n} - 0} = \frac{1}{m+n} < \epsilon
	\end{equation*} for all $m,n \geq N$. It is easy to see that both the iterated limits are also equal to $0$.
			        
	\item Let $a_{mn}= mn/(m^2+n^2)$. Then, $a_{mn} = m/(m^2/n+n)$, which is easily seen to equal zero when taking the limit as $n \to \infty$ (just set $N > m/\epsilon$). By symmetry, both the iterated limits are equal to $0$. Now, assume for contradiction that $\lim_{m,n \to \infty} a_{mn}= a$ for some $a \in \R$. Then, there is a $N \in \N$ such that $\abs{a_{mn} - a}  < 1/20$ for all $m,n \geq N$. In particular, $\abs{a_{NN}-a} = \abs{1/2-a} < 1/20$ and $\abs{a_{2N N} - a} = \abs{2/5-a} < 1/20$. Summing these equations and applying the triangle inequality we have $\abs{1/2-2/5}\leq \abs{1/2-a}+\abs{2/5-a}<1/10$, which simplifies to $1/10 < 1/10$, a contradiction. In summary, both the iterated limits are zero but $\lim_{m,n \to \infty} a_{mn}$ does not exist.
\end{enumerate}
	    
    \item \begin{equation*}
    a_{m n} = \begin{cases}
    0 & n,m \geq 2 \\
    n & m = 1 \\
    m & n = 1 \\
    1 & m = n = 1
    \end{cases}
    \end{equation*}
    	        
    \item \lep \space and choose $N \in \N$ such that for all $m,n \geq N$, $\abs{b_n-a_{mn}}, \abs{a-a_{mn}} < \epsilon/2$. Then, $\abs{b_m-a} = \abs{b_m-a_{mn} + a_{mn} - a}$ 
    
    \noindent $\leq \abs{b_m-a_{mn}} + \abs{a_{mn}-a} < \epsilon$, therefore $(b_m) \to a$.
    \end{enumerate}
    	
    \exc{2.4.1}
    \begin{enumerate}
    	\item First we use induction to show that $x_{n+1} \leq x_n$ for all $n \in \N$. For the base case, $x_2 = 1 \leq x_1 = 3$. Now, assume inductively that $x_{n+1} \leq x_n$. Then, \begin{gather*}
    	      4-x_{n+1} \geq 4-x_n \\ 
    	      \frac{1}{4-x_{n+1}} \leq \frac{1}{4-x_n} \\ 
    	      x_{n+2} \leq x_{n+1} ~ .
    	\end{gather*}
    	Next, we use induction again to prove that $x_n \geq 0$ for all $n \in \N$. The base case is $x_1 = 3 \geq 0$. Now, assume $x_n \geq 0$. Then, \begin{gather*}
    	4-x_n \leq - 4 < 4 \\ 
    	x_{n+1} = \frac{1}{4-x_n} > \frac{1}{4} \geq 0.
    	\end{gather*} We will also show that $3 \geq \abs{x_n} = x_n$ for all $n \in \N$ by induction. The base case is trivial so we assume $3 \geq x_n$. Then, \begin{gather*}
    	\frac{1}{3} \leq 4-3 \leq 4-x_n \\
    	3 \geq \frac{1}{4-x_n} = x_{n+1}.
    	\end{gather*} Therefore the sequence is monotone and bounded, so the Monotone Convergence Theorem applies and $(x_n)$ converges.
    		    	
    	\item The sequence $(x_{n+1})$ is simply $(x_n)$ "shifted over" by one, so if $(x_n)$ eventually gets arbitrarily close to some real number, so will $(x_{n+1})$.
    	
    	\item Let $x := \lim x_n$. Then, applying the Algebraic Limit Theorem, we have \begin{equation*}
    	    x = \lim \frac{1}{4-x_n} = \frac{1}{4-\lim x_n} = \frac{1}{4-x}.
    	\end{equation*} There are two real numbers that satisfy this equation, namely $2 \pm \sqrt{3}$. Since $3 > 1$, we have $\sqrt{3} > 1$, therefore $2 + \sqrt{3} > 3$. But, since for every $x_n$ we have $x_n \leq 3$, the Order Limit Theorem guarantees that $x \leq 3 < 2 + \sqrt{3}$, so $x$ must be $2 - \sqrt{3}$. 
    \end{enumerate} 
    
    \exc{2.4.2} 
    \begin{enumerate}
        \item  The problem with the argument is that $\lim y_n$ does not exist, since $(y_n) = (1,2,1,2 \dots)$ which does not converge.
        
        \item Yes, since $(y_n)$ converges. To see that, first we show that $y_n \leq 4$ for all $n \in \N$ with induction. After verifying the base case, assume $y_n \leq 4$. Now, \begin{gather*}
            \frac{1}{y_n} \geq \frac{1}{4} \\ 
            3 - \frac{1}{y_n} = y_{n+1} \leq 3-\frac{1}{4} \leq 4.
        \end{gather*} It is also easy to verify with induction that $(y_n)$ is increasing, therefore it must converge, so the strategy in (a) can be applied to compute the limit of the sequence.
    \end{enumerate}
    
    \exc{2.4.3} 
    \begin{enumerate}
        \item The given sequence can be defined recursively as $a_1 = \sqrt{2}$ and
        \begin{equation*}
            a_{n+1} = \sqrt{2 + a_n}.
        \end{equation*} Induction can be easily used to verify that the sequence is increasing and bounded above, therefore it converges to some real number $a$. Using the strategy presented in the previous exercise, we have $a = \sqrt{2 + a}$, and $a=2$ is the only solution to this equation, therefore $(a_n) \to 2$.
        
        \item The given sequence can be defined recursively as $a_1 = \sqrt{2}$ and
        \begin{equation*}
            a_{n+1} = \sqrt{2 a_n}.
        \end{equation*} Induction can be easily used to verify that the sequence is increasing and bounded above, therefore it converges to some real number $a$. Using the strategy presented in the previous exercise, we have $a = \sqrt{2a}$, which has solutions $a = 1$ or $a = 0$, but it is easy to see that $a_n \geq 1$ for all $n$, so the Order Limit Theorem guarantees that $a \geq 1$, therefore $a = 1$.
    \end{enumerate}
    
    \exc{2.4.4}
    \begin{enumerate}
        \item Assume for contradiction that the naturals are bounded above. Then, the sequence $a_n = n$ is bounded above and increasing, so it converges to some real number $x$, by the Monotone Convergence Theorem. Then, there is some natural $N$ such that $\abs{n-x}, \abs{n+2-x} < 1$ for all $n \geq N$. Adding the inequalities and using the Triangle Inequality we have $\abs{(n+2)-n} \leq \abs{n-x}+\abs{n+2-x} < 2$, therefore $\abs{2}=2 < 2$, a contradiction. Thus, for every real number $x$ there is some $n \in \N$ such that $n > x$
        
        \item Since $I_{n} \supseteq I_{n+1}$ for every natural $n$, we have $a_{n+1} \geq a_n$ for every $n \in \N$. Also, $b_1$ is an upper bound for $(a_n)$, so the Monotone Convergence Theorem guarantees that $(a_n)$ converges to some real number $a$. Since $a_{n+m} \geq a_n$ for every $n,m \in \N$, we can use the Order Limit Theorem to see that $\lim_{m \to \infty} a_{m+n} = a \geq a_n$ for every $n \in \N$. We also have $a_m \leq b_n$ for every $n, m \in \N$, which also implies $a \leq b_n$. Therefore, $a_n \leq a \leq b_n$ for every natural $n$, so all the $I_n'$s contain $a$, which means their intersection cannot be empty.
    \end{enumerate}
    
    \exc{2.4.5}
    \begin{enumerate}
        \item Since $x_1^2 = 4 \geq 2$ and
        \begin{equation*}
            x_{n+1}^2 = \frac{1}{4}\left(x_n+\frac{2}{x_n}\right)^2 = 
            \frac{1}{4}\left(x_n-\frac{2}{x_n}\right)^2 + 2 \geq 2,
        \end{equation*} $x_n \geq 2$ for all $n \in \N$.
        
        Now, we show that the sequence is decreasing. Let $n \in \N$ be arbitrary. Since $x_n$ is rational, we can write $x_n = a/b$ for $a,b \in \N$, since every $x_n$ is also positive (this is easy to verify with induction). Applying the formula for $x_{n+1}$, we get 
        \begin{equation*}
            x_{n+1} = \frac{a^2+2b^2}{2ab}.
        \end{equation*}
        Then, 
        \begin{equation*}
            x_n \geq x_{n+1} \iff \frac{a}{b} \geq \frac{a^2+2b^2}{2ab} \iff
            \frac{a^2}{b^2} \geq 2 \iff x_n^2 \geq 2,
        \end{equation*} and we have already shown that $x_n^2$ is always greater than or equal to 2, so the sequence decreases.
        
        Since the sequence is monotone and bounded, it must converge to some real $x$. In particular, the Order Limit Theorem guarantees that $x \geq 0$ since every $x_n \geq 0$.
        Then, we can use the fact that $\lim x_{n+1} = \lim x_n$ to get 
        \begin{equation*}
            x = \frac{1}{2} \left(x + \frac{2}{x}\right)
        \end{equation*} which, after simple algebra, means that $x = \pm \sqrt{2}$, but $x$ is nonnegative so $x = \sqrt{2}$.
        
        \item Let $x_1 = c$ for some $c \geq 0$, and define
        \begin{equation*}
            x_{n+1} = \frac{1}{2} \left(x_n + \frac{c}{x_n} \right).
        \end{equation*} To prove that $(x_n) \to \sqrt{c}$, we first show that $x_n^2 \geq c$ and then use this fact to show that $x_n \geq x_{n+1}$ for every $n \in \N$. This implies the convergence of the sequence, in particular to $\sqrt{c}$. All of this is achieved almost identically to the previous exercise simply by switching the appropriate $2'$s with $c'$s. The case $c = 0$ can be easily be verified separately.
    \end{enumerate}
    
    \exc{2.4.6}
    
    \begin{enumerate}
        \item Since both $x$ and $y$ are positive, we can write the following equivalences:
        \begin{equation*}
            \frac{x+y}{2} \geq \sqrt{x y} \iff x^2+2x y+y^2 \geq 4x y \iff  \left(x-y\right)^2 \geq 0.
        \end{equation*} Since any real number squared is nonnegative, the result follows.
        
        \item Notice that it is easy to verify with induction that every $x_n, y_n \geq 0$.
        Since $y_1 \geq x_1$ by assumption and we have shown in the previous exercise that $y_{n+1} = (x_n+y_n)/2 \geq \sqrt{x_n y_n}= x_{n+1}$, it follows that $y_n \geq x_n$ for all $n$. Then, $y_n x_n \geq x_n^2$, which means $\sqrt{y_n x_n} = x_{n+1} \geq x_n$ so $(x_n)$ is increasing. Similarly, $y_n \geq x_n \implies 2y_n \geq x_n + y_n \implies y_n \geq y_{n+1}$, so $(y_n)$ is decreasing. Since $(y_n)$ is also bounded below by $0$, it must converge to some real number $y$. Now, we show that $(x_n)$ is bounded above by $y_1$ with induction. The base case is true by assumption, so we assume that $x_n \leq y_1$. Since $(y_n)$ decreases, $y_n \leq y_1$, therefore $x_n y_n \leq y_1^2$ thus $\sqrt{x_n y_n} = x_{n+1} \leq y_1$ and the induction is complete. Since $(x_n)$ increases and is bounded above, it must converge to some real number $x$. Then, 
        
        \begin{equation*}
            \lim y_{n+1} = y = \lim (\frac{x_n + y_n}{2}) = \frac{x + y}{2}
        \end{equation*} which can only happen when $x = y$.
    \end{enumerate}

    \exc{2.4.7}
    First we prove a useful lemma. \newline
    \begin{shortlemma} \label{lem_2.4.7}
        Let $A$ and $B$ be two bounded nonempty sets of real numbers with $A \subseteq B$. Then, $\sup{A} \leq \sup{B}$ and $\inf{A} \geq \inf{B}$.
    \end{shortlemma}
    
    \begin{proof}
        Let $a \in A$ be arbitrary. Since $A \subseteq B$, it follows that $a \in B$. Then, $\sup B \geq a$, so $\sup B$ is an upper bound for $A$. By the definition of the least upper bound, this means that $\sup {A} \leq \sup{B}$. Also, $\inf{B} \leq a$, so $\inf{B}$ is a lower bound for $A$, and we must have $\inf{A} \geq \inf{B}$. 
    \end{proof}
    \begin{enumerate}
        \item Since $\set{a_k : k \geq n+1} \subseteq \set{a_k : k \geq n}$, it follows from Lemma \ref{lem_2.4.7} that $y_{n+1} \leq y_n$, so $(y_n)$ is decreasing. Also, there is some real number $M$ such that $M \leq a_n$ for every $n \in \N$, since we assumed $(a_n)$ is bounded. Then, the $y_n'$s are upper bounds, we have $y_n \geq a_n \geq M$, so $y_n$ is bounded below by $M$, and it must converge.
        
        \item Use the sequence defined by $x_n = \inf \set{a_k : k \geq n}$ to define 
        \begin{equation*}
            \liminf{a_n} := \lim x_n.
        \end{equation*} We can then show that $(x_n)$ increases using the fact that each next set in the definition of $(x_n)$ is a subset of the previous (just like in the last case) to see that $x_{n+1} \geq x_n$ by Lemma \ref{lem_2.4.7}. Also, $(a_n)$ is bounded above, so $N \geq a_n \geq x_n$ some $N \in \R$ and for every $n$, so $(x_n)$ is also bounded above, which means it must converge.
        
        \item Define the sequences $(x_n), (y_n)$ by 
        \begin{align*}
            A_n &:= \set{a_k : k \geq n} \\
            x_n &:= \inf{A_n} \\
            y_n &:= \sup{A_n}.
        \end{align*} Since $\sup{B} \geq \inf{B}$ for any nonempty bounded set $B$, we have $y_n \geq x_n$ for every natural $n$, so we can apply the Order Limit Theorem to get $\lim y_n \geq \lim x_n$,
        which means that $\limsup{a_n} \geq \liminf{a_n}$.
        
        The sequence defined by $a_n = (0,1,0,1 \dots)$ has $\limsup{a_n} = 1$ and $\liminf{a_n} = 0$, so $\limsup{a_n} > \liminf{a_n}$.
        
        \item First, assume $\lim a_n = a$ and \lep[l]. Then, we use Theorem \ref{thm_convergCauchy} to find $N_1, N_2 \in \N$ such that $\abs{a_n-a} < \epsilon/2$ for all $n \geq N_1$ and $\abs{a_n-a_m} < \epsilon/4$ for all $n, m \geq N_2$. Also, define $N := \max{N_1, N_2}$. Since the $y_n'$s are least upper bounds, for every $n \geq N$, we can find $a_L$ with $L \geq N$ such that $y_n-\epsilon/4 < a_L$, which implies $\abs{y_n-a_L} < \epsilon/4$. Then, 
        \begin{equation*}
            \abs{y_n-a_n} \leq \abs{y_n-a_L} + \abs{a_L-a_n} < \frac{\epsilon}{4} + \frac{\epsilon}{4} = \frac{\epsilon}{2}
        \end{equation*} for all $n \geq N$. Then,
        \begin{equation*}
            \abs{y_n-a} \leq \abs{y_n-a_n} + \abs{a_n-a} < \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon
        \end{equation*} for all $n \geq N$, which means that $\lim y_n = \liminf{a_n} = a$. The proof that $\liminf{a_n} = a$ is almost identical.
        
        For the other direction, assume $\limsup a_n = \liminf a_n = a$ for some $a \in \R$. For every $n$, we know that $x_n \leq a_n \leq y_n$, so we can apply the Squeeze theorem to get $\lim a_n = \lim x_n = \limsup a_n = a$.
    \end{enumerate}
    
    \exc{2.4.8}
    \begin{enumerate}
        \item It is easy to verify with induction that the sequence of partial sums is \begin{equation*}
            s_n = 1-\frac{1}{2^n}.
        \end{equation*} We can then apply the Algebraic Limit Theorem together with the fact that $(1/2^n) \to 0$ to get $(s_n) \to 1$. Therefore the sum converges, in particular
        \begin{equation*}
            \sum_{n=1}^\infty \frac{1}{2^n} = 1.
        \end{equation*}
        
        \item Using the fact that 
        \begin{equation*}
            \frac{1}{n(n+1)} = \frac{1}{n} - \frac{1}{n+1},
        \end{equation*} it is easy to see with induction that $s_n = 1 - 1/(n+1)$, which clearly converges to $1$, so 
        \begin{equation*}
            \sum_{n=1}^{\infty} \frac{1}{n(n+1)} = 1.
        \end{equation*}
        
        \item Since 
        \begin{equation*}
            \log{\frac{n+1}{n}}= \log{n+1} - \log{n},
        \end{equation*} it is easy to verify that $s_n = \log{n+1}$. Now, assume for contradiction that there is some $M \in \R$ such that $M \geq s_n$ for all $n \in \N$. Then, we can choose a natural number $N$ such that $N > e^M - 1$, but this implies $\log{N+1} = s_N > M$, a contradiction. Since $(s_n)$ is not bounded above, it must diverge, and so does the corresponding sum.
    \end{enumerate}
    
    \exc{2.4.9}
    Let $(t_n)$ be the partial sum sequence of \begin{equation*}
        \sum_{n=0}^\infty 2^n b_{2^n},
    \end{equation*} which diverges by assumption. Notice that 
    \begin{align*}
        s_{2^k} &= b_1 + b_2 + (b_3 + b_4) + \dots + (b_{2^{k-1}+1} + \dots + b_{2^k}) \\
        &\geq b_1 + b_2 + (b_4 + b_4) + \dots + (b_{2^k} + \dots + b_{2^k}) \\
        &= b_1 + b_2 + 2b_4 + 4b_8 + \dots + 2^{k-1}b_{2^k} \\
        &= \frac{b_1}{2} + \frac{b_1 + 2b_2 + 8b_8 + \dots + 2^k b_{2^k}}{2} \\
        &= \frac{b_1}{2} + \frac{t_k}{2} \\
        &\geq \frac{t_k}{2},
    \end{align*} which is unbounded. Thus, $(s_n)$ diverges and so does $\sum_{n=1}^\infty b_n.$
    
    \exc{2.4.10}
    \begin{enumerate}
        \item Induction shows that the partial product sequence is given by $p_m = m+1$, which diverges. The first few terms of the partial product sequence when $a_n = 1/n^2$ are 
        \begin{equation*}
            2, 2.5, 2.77\dots, 2.95\dots,3.069\dots
        \end{equation*} with the $10000$th term being around $3.67$, not much bigger than the $6$th term. With this in mind, we conjecture that this sequence converges.
        
        \item Let $(s_n),(p_n)$ be the sequences of the partial sums and partial products respectively. First, we show by induction that $p_n \geq 1$ and $p_n \geq s_n$ for all $n \in \N$. The base case is $1+a_1 \geq 1$ and $(1+a_1) \geq a_1$, which are both true since $a_n \geq 0$. Now, assume $p_n \geq 1$ and $p_n \geq s_n$. Then, $p_{n+1} = p_n + p_{n}a_{n+1} \geq 1$ since $a_{n+1} \geq 0$. Also, $p_{n+1}-s_{n+1} = p_n-s_n + (p_{n}-1)a_{n+1}$. Since $p_n - s_n \geq 0$ and $p_n -1 \geq 0$, $p_{n+1}-s_{n+1} \geq 0$, as we wanted to show.
        
        Thus, if $(s_n)$ diverges it is not bounded, which means $(p_n)$ is also not bounded since $p_n \geq s_n$ for all $n \in \N$. Therefore $(p_n)$ must also diverge in this case.
        
        For the other direction, assume $(s_n)$ converges. Then, we can use the inequality given in the exercise to get 
        \begin{equation*}
            p_m = \prod_{n=1}^m (1+a_n) \leq \prod_{n=1}^m 3^{a_n} 
            = 3^{\sum\limits_{n=1}^m a_n} = 3^{s_m}.
        \end{equation*} Since $(s_n)$ converges, there is some $M \in \R$ such that $s_m \leq M$ for all $m \in \N$. Then, $p_m \leq 3^{s_m}\leq 3^M$, which means $(p_m)$ is bounded. Since it is also increasing (every term in the product is greater than or equal to $1$), it must converge.
    \end{enumerate}
    
    \exc{2.5.1}
    \begin{enumerate}
        \item This is not possible. If a sequence $(a_n)$ has a subsequence $(b_n)$ which is bounded, then $(b_n)$ must have a subsequence $(c_n)$ which converges, by the Bolzano-Weierstrass Theorem. Since $(b_n)$ is a subsequence of $(a_n)$, $(c_n)$ is also a subsequence of $(a_n)$.
        
        \item $(0.1,0.9,0.01,0.99,0.001,0.999, \dots)$.
        
        \item \begin{equation*}
            (1, 1, \frac{1}{2}, 1, \frac{1}{2}, \frac{1}{3}, 1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, 1, \dots)
        \end{equation*}
        
        \item This is not possible. Let $(a_n)$ be a sequence that contains a subsequence converging to $1/m$ for every $m \in \N$. We will show that $(a_n)$ must have a subsequence which converges to zero. \lep. Then, $(a_n)$ must have infinitely many elements which are in the $\epsilon/2$-neighborhood of $1/1$. Pick one of those elements and call it $a_{n_1}$. In general, $(a_n)$ has infinitely many members which are $\epsilon/2$-close to $1/(k+1)$, so we can pick one called $a_{n_{k+1}}$ with $n_{k+1} > n_k$. By the construction of $(a_{n_k})$, it follows that $\abs{a_{n_k} - 1/k} < \epsilon/2$ for all $k \in \N$. Now, choose $N \in \N$ such that $N > 2/\epsilon$ and notice that $k \geq N \implies \abs{1/k} < \epsilon/2$. Next, for all $k \geq N$ we have that $\abs{a_{n_k}-0} \leq \abs{a_{n_k}-1/k}+\abs{1/k} < \epsilon/2 + \epsilon/2 = \epsilon$, so $(a_{n_k}) \to 0$, which is outside the set mentioned in the exercise.
    \end{enumerate}
    
    \exc{2.5.2}
    \begin{enumerate}
        \item True. Since every proper subsequence converges, the subsequence containing all but the first element also converges, but this clearly implies the convergence of the whole sequence.
        
        \item True. By Theorem 2.5.2, if $(x_n)$ converges than every one of its subsequences also converge, so a sequence that contains a divergent subsequence cannot converge.
        
        \item True. Let $(x_n)$ be bounded and divergent. By Theorem 2.5.5, there is a subsequence $(x_{n_k})$ which converges to some real number $x$. Since $(x_n)$ diverges, there is a $\epsilon_0 > 0$ such that for all $N \in \N$ there exists some $n \geq N$ such that $\abs{x_n-x} \geq \epsilon_0$. In particular, let $n_1 \geq 1$ be such that $\abs{x_{n_1}-x} \geq \epsilon_0$, and let $n_{k+1} \geq n_k$ be such that $\abs{x_{n_{k+1}} - x} \geq \epsilon_0$. Since $(x_n)$ is bounded, so is the subsequence $(x_{n_k})$, therefore $(x_{n_k})$ must a have subsequence $(y_n)$ which converges to some real number $y$. However, none of the terms in $(y_n)$ are in the $\epsilon_0$-neighborhood of $x$, so $(y_n)$ cannot converge to $x$, which means $x \neq y$. Thus, we have found two subsequences of $(x_n)$ which converge to different limits.
        
        \item True. Assume $(x_n)$ is increasing and contains $(x_{n_k})$, which is a convergent subsequence. Since $(x_{n_k})$ converges, it must be bounded above by some real number $M$. Now, assume for contradiction that there is some $n \in \N$ such that $x_n > M$. Now, pick some $n_k$ which is greater than $n$. Since $(x_n)$ is increasing, it follows that $x_{n_k} \geq x_n > M$, so $(x_{n_k})$ is not bounded above by $M$, a contradiction. Hence, $(x_n)$ must also be bounded above by $M$, so the Monotone Convergence Theorem guarantees that it also converges. A very similar proof follows if $(x_n)$ is decreasing.
    \end{enumerate}
    
    \exc{2.5.3}
    \begin{enumerate}
        \item Let $(b_n)$ be a sequence of natural numbers with $b_1 = 1$ that is strictly increasing ($b_1<b_2<b_3 \dots$). We will use sequences of this kind to define a different grouping of the sum of a sequence $(a_n)$. We define the regrouping of $(a_n)$ under $(b_n)$ as 
        \begin{equation*}
            \sum_{i=1}^\infty \sum_{n=b_i}^{b_{i+1}-1} a_n = (a_1 + a_2 + \dots a_{b_{2}-1}) + (a_{b_2} + a_{b_2 + 1} + \dots + a_{b_3-1}) + \dots \, .
        \end{equation*} We need to show that if $\sum_{n=1}^\infty (a_n)$ converges to $L$, then so does the regrouping of $(a_n)$ under any $(b_n)$. First, we will show that $p_m = s_{b_{m+1}-1}$ for all $m \in \N$, where $(s_m)$ and $(p_m)$ are the partial sums of $(a_n)$ and the regrouping of $(a_n)$ under $(b_n)$. To be clear,
        \begin{align*}
            s_m &= \sum_{n=1}^m a_n \\
            p_m &= \sum_{i=1}^m \sum_{n=b_i}^{b_{i+1}-1} a_n.
        \end{align*}
        
        The base case is 
        \begin{equation*}
            p_1 = \sum_{i=1}^1 \sum_{n=b_i}^{b_{i+1}-1} a_n = \sum_{n=b_1}^{b_{2}-1} a_n = \sum_{n=1}^{b_{2}-1} a_n = s_{b_{2}-1}.
        \end{equation*} Now, assume $p_m = s_{b_{m+1}-1}$. Then, 
        \begin{align*}
            p_{m+1} &= \sum_{n=b_{m+1}}^{b_{m+2}-1} a_n + \sum_{i=1}^m \sum_{n=b_i}^{b_{i+1}-1} a_n = \sum_{n=b_{m+1}}^{b_{m+2}-1} a_n + s_{b_{m+1}-1} \\ & = \sum_{n=b_{m+1}}^{b_{m+2}-1} a_n + \sum_{n=1}^{b_{m+1}-1} a_n = \sum_{n=1}^{b_{m+2}-1} a_n = s_{b_{m+2}-1}, 
        \end{align*} which completes the induction. This means that $(p_m)$ is a subsequence of $(s_m)$ so it must converge to the same value as $(s_m)$ by Theorem 2.5.2. Then,
        \begin{align*}
             \lim (p_m) &= \sum_{i=1}^\infty \sum_{n=b_i}^{b_{i+1}-1} a_n = L \\
             &= (a_1 + a_2 + \dots a_{b_{2}-1}) + (a_{b_2} + a_{b_2 + 1} + \dots + a_{b_3-1}) + \dots \\ &= a_1+a_2+a_3+\dots
        \end{align*} which is what we wanted to show.
        
        \item The proof in $(a)$ doesn't apply because the series being summed over in that example, was $a_n = (-1)^n$, which does not converge, since one of its subsequences converges to $1$ and another to $-1$.
    \end{enumerate}
    
    \exc{2.5.4}
    \begin{enumerate}
        \item Let $A$ be a nonempty set of real numbers that is bounded above by $M$. Since $A \neq \emptyset$, we can choose some $a_1 \in A$. Notice that $a_1 \leq M$, and if $a_1 = M$ it would be the least upper bound of $A$, so we consider only the case $a_1 < M$. Then, the interval $I_1 =[a_1, M]$ has a nonzero length. Bisect the interval into $l_1 = [a_1, (a_1+M)/2]$ and $r_1 = [(a_1+M)/2, M]$. If none of the elements of $A$ is in $r_1$, let $I_2 = l_1$, otherwise set $I_2 = r_1$. In general, we construct the interval $I_{k+1}$ by bisecting $I_k = [p, q]$ into $r_k=[p, (p+q)/2]$ and $l_k = [(p+q)/2, q]$, setting $I_{k+1}$ to $r_k$ if some element of $A$ is in $r_k$ and setting it to $l_k$ otherwise. Notice that every $I_k$ must contain at least one element from $A$. Also, the length of $I_k$ is $2 (M-a_1)/2^k$ (this is easily shown by induction), so it gets arbitrarily small, since we assumed $(1/2^k) \to 0$. By the Nested Interval Property, we can pick an $x \in \R$ that is contained by every $I_k$. We claim that $x = \sup A$.
        
        To see that $x$ is an upper bound, assume for contradiction that there is some $a \in A$ with $a > x$. If $a \notin I_1$ then $x \geq a_1 > a$, so we can assume $a \in I_1$. Then, $a-x = \epsilon_0$, where $\epsilon_0 > 0$. We know that there is some $I_k$ whose length is less than $\epsilon_0$, which means it cannot contain both $a$ and $x$, but, since every $I_k$ contains $x$, this means $a \notin I_k$. But, since $a \in I_1$, there must have been some point $n$ where $a \in I_n$ and $a \notin I_{n+1}$. This can only happen when $I_{n+1}$ is the right interval $r_n$ of $I_n$ (otherwise $a$ would be in $I_{n+1}$). Since $a \in l_n$ and $x \in I_{n+1} = r_n$, we must have $x \geq a$, a contradiction. Thus, $x \geq a$, so $x$ is an upper bound for $A$.
        
        We show $x$ is the least upper bound of $A$ using Lemma 1.3.8. \lep. We are looking for some element of $A$ which is greater than $x-\epsilon$. Since the length of the $I_k'$s is eventually less than $\epsilon$, there must be some $I_k$ which does not contain $x-\epsilon$, since every $I_k$ contains $x$. Notice that if $x-\epsilon \notin I_1$ then $a_1 > x-\epsilon$ and we would be done, so we can assume $x-\epsilon \in I_1$. By the same reason as before, this means there is some $n$ where $x-\epsilon \in I_n$ and $x-\epsilon \in I_{n+1}$, which again means $r_n$ contains some element $a \in A$, which must then be greater than $x-\epsilon$. Thus, every nonempty set that is bounded above must have a least upper bound.
        
        We had to assume that $(1/2^n) \to 0$ because the Archimedean Property was used to show this, but we used the Axiom of Completeness to prove the Archimedean Property, so not making this assumption would make the proof just given circular.
    \end{enumerate}
    
    \exc{2.5.5}
    Since no two subsequences of $(a_n)$ converge to different limits, $(a_n)$ cannot diverge, since that would be in contradiction with what we have shown in Exercise 2.5.2 (c). Then, Theorem 2.5.2 guarantees that $(a_n)$ converges to the same limit as its subsequences, namely $a$.
    
    \exc{2.5.6}
    If $b = 0$, then clearly $\lim(0^{1/n}) = 0$, and the case $b=1$ also trivially converges to $1$. Now, assume $0 < b < 1$. It is not hard to see that $(b^{1/n})$ is strictly increasing and bounded above by one, so it must converge to some real number $l$. But Theorem 2.5.2 guarantees that the subsequence $(b^{1/(2n)})$ must also converge to $l$. Also, by the results in Exercise 2.3.1, $(b^{1/(2n)}) = (\sqrt{b^{1/n}}) \to \sqrt{l}$. Therefore, $l = \sqrt{l}$, which means $l = 1$, since it must be greater than 0.
    
    If $b > 1$, it is easy to see that the sequence $(b^{1/n})$ is strictly decreasing and bounded below by one, so it must converge, and we can use the same strategy as before to get $(b^{1/n}) \to 1$.
    
    \exc{2.5.7}
     We have already shown the forward direction to be true for every case except $-1 < b < 0$, so assume $b$ is in this interval. Notice that $1 > b^2 > 0$, so the sequence $((b^2)^n)$ must converge to $0$, as shown in Example 2.5.3. Also, the subsequence $(b^{2n-1}) = ((b^2)^n/b)$ must also converge to $0$ by the The Algebraic Limit Theorem and Example 2.5.3. Then, $(b_n)$ is the shuffled sequence of $(b^{2n})$ and $(b^{2n-1})$, so Exercise 2.3.5 guarantees $(b_n) \to 0$.
    
    Now, assume $\abs{b} \geq 1$. If $b = 1$, clearly $(b_n) \to 1 \neq 0$. If $b = -1$, the sequence $(b^n) = (-1,1-1,\dots)$ has a subsequence which converges to $1$ and another with converges to $-1$, so $(b_n)$ diverges in this case. Next, assume $b > 1$. The, we can write $b := 1 + a$ for some $a > 0$. But, it is clear that 
    \begin{equation*}
        \sum_{n=1}^\infty a = a + a + a + \dots
    \end{equation*} diverges, which means
    \begin{equation} \label{exponentProd}
        \prod_{n=1}^\infty (1+a)
    \end{equation} also diverges, as we have shown in Exercise 2.4.10. But the partial products of (\ref{exponentProd}) form the sequence $((1+a)^1, (1+a)^2, \dots) = (b^n)$, so $(b^n)$ must diverge. The last case is $b < -1$. In this case, since $b^2 > 1$, the sequence $((b^2)^n)$ diverges, but this is a subsequence of $(b^n)$, so $(b_n)$ also diverges.
    
    \exc{2.5.8}
    \begin{enumerate}
        \item 
        0 peak terms: $(1,2,3,4 \dots)$.
        
        1 peak term: \space $(1,0.9,0.99,0.999 \dots)$.
        
        2 peak terms: $(2,1,0.9,0.99, 0.999, \dots)$.
        
        Infinitely many peak terms but not monotone: $(-1)^n$.
        
        \item First, assume a sequence $(a_n)$ has infinitely many peak terms. Choose one of them and label it $a_{n_1}$. In general, in order to construct $a_{n_{k+1}}$, choose a peak term $a_n$ with $n \geq n_k$ and set $a_{n_{k+1}} = a_n$. This can always be done, since the sequence will never stop having peak terms. Since every $a_{n_k}$ is a peak term, we must have $a_{n_k} \geq a_{n_{k+1}}$ for every $k \in \N$, so $(a_{n_k})$ is a decreasing, therefore monotone, subsequence of $(a_n)$.
        
        Now, assume there are not infinitely many peak terms in $(a_n)$. Then, there must be some $N \in \N$ such that $a_n$ is not a peak term for every $n \geq N$. Then, let $a_{n_1} = a_N$. Since $a_{n_1}$ is not a peak term, there must be some $a_{n_2} > a_{n_1}$ with $n_2 > n_1 \geq N$. In general, use the fact that $a_{n_k}$ is not a peak term to choose some $n_{k+1} > n_k$ such that $a_{n_{k+1}} > a_{n_k}$. Then, $a_{n_k}$ is an increasing, therefore monotone, subsequence of $(a_n)$.
        
        Now, consider some bounded sequence $(x_n)$. We have just shown that is must have some monotone subsequence, but this subsequence will also be bounded, so the Monotone Convergence Theorem guarantees it must converge. 
    \end{enumerate}
    
    \exc{2.5.9}
    Since $(a_n)$ is bounded, every term in the sequence is in $[-M, M]$ for some $M \in \R$, so $-M-1 < a_n$ for all the $(a_n)$, hence $-M-1 \in S$. Also, if $x \in S$, then $x \leq M$, otherwise $x$ would be greater than every in $a_n$, which contradicts the fact that $x \in S$. Since $S$ is nonempty and bounded above, we are justified in setting $s := \sup S$.
    
    Now let $k \in \N$. We know that $s-1/k < x$ for some $x \in S$. This means that $s-1/k < a_n$ for infinitely many $a_n$. Also, $s + 1/k \notin S$, therefore there are only finitely many $a_n$ such that $ s + 1/k < a_n$. This means that for every $k \in N$ there must be infinitely many terms of $(a_n)$ such that $s-1/k < a_n \leq s+1/k$. To construct $a_{n_1}$, pick one $a_n$ such that $s-1 < a_n \leq s+1$, and to construct $a_{n_{k+1}}$, choose some $s-1/(k+1) < a_n \leq s+1/(k+1)$. Since there are infinitely many such $a_n$, it can be chosen to make sure $n > n_{k+1}$.
    
    To see that $(a_{n_k}) \to 0$, \lep[l]. Choose $N \in \N$ such that $N > 1/\epsilon$. Since $s-1/k < a_{n_k} \leq s+1/k$ for every $k \in \N$, then $\abs{a_{n_k} - s} \leq 1/k$. Letting $k \geq N$, we have $\abs{a_{n_k}-s} \leq 1/k < \epsilon$, and we are done.

    \exc{2.6.1}
    Assume $(x_n) \to x$ and \lep[l]. Choose $N \in \N$ such that $\abs{x_n-x} < \epsilon/2$ for all $n \geq N$. Then, $\abs{x_n-x_m} \leq \abs{x_n-x} + \abs{x_m-x} < \epsilon$ for all $n,m \geq N$.
    
    \exc{2.6.2}
    \begin{enumerate}
        \item $(-1)^n/n$.
        
        \item This is not possible, since if a subsequence is unbounded then so is the original sequence, but every Cauchy sequence is bounded.
        
        \item Let $(a_n)$ be a divergent increasing sequence. If $(a_n)$ were bounded it would converge (Monotone Convergence Theorem), so it must be unbounded, in particular, $(a_n)$ is not bounded above. Now, assume $(a_{n_k})$ is a Cauchy subsequence of $(a_n)$. By the Cauchy Criterion, it must converge to some real number $L$. Then, there must be some $N_1 \in \N$ such that $\abs{a_n - L} < 1$ for all $n \geq N_1$. Since $a_n$ is unbounded and increasing, there is also some $N_2 \in \N$ such that $a_n > \abs{L}+1$ for all $n \geq N_2$. Taking $N := \max{N_1, N_2}$, we have $a_N > \abs{L} + 1$ and $\abs{a_N-L} < 1$. But the second equation implies $\abs{a_N} < \abs{L} + 1$, a contradiction. Therefore $(a_n)$ cannot have a Cauchy subsequence. The proof is similar if $(a_n)$ is decreasing.
        
        \item $(1, 1, 2, 1, 3, 1, 4, 1, \dots)$
    \end{enumerate}
    
    \exc{2.6.3}
    \begin{enumerate}
        \item \lep. Choose $N \in \N$ such that 
         $\abs{x_n-x_m}, \abs{y_n-y_m} \leq \epsilon/2$ for all $n,m \geq N$. Then, $\abs{x_n+y_n-(x_m+y_m)} \leq \abs{x_n-x_m} + \abs{y_n-y_m} < \epsilon$ for all $n,m \geq N$, so $(x_n + y_n)$ is Cauchy.
        
        \item \lep. Since both sequences are Cauchy, we can pick some positive $M_1, M_2 \in \R$ such that $\abs{x_n} \leq M_1$ and $\abs{y_n} \leq M_2$ for all $n \in \N$. Choose $N \in \N$ such that $\abs{x_n-x_m} < \epsilon/(2M_2)$ and $\abs{y_n-y_m} < \epsilon/(2M_1)$ for all $n, m \geq N$. Then,
        \begin{align*}
            \abs{x_n y_n - x_m y_m} &= \abs{x_n y_n - x_n y_m + x_n y_m - x_m y_m} \\
            &\leq \abs{x_n} \abs{y_n - y_m} + \abs{y_m} \abs{x_n - x_m} \\ &\leq M_1 \abs{y_n-y_m} + M_2 \abs{x_n - x_m} < \epsilon
        \end{align*} for all $n,m \geq N$, so $(x_n y_n)$ is Cauchy.
    \end{enumerate}
    
    \exc{2.6.4}
    \begin{enumerate}
        \item By Exercise $2.6.3$, $(a_n - b_n) = (a_n + (-b_n))$ is Cauchy, so it must converge. We've already shown that if a sequence $(x_n)$ converges, then so does $(\abs{x_n})$, therefore $(\abs{a_n-b_n})$ must converge, which means it is also Cauchy.
        
        \item Not necessarily. If $(a_n) = (-1)^n/n$, then $(c_n) = 1/n$ which is Cauchy. However, if $(a_n) = (1,1,1\dots)$, $(c_n) = (-1)^n$ which is not Cauchy.
        
        \item Not necessarily. If $(a_n) = (1,1,1,\dots)$, then $(c_n) = (a_n)$ which is Cauchy. However, if $(a_n) = (-1)^n/n$, $(c_n) = (-1, 0,-1, 0, \dots)$ which is not Cauchy.
    \end{enumerate}
    
    \exc{2.6.5}
    Consider the sequence 
    \begin{equation*}
        (a_n) = (\frac{1}{1}, \frac{1}{2}, \frac{1}{2}, \frac{1}{3}, \frac{1}{3}, \frac{1}{3}, \frac{1}{4}, \dots)
    \end{equation*} of $1$ appearance of $1/1$, followed by $2$ $1/2'$s, then $3$ $1/3's$ and so on. Since $(a_n)$ is decreasing and bounded by $1$, it must converge. Also, it is clear that $(1/n)$ is a subsequence of $(a_n)$ which means $(a_n) \to 0$. Now, define the sequence $(s_m)$ by
    \begin{equation*}
        (s_m) = (\sum_{n=1}^m a_n) = (1, \frac{3}{2}, 2, \frac{7}{3}, \frac{8}{3}, 3, \dots).
    \end{equation*} To see that $(s_m)$ is pseudo-Cauchy, \lep[l]. Since $(a_n) \to 0$, we can choose $N \in \N$ such that $a_{m+1} \leq \epsilon$ for all $m+1 \geq N$. Then, it is easy to see that $\abs{s_{m+1}-s_m} = a_{m+1} < \epsilon$, which means $(s_m)$ is pseudo-Cauchy. However, $(m)$ is a subsequence of $(s_m)$ which is not bounded above, therefore $(s_m)$ is also not bounded above, which means the first claim in the exercise is false.
    
    To verify the second claim, \lep[l]. Since $(x_n)$ and $(y_n)$ are pseudo-Cauchy, we can choose $N \in \N$ such that $\abs{x_{n+1}-x_n}, \abs{y_{n+1}-y_n} < \epsilon/2$ for all $n \geq N$. Then, \begin{equation*}
        \abs{(x_{n+1}+y_{n+1}) - (x_n + y_n)} \leq \abs{x_{n+1}-x_n} + \abs{y_{n+1}-y_n} < \epsilon
    \end{equation*} for all $n \geq N$, therefore $(x_n + y_n)$ is also pseudo-Cauchy. 
    
    \exc{2.6.6}
    \begin{enumerate}
        \item Let $(a_n) = (-1)^n/n$. To see that $(a_n)$ is quasi-increasing, \lep[l]. It is easy to see that 
        \begin{equation*}
            \lim_{n \to \infty} \left(\frac{2n+k}{n(n+k)}\right) = 0,
        \end{equation*} for any $k \in \N$, therefore we can choose $N \in \N$ such that
        \begin{equation*}
            \frac{2n+k}{n(n+k)} < \epsilon
        \end{equation*} for all $n \geq N$ and $k \in \N$. Then, 
        \begin{equation*}
            \frac{(-1)^n}{n} - \frac{(-1)^{n+k}}{n+k} \leq \frac{1}{n} + \frac{1}{n+k} = \frac{2n+k}{n(n+k)} < \epsilon.
        \end{equation*} This means that $a_{m}-a_{m+k} < \epsilon$ which implies $a_n > a_m - \epsilon$ for all natural $n > m \geq N$, as we wanted to show.
        
        \item Let $(a_n)$ be defined by 
        \begin{equation*}
            a_n = \begin{cases}
            \frac{n+1}{2} & n \text{ is odd} \\
            \frac{n}{2} - \frac{1}{n} & n \text{ is even}
            \end{cases}.
        \end{equation*} To see that $(a_n)$ is quasi-increasing, \lep[l]. Choose $N \in \N$ such that $N > 1/\epsilon$. Then, we need to show that $a_m - a_{m+k} < \epsilon$ for every $m+k > m \geq N$. There are four cases, depending on the parity of $a_m$ and $a_{m+k}$. It is easy to verify that $a_m-a_{m+k} < 0 < \epsilon$ in every case except possibly when $a_m$ is odd and $a_{m+k}$ is even. In that case, 
        \begin{equation*}
            a_m-a_{m+k} = \frac{1-k}{2} + \frac{1}{m+k} \leq \frac{1}{m+k} < \frac{1}{m} < \epsilon
        \end{equation*} therefore $(a_n)$ is quasi-increasing. It is also easy to verify that $(a_n)$ is not eventually monotone or bounded above.
        
        \item 
    \end{enumerate}
    
    \exc{2.6.7}
    \begin{enumerate}
        \item Let $(x_n)$ be an increasing bounded sequence. 
        
        By the Bolzano-Weierstrass Theorem, there we can construct a subsequence $(x_{n_k})$ which converges to some $x \in \R$. To prove that $(x_n) \to x$, \lep[l]. Choose $N_1 \in \N$ such that $\abs{x_{n_k}-x} < \epsilon$ for all $k \geq N_1$. Now, set $N := n_{N_1}$ and let $n \geq N$ be arbitrary. Since $(x_n)$ is increasing, we have $x_{n_{N_1}} \leq x_n \leq x_{n_m}$ for some $n_m \geq n$. Using the fact that $-\epsilon< x_{n_k} -x < \epsilon$ for all $k \geq N_1$, 
        \begin{align*}
            x_{n_{N_1}} \leq &x_n \leq x_{n_m} \implies \\
            x_{n_{N_1}}-x \leq x_n&-x \leq x_{n_m}-x \implies \\
            -\epsilon < x_{n_{N_1}}-x \leq x_n&-x \leq x_{n_m}-x < \epsilon \implies
            \abs{x_n-x} < \epsilon,
        \end{align*} therefore $(x_n) \to x$.
        
        \item Let $(x_n)$ be bounded by $M$. Then, every $x_n$ is in the interval $I_1 = [-M,M]$, so we can choose one of them, labeled $x_{n_1}$. Bisect $I_1$ into $[-M, 0]$ and $[0, M]$. At least one of this intervals must contain infinitely many terms in $(x_n)$, so select one such interval and label it $I_2$. In general, we construct the closed interval $I_{k+1}$ by taking a half of $I_k$ containing an infinite number of terms $(x_n)$ and then select $n_{k+1} > n_k > n_{k-1} \dots > n_1$ so that $x_{n_{k+1}} \in I_k$ (This is the same construction as the one in the BW proof given in the book). To prove that $(x_{n_k})$ converges, \lep[l]. If we assume the Archimedean Property, it is easy to see that the length of the $I_k'$s gets arbitrarily close to $0$, so we can choose $N \in \N$ such that the length of $I_k$ is less than $\epsilon$ for all $k \geq N$. Since the $I_k'$s are nested, and $x_{n_k} \in I_k$, we also have $x_{n_m} \in I_k$ for all $m \geq k$. Thus, choosing arbitrary $a,b \geq N$, we have $x_{n_a},x_{n_b} \in I_N$. Since the length of $I_N$ is less than $\epsilon$, we must have $\abs{x_{n_a}-x_{n_b}} < \epsilon$, therefore $(x_{n_k})$ is Cauchy. By the Cauchy Criterion, we can conclude that $(x_{n_k})$ converges.
        
        \item The Archimedean Property holds in the rational numbers, in the sense that given any $x \in \Q$, there exists an $n \in \N$ satisfying $n > x$, and given any positive $y \in \Q$, there exists an $n \in \N$ satisfying $1/n < y$. Since we know the rational analogue of AoC is not true, this means we cannot use the Archimedean Property to prove it.
    \end{enumerate}
    
    \item 
    \begin{shortdefinition} \label{def_equivSequences}
        Two sequences $(a_n)$ and $(b_n)$ are equivalent if and only if for all $\epsilon > 0$ there exists a $N \in \N$ such that $\abs{a_n-b_n} < \epsilon$ for all $n \geq N$.
    \end{shortdefinition}
    
    \begin{shortlemma} \label{lem_equivCauchySequences}
        If $(a_n)$ is equivalent to $(b_n)$ and $(b_n) \to b$, it follows that $(a_n) \to b$.
    \end{shortlemma}
    \begin{proof}
        Let $(a_n)$ and $(b_n)$ be equivalent sequences. \lep \space and choose $N \in \N$ such that $\abs{a_n-b_n},\abs{b_n-b} < \epsilon/2$ for all $n \geq N$. Then, 
        \begin{equation*}
            \abs{a_n-b} \leq \abs{a_n-b_n} + \abs{b_n-b} < \epsilon
        \end{equation*} for all $n \geq N$. Thus, $(a_n)$ also converges to $b$.
    \end{proof}
    
    \exc{2.7.1}
    \begin{enumerate}
        \item Let $I_n = [S_{2n}, S_{2n-1}]$. The length of $I_n$ is $a_{2n}$, a subsequence of $(a_n)$, therefore it must converge to $0$. Also, we can use the fact that $S_{2n}$ is increasing and $S_{2n-1}$ is decreasing to show that $I_n \supseteq I_{n+1}$. \lep. Choose $N \in \N$ such that $\abs{a_{2n}} < \epsilon$ for all $n \geq N$ and define $M := 2N-1$. Then, since $S_n,S_m \in I_N$ for all $n,m \geq M$, and the length of $I_N$ is less than $\epsilon$, we must have $\abs{S_n-S_m} < \epsilon$ for all $n \geq M$, so $(S_n)$ is Cauchy, therefore converges.
        
        \item This is almost identical to the previous proof. Construct the intervals $I_n$ just as before. By the Nested Interval Property, we can choose some $x \in \R$ such that $x \in I_n$ for every $n$. To prove that $(S_n) \to x$, \lep[l]. Choose $N \in \N$ such that $\abs{a_{2n}} < \epsilon$ for all $n \geq N$ and define $M := 2N-1$. Then, since $S_n,x \in I_N$ for all $n \geq M$, and the length of $I_N$ is less than $\epsilon$, we must have $\abs{S_n-x} < \epsilon$ for all $n \geq M$.
        
        
        \item It is easy to see with induction that $a_1 \geq S_n \geq 0$ for all $n \in \N$. Since $S_{2n}$ is increasing and bounded above by $a_1$, the Monotone Convergence Theorem guarantees its convergence. Also, ${S_{2n-1}}$ is decreasing and bounded below by $0$, so it must also converge. Now, we show that $(S_{2n})$ and $(S_{2n-1})$ are equivalent (Definition \ref{def_equivSequences}). \lep. Choose $N \in N$ such that $\abs{a_n} < \epsilon$ for all $n \geq N$. Then, $\abs{S_{2n}-S_{2n-1}} = \abs{a_{2n}} \leq \abs{a_n} < \epsilon$ for all $n \geq N$, so the sequences are equivalent. By Lemma \ref{lem_equivCauchySequences}, they must converge to the same real number $a$, and we've already shown in Exercise 2.3.5 that this means $(S_n) \to a$ as well, since $(S_n)$ is the shuffled sequence of $(S_{2n-1})$ and $(S_{2n})$. 
    \end{enumerate}
    
    \exc{2.7.2}
    \begin{enumerate}
        \item Notice that 
        \begin{equation*}
            0 \leq \frac{1}{2^n+n} \leq \frac{1}{2^n}
        \end{equation*} for all $n \in \N$. Also, $\sum_{n=0}^\infty 1/2^n$ is a geometric series with $a = 1$ and $r = 1/2$, so it converges, which clearly means $\sum_{n=1}^\infty 1/2^n$ also converges. By the Comparison Test, our original sum must also converge.
        
        \item For every $n \in \N$ we have 
        \begin{equation*}
            0 \leq \abs{\frac{\sin{n}}{n}} \leq \frac{1}{n^2},
        \end{equation*} and, since $\sum_{n=1}^\infty 1/n^2$ converges, so must 
        \begin{equation*}
            \sum_{n=1}^\infty \abs{\frac{\sin{n}}{n}}.
        \end{equation*} By the Absolute Convergence Test, or original sum also converges.
        
        \item The original sum is equal to 
        \begin{equation*}
            1 - \sum_{n=3}^\infty \frac{n}{2(n-1)} (-1)^{n+1}.
        \end{equation*} Notice that 
        \begin{equation*}
            a_{2n+1} = \frac{2n+1}{4n} > \frac{1}{2}
        \end{equation*} for every $n$. Therefore, by Theorem 2.7.3, the series diverges.
        
        \item We can write this sum as 
        \begin{equation*}
            \sum_{n=1}^\infty \frac{1}{3n-2}+\frac{1}{3n-1}-\frac{1}{3n}.
        \end{equation*} Also, 
        \begin{equation*}
             \frac{1}{3n-2}+\frac{1}{3n-1}-\frac{1}{3n} \geq \frac{1}{3n-2} \geq 0.
        \end{equation*} By the Cauchy condensation test, we have
        \begin{equation*}
            \sum_{n=1}^\infty \frac{1}{3n-2} \text{ converges} \iff 
            \sum_{n=1}^\infty \frac{2^n}{3\cdot2^n-2} \text{ converges.}
        \end{equation*} But, 
        \begin{equation*}
            \lim\left({\frac{2^n}{3\cdot2^n-2}}\right) = \frac{1}{3} \implies 
            \sum_{n=1}^\infty \frac{2^n}{3\cdot2^n-2} \text{ diverges.}
        \end{equation*} By the comparison test, the original sum also diverges.
        
        \item By the Alternating Series Test, the series converges.
    \end{enumerate}
    
    \exc{2.7.3}
    \begin{enumerate}
        \item To prove (i), assume $\sum_{n=1}^\infty (b_n)$ converges and $0 \leq a_n \leq b_n$ for all $n \in \N$. \lep. Then, the Cauchy Criterion for Series lets us choose $N \in \N$ such that $\abs{b_{m+1}+b_{m+2} + \dots + b_{n}} < \epsilon$ whenever $n > m \geq M$. But, since every $a_n$ is nonnegative and less than or equal to $b_n$, we have 
        \begin{equation*}
           \abs{a_{m+1}+a_{m+2} + \dots + a_{n}} \leq \abs{b_{m+1}+b_{m+2} + \dots + b_{n}} < \epsilon
        \end{equation*} therefore $\sum_{n=1}^\infty a_n$ converges, by the Cauchy Criterion for Series. Also, (ii) is the contrapositive of (i), therefore we only need to prove (i).
        
        \item Assume $\sum_{n=1}^\infty (b_n) = t$ and $0 \leq a_k \leq b_k$ for all $k \in \N$. Also, let $(s_n), (t_n)$ be the partial sum sequences of $(a_n)$ and $(b_n)$ respectively. Then, $0 \leq s_k \leq t_k$ for every $k \in \N$. This implies $0 \leq s_k \leq t$ for all $k \in \N$, therefore, $s_k$ is bounded. Thus, it must converge by the Monotone Convergence Theorem.
    \end{enumerate}
    
    \exc{2.7.4}
    \begin{enumerate}
        \item $(x_n) = 1/n$, $(y_n) = 1/n$.
        
        \item $(x_n) = (-1)^{n+1}/n$, $(y_n) = (-1)^{n+1}$.
        
        \item Notice that $\sum y_n = \sum ((x_n + y_n) - x_n)$, which, by the Algebraic Limit Theorem for Series, implies $\sum y_n = \sum  (x_n + y_n) - \sum x_n$, therefore $\sum y_n$ necessarily converges, therefore the request is impossible.
        
        \item Notice that $1/n$ is decreasing and $(1/n) \to 0$, therefore $\sum (-1)^n/n$ converges by the Alternating Series Test. Then, we can apply the Comparison test to see that $\sum x_n$ converges, therefore the request is impossible.
    \end{enumerate}
    
    \exc{2.7.5} By the Cauchy Condensation Test, we have 
    \begin{equation*}
        \sum_{n=1}^\infty \frac{1}{n^p} \text{ converges } \iff 
        \sum_{n=0}^\infty 2^n \frac{1}{2^{n p}} \text{ converges.} 
    \end{equation*} But, the second sum is a geometric series with $r = 2/2^p$, therefore it converges if and only if $2/2^p < 1$, therefore 
    \begin{equation*}
        \sum_{n=1}^\infty \frac{1}{n^p} \text{ converges} \iff p > 1.
    \end{equation*}
    
    \exc{2.7.6}
    \begin{enumerate}
        \item $(a_n) = (1,1,1,\dots)$ is a counterexample.
        
        \item If a series converges then by definition the sequence of partial sums also converges, thus all of its subsequences converge.
        
        \item Since $\abs{a_n} \geq 0$ for all $n \in \N$, the partial sums sequence of $(\abs{a_n})$ is increasing. Since $\sum \abs{a_n}$ subverges, the partial sums have some convergent subsequence. By Exercise 2.5.2 (d), this means the partial sums sequence converges, therefore $\sum \abs{a_n}$ converges, which means $\sum a_n$ also converges by the Absolute Convergence Test. Since every convergent series subverges, $\sum a_n$ subverges as well.
        
        \item $(a_n) = (1,-1,2,-2,3,-3, \dots)$ is a counterexample.
    \end{enumerate}
    
    \exc{2.7.7}
    \begin{enumerate}
        \item Notice that $(2^n a_{2^n})$ is a subsequence of $(n a_n)$, therefore it also converges to $l$. Since $l \neq 0$, $\sum 2^n a_{2^n}$ diverges, therefore $\sum a_n$ also diverges, by the Cauchy Condensation Test.
        
        \item Since $\lim(n^2 a_n)$ exists, there is some $L \in \R$ and some $N \in \N$ such that $\abs{n^2 a_n - L} < 1$ for all $n \geq N$. This implies $0 < a_n < (1+\abs{L})/n^2$. Since 
        \begin{equation*}
            \sum_{n = N}^\infty \frac{1+\abs{L}}{n^2}
        \end{equation*} converges, the Comparison Test guarantees that 
        \begin{equation*}
            \sum_{n = N}^\infty a_n 
        \end{equation*} also converges, which clearly means that $\sum a_n$ converges.
    \end{enumerate}
    
    \exc{2.7.8}
    \begin{enumerate}
        \item Since $\sum (a_n)$ converges, we can choose $N \in \N$ such that $\abs{a_n} < 1$ for all $n \geq N$. Then, $0 \leq \abs{a_n}^2 = \abs{a_n^2} < \abs{a_n}$ for all $n \geq N$. By the Comparison test, since $\sum \abs{a_n}$ converges, $\sum \abs{a_n^2}$ also converges.
        
        \item $(a_n) = (b_n) = ((-1)^n/\sqrt{n}))$ is a counterexample.
        
        \item We prove the contrapositive of the statement. Assume $\sum n^2 a_n$ converges. We need to show that $\sum a_n$ does not converge conditionally. Since $\lim (n^2 a_n) = 0$, $\sum a_n$ must converge by Exercise 2.7.7 (b). This means that we need to show that $\sum \abs{a_n}$ converges. Since $(n^2 a_n) \to 0$, there is some $M \in \R$ such that $\abs{k^2 a_k} \leq M$ for every $k \in \N$. It follows that $0 \leq \abs{a_k} \leq M/k^2$ for every natural $k$, so $\sum a_k$ converges absolutely by the Comparison Test.
    \end{enumerate}
    
    \exc{2.7.9}
    \begin{enumerate}
        \item Choose $N \in \N$ such that $\abs{a_{n+1}/a_n - r} < r'- r$ for all $n \geq N$. Applying the triangle inequality, we get $\abs{a_{n+1}/a_n} < r' -r + \abs{r}$. But, a simple application of the Order Limit Theorem reveals that $r \geq 0$, therefore $\abs{r}=r$. Then, the inequality becomes $\abs{a_{n+1}/a_n} < r'$, for all $n \geq N$, therefore $\abs{a_n+1} \leq \abs{a_n} r'$ for every $n \geq N$.
        
        \item $\sum (r')^n$ is a geometric series with, $0 < r' < 1$, so it converges.
        
        \item By (a), we can choose some $N \in \N$ such that $\abs{a_{n+1}} \leq \abs{a_n} r'$ for every $n \geq N$. It follows easily by induction that for every $n \in N$, $0 \leq \abs{a_{N+n}} \leq \abs{a_N} (r')^n$. Since we have shown in (b) that $\sum \abs{a_N} (r')^n$ converges, we can apply the comparison test to get that 
        \begin{equation*}
            \sum_{n=N+1}^\infty \abs{a_n}
        \end{equation*} converges, then the Absolute Convergence Test guarantees the convergence of $\sum a_n$.
    \end{enumerate} 
    
    \item \begin{shortlemma} \label{lem_1OverInfinityIsZero}
        Let $(x_n)$ be a sequence such that for all $n \in \N$ $x_n \neq 0$.
        If $(x_n)$ is increasing and not bounded above, then $(1/x_n) \to 0$. Similarly, if $(x_n)$ is decreasing and not bounded below, then $(1/x_n) \to 0$.
    \end{shortlemma}
    \begin{proof}
        Assume $(x_n)$ is increasing and not bounded above and \lep[l]. By assumption, we can choose $N \in \N$ such that $x_n \geq 1/\epsilon$ for all $n \geq N$. It follows that $\abs{1/x_n} < \epsilon$ for all $n \geq N$, therefore $(1/x_n) \to 0$. The proof for the case where $(x_n)$ is decreasing and not bounded below is very similar.
    \end{proof}
    
    \exc{2.7.10}
    \begin{enumerate}
        \item We can write this product as 
        \begin{equation*}
            2  \prod_{n=1}^\infty (1 + \frac{1}{2^n})
        \end{equation*} which, as was shown in Exercise 2.4.10 (b), converges if and only if 
        \begin{equation} \label{sum_2.7.10}
            \sum_{n=1}^\infty \frac{1}{2^n}
        \end{equation} converges. But, (\ref{sum_2.7.10}) is a geometric series with $r = 1/2$, so it must converge, therefore the original product also converges.
        
        \item Since every term in the product is less than $1$, the partial product sequence is decreasing and bounded below by $0$, so it must converge by the Monotone Convergence Theorem.
        
        Consider the sequence $(1/p_m)$, consisting of the inverse partial products. We can write it as 
        \begin{equation*}
            \frac{1}{p_m}= \frac{1}{\prod_{n=1}^m \frac{2n-1}{2n}} = 
            \prod_{n=1}^m \frac{2n}{2n-1} = \prod_{n=1}^m (1 + \frac{1}{2n-1}).
        \end{equation*} It is easy to see by the Cauchy Condensation Test that 
        \begin{equation*}
            \sum_{n=1}^\infty \frac{1}{2n-1}
        \end{equation*} diverges, therefore $(1/p_m)$ also diverges. Since every term in the product defining $(1/p_m)$ is greater than one, the sequence increases. Thus, it cannot be bounded above, because in that case the Monotone Convergence Theorem would imply its convergence. Therefore, we can apply Lemma \ref{lem_1OverInfinityIsZero} to conclude that $(1/(1/p_m)) = (p_m) \to 0$.
        
        \item The original product can be written as 
        \begin{equation*}
            \prod_{n=1}^\infty (1+\frac{1}{(2n)^2-1})
        \end{equation*} which we have shown to converge if and only if 
        \begin{equation} \label{sum2_2.7.10}
            \sum_{n=1}^\infty \frac{1}{(2n)^2-1}
        \end{equation} converges (Exercise 2.4.10 (b)). It is easy to see by the Comparison Test with $1/n^2$ that (\ref{sum2_2.7.10}) converges, therefore the original product also converges.
    \end{enumerate}
    
    \exc{2.7.11}
    One example where neither $(a_n)$ nor $(b_n)$ are monotone is 
    \begin{align*}
        (a_n) &= (\frac{1}{1^2}, 1, \frac{1}{3^2}, 1, \frac{1}{5^2}, 1, \dots) \\ 
        (b_n) &= (1, \frac{1}{2^2}, 1, \frac{1}{4^2}, 1, \dots).
    \end{align*}
    
    \exc{2.7.12}
    It is straightforward to verify this with induction.
    
    \exc{2.7.13}
    \begin{enumerate}
        \item This is exactly the formula in Exercise 2.7.12 with $m$ set to $1$.
        
        \item Notice that $(y_n)$ is decreasing and bounded below by $0$, so it must converge to some real number $y$. Also, $(s_n)$ converges, so there is some $M \in \R$ such that $\abs{s_k} \leq M$ for all $k \in \N$. Consider the series
        \begin{equation}\label{sum1_2.7.13}
            \sum_{n=1}^\infty M (y_k - y_{k+1}).
        \end{equation} It is easy to verify with induction that the partial sum sequence of (\ref{sum1_2.7.13}) is $(p_m) = (M(y_1-y_m))$. Applying the Algebraic Limit Theorem, we can conclude that $(p_m) \to M(y_1-y)$. Now, notice that 
        \begin{equation*}
            0 \leq \abs{s_k (y_k-y_{k+1})} = \abs{s_k} (y_k-y_{k+1}) \leq 
            M (y_k-y_{k+1}),
        \end{equation*} therefore 
        \begin{equation} \label{sum2_2.7.13}
            \sum_{n=1}^\infty s_k (y_k-y_{k+1})
        \end{equation} converges absolutely by the Comparison Test. Then, the Absolute convergence test guarantees the convergence of (\ref{sum2_2.7.13}).
        
        Since both $(y_n)$ and $(s_n)$ converge, $(s_n y_{n+1})$ also converges, by the Algebraic Limit Theorem. By the Absolute Convergence test, the series 
        \begin{equation*}
            \sum_{k=1}^\infty s_k(y_k-y_{k+1})
        \end{equation*} also converges, so we can apply the Algebraic Limit Theorem again to conclude that 
        \begin{equation*}
           s_n y_{n+1} + \sum_{k=1}^n s_k(y_k-y_{k+1})
        \end{equation*} also converges, which finally implies the convergence of 
        \begin{equation*}
            \sum_{k=1}^\infty x_k y_k. 
        \end{equation*}
    \end{enumerate}
    
    \exc{2.7.14}
    \begin{enumerate}
        \item Abel's test requires $\sum x_k$ to converge, whereas Dirichlet's Test makes the weaker assumption that the partial sums are bounded. While both tests assume $(y_n)$ is decreasing and bounded below by zero, Abel's test does not require that $(y_n) \to 0$, which is in contrast to the other test.
        
        While in the previous proof we used the fact that $(s_n)$ converges to prove that
        \begin{equation*}
            \sum_{k=1}^\infty s_k(y_k-y_{k+1})
        \end{equation*} converges, we really only needed $(s_n)$ to be bounded, so the same proof applies. The other slight difference is when we applied the Algebraic Limit Theorem to show that $s_n y_{n+1}$ converges. This does not work here, since $(s_n)$ does not necessarily converge. To see that this is not a problem, \lep. Let $M \in \R$ be a bound for $(s_n)$. Use the fact that $(y_n) \to 0$ to choose $N \in \N$ such that $\abs{y_n} < \epsilon/M$ for all $n \geq M$. Then, $\abs{s_n y_{n+1}} \leq M \abs{y_{n+1}} < \epsilon$, therefore $(s_n y_{n+1}) \to 0$. For the same reason as before, this means that
        \begin{equation*}
            \sum_{k=1}^\infty x_k y_k
        \end{equation*} converges.
        
        \item Let $(x_k) = (-1)^{k+1}$ and $(y_n) \to 0$ be decreasing. Notice that the partial sums of $(x_k)$ are bounded by $1$. By Dirichlet's Test, 
        \begin{equation*}
            \sum_{k=1}^\infty (-1)^{k+1} y_n
        \end{equation*} converges.
    \end{enumerate}
    
    \exc{3.2.1}
    \begin{enumerate}
        \item In Theorem 3.2.3, we set $\epsilon := \min{\epsilon_1, \epsilon_2, \dots 
        \epsilon_N}$, which we may not be able to do if there are infinitely many $\epsilon'$s, as infinite sets do not always have minimums, such as the set $\set{1/n : n \in \N}$.
        
        \item Let $O_n := (-1/n, 1/n)$ for every natural $n$, and define $O := \bigcap_{n=1}^\infty O_n$. It is easy to verify that $0 \in O$. Now, let $x \in O$.. If $x > 0$, we can pick a natural $N$ such that $N > 1/x$. Then, $x > 1/N$, therefore $x \notin O_N$ which contradicts our assumption that $x \in O$. A similar contradiction arises when $x < 0$, which means $x$ can only be zero, so we have $O = \set{0}$. The only sequence that is contained in $A$ is $(0, 0, 0, \dots) \to 0$, so $O$ has no limit points, therefore it is closed.
    \end{enumerate}
    
    \item \begin{shortlemma} \label{lem_openSetCountable}
        Every open non-empty subset of the reals is uncountable.
    \end{shortlemma}
    
    \begin{proof}
        Let $A \subseteq \R$ be a non-empty open set and assume $A$ is countable. Let $a \in A$ and choose some $\epsilon$ such that $V_\epsilon(a) \subseteq A$. Since $A$ is countable, $V_\epsilon(a)$ must be empty, finite or countable. However, $V_\epsilon(a) = (a-\epsilon, a+\epsilon)$ is an open interval of the reals, and we have already shown these are always uncountable, which is a contradiction. Thus $A$ cannot be countable.
    \end{proof}
    
    \exc{3.2.2}
    \begin{enumerate}
        \item $1$ and $-1$ are limit points of $A$, meanwhile any number in $[0, 1]$ is a limit point of $B$.
        
        \item $A$ is clearly countable, so Lemma \ref{lem_openSetCountable} guarantees it cannot be open. $A$ is also not closed, since the limit point $1$ is not in $A$. $B$ has the same cardinality of the rationals, so it must be countable, therefore not open. Also, $0$ is a limit point of $B$, but $0 \notin B$, so $B$ is also not closed.
        
        \item $2$ is an isolated point of $A$, but $B$ contains no isolated points.
        
        \item $\overline{A} = -1$, $\overline{B} = \set{0, 1}$.
    \end{enumerate}
    
    \exc{3.2.3}
    \begin{enumerate}
        \item $\Q$ is countable, so it is not open (Lemma \ref{lem_openSetCountable}). Also, every real number is a limit point of $\Q$ but not every real number is rational, so $\Q$ is not closed. For example $\sqrt{2}$ is a limit point of $\Q$ but $\sqrt{2} \notin \Q$. No element of $\Q$ has an $\epsilon$-neighborhood contained in $\Q$.
        
        \item $\N$ is countable, so it is not open (Lemma \ref{lem_openSetCountable}). Assume $x \in \R$ is a limit point of $\N$. Then, $x = \lim(a_n)$ for some Cauchy sequence of naturals $a_n$ where no $a_n$ is equal to $x$. This means that there is some $N \in \N$ such that $\abs{a_n-a_m} < 1/2$ for all $n,m \geq N$, but this cannot happen unless $a_n = a_m = x$ for all $n,m \geq N$, which is a contradiction. Therefore, $\N$ has no limit points, so it is closed.
        
        \item To see that the set $A = \set{x \in \R : x \neq 0}$ is open, let $a \in A$ be arbitrary. Then, $V_{\abs{a}}(a) \subseteq A$, therefore $A$ is open. The sequence $a_n = 1/n$ shows that $0$ is a limit point of $A$, but $0 \notin A$, therefore $A$ is not closed.
        
        \item The set $A := \set{\sum_{k=1}^n 1/k^2 : n \in \N}$ is countable, therefore not open. Consider the sequence 
        \begin{equation*}
            s_n = \sum_{k=1}^n \frac{1}{k^2},
        \end{equation*} which is contained in $A$. We have previously shown that $\lim s_n$ converges, so we can say that 
        \begin{equation*}
            \sum_{k=1}^n \frac{1}{k^2} \leq \sum_{k=1}^\infty \frac{1}{k^2} = I.
        \end{equation*} for every $n \in \N$, therefore $I$ is an upper bound for $A$. Now, assume there is some $x_1 \in A$ such that $x_1 = I$. Then, $x_1 = \sum_{k=1}^N 1/k^2$ for some $N \in \N$. But $\sum_{k=1}^{N+1} 1/k^2 > x_1 = I$ is also an element of $A$, which contradicts the fact that $I$ is an upper bound for $A$. Thus, there is no $x \in A$ such that $x = I$. This means that $I$ is a limit point of $A$ that is not contained in $A$, therefore $A$ is not closed.
        
        \item The set $A := \set{\sum_{k=1}^n 1/k : n \in \N}$ is countable, therefore not open. Since the harmonic series diverges, this set contains no Cauchy sequences, therefore it is closed (Theorem 3.2.8).
    \end{enumerate}
    
    \exc{3.2.4}
    \begin{enumerate}
        \item If $s \in A$, then clearly $s \in \overline{A}$ so assume $s \notin A$. Then, for every $\epsilon > 0$ we can find $a \in A$ such that $s-\epsilon < a$. Since $s$ is an upper bound, we also have $a < s+\epsilon$, therefore $a \in (s-\epsilon, s+\epsilon)$, which means $s$ is a limit point of $A$, so it must be an the closure of $A$.
        
        \item Let $A$ be open. \lep \space and consider the neighborhood $V_\epsilon(s) = (s-\epsilon, s+\epsilon)$. Clearly $s + \epsilon/2 \in V_\epsilon(s)$, but it cannot be an element of $A$, since $s + \epsilon/2 > s$. Thus, no $\epsilon$-neighborhood around $s$ is contained in $A$, which means $s$ cannot be an element of $A$. In conclusion, an open set cannot contain its supremum.
    \end{enumerate}
    
    \exc{3.2.5}
    \begin{enumerate}
        \item ($\Rightarrow$) Assume $F \subseteq \R$ is closed. Let $(a_n)$ be an arbitrary Cauchy sequence contained in $F$. By the Cauchy Criterion, $(a_n) \to L$ for some $L \in \R$, which we must show to be contained in $F$. If there is some $n_0 \in \N$ such that $a_{n_0} = L$, then $L \in F$, which is what we wanted to show. On the other hand, if none of the $a_n'$s are equal to $L$, then, by Theorem 3.2.5, $L$ is a limit point of $F$. Since $F$ is closed, we must have $L \in F$.
        
        ($\Leftarrow$) For the reverse implication we assume that every Cauchy sequence contained in $F$ has a limit that is also an element of $F$. If $F$ has no limit points, then it is closed and we are done. Otherwise, let $x$ be a limit point of $F$. By Theorem 3.2.5, $x = \lim(a_n)$ for some sequence $(a_n)$ contained in $F$. Since $(a_n)$ converges, it is Cauchy. By assumption, we can conclude that its limit, namely $x$, is an element of $F$.
    \end{enumerate}
    
    \exc{3.2.6}
    \begin{enumerate}
        \item Define $O_n := (\sqrt{2}+n,\sqrt{2}+n+1)$ for all $n \in \Z$ and let $O = \bigcup_{n=-\infty}^\infty O_n$. By Theorem 3.2.3, $O$ is an open set. Now, assume there is some $k \in \Z$ such that $\sqrt{2} \in O_k$. Then, $\sqrt{2}+n<\sqrt{2} < \sqrt{2}+n+1$, which implies $0 < -n < 1$, but this is a contradiction since there are no integers strictly between $0$ and $1$. Therefore, $\sqrt{2} \notin O$, thus $O \neq \R$. Now let $x \in \Q$ be arbitrary. Find some $N \in \Z$ such that $N \leq x-\sqrt{2} < N+1$. Since $x-\sqrt{2}$ is irrational and $N$ is rational, the previous inequality can be made strict. Then, we have $\sqrt{2} + N < x < \sqrt{2} + N + 1$, thus $x \in O_N$. This means that $O$ is an open set that contains every rational number but is not equal to $\R$.
        
        \item Notice that $\emptyset \supseteq \emptyset \supseteq \emptyset \dots$, and the intersection of all these sets is of course empty. Since $\emptyset$ is closed, this is a counterexample.
        
        \item Let $A$ be a nonempty open set. Choose some $a \in A$. Then, there is some $V_\epsilon(a) = (a-\epsilon, a+\epsilon) \subseteq A$. But we can always choose some rational number strictly between $a-\epsilon$ and $a + \epsilon$, so $A$ must contain a rational number.
        
        \item Define $A := \set{\sqrt{2} + 1/k : k \in \N} \cup \sqrt{2}$. The only limit point of $A$ is $\sqrt{2}$, so $A$ is closed, bounded and infinite but contains no rational numbers.
        
        \item The Cantor set is an intersection of closed sets, so it must be closed, by Theorem 3.2.14.
    \end{enumerate}
    
    \exc{3.2.7}
    \begin{enumerate}
        \item Let $x$ be a limit point of $L$. We want to show that $x \in L$, which amounts to showing that $x$ is a limit point of $A$. \lep. Since $x$ is a limit point of $L$, there is some $l \in L$ such that $l \neq x$ and $l \in V_\epsilon(x)$. This means that $l$ is either in $(x, x+\epsilon)$ or $(x-\epsilon, x)$, which are both open intervals, which means we can find some $\epsilon'$-neighborhood contained in the interval. Then, we have $V_\epsilon'(l) \subseteq V_\epsilon(x)$. Since $l$ is a limit point of $A$, there is some $a \in A$ such that $a \neq l$ and $a \in V_\epsilon'(l)$, which implies $a \neq x$ and $a \in V_\epsilon(x)$, therefore $x$ is a limit point of $A$, thus $x \in L$.
        
        \item Let $x$ be a limit point of $A \cup L$. \lep. Then, there is some $a_1 \in A \cup L$ such that $a_1 \neq x$ and $a_1 \in V_\epsilon(x)$. If $a_1 \in A$ we would be done, so assume $a_1 \notin A$. Then, $a_1$ must be a limit point of $A$. Thus, we can choose some $\epsilon'$ such that $a_2 \in V_\epsilon'(a_1) \subseteq V_\epsilon(x)$ where $a_2 \in A$ and $a_2 \neq x$, therefore $x$ is a limit point of $A$.
        
        Now, assume $x$ is a limit point of  $\overline{A}$. Then, $x$ is also a limit point of $A$, therefore $x \in L$, which implies $x \in A \cup L = \overline{A}$, therefore $\overline{A}$ is closed.
    \end{enumerate}
    
    \exc{3.2.8}
    \begin{enumerate}
        \item Definitely closed (Theorem 3.2.12), but not necessarily open. If $A = \R$ and $B = \emptyset$, then $\overline{A \cup B} = \R$ which is open, but if $A = (1, 2)$ and $B = [1,2]$ then $\overline{A \cup B} = B$ which is not open.
        
        \item Definitely open. To see this, choose some arbitrary $a \in A \backslash B$ (if $A$ is empty then $A \backslash B = \emptyset$, which is open). Since $a \in A$, there is some $V_{\epsilon_0}(a) \subseteq A$. Since $a \notin B$, it is not a limit point of $B$, so there is some $V_{\epsilon_1}(a)$ which does not intersect $B$. Now, set $\epsilon := \min (\epsilon_0, \epsilon_1)$, then $V_\epsilon(a)$ is contained in $A$ and has no intersection with $B$, therefore $A\backslash B$ is open.
        
        If $A = (0,1)$ and $B = [1/3, 2/3]$ then $A \backslash B$ is open and not closed, but if $A = (0,1)$ and $B = [0, 1]$ then $A\backslash B$ is both open and closed.
        
        \item By Theorem 3.2.13, $A^c$ is closed, and Theorem 3.2.14 guarantees that the union of two closed sets is closed, therefore $A^c \cup B$ is closed, so $(A^c \cup B)^c$ is open. This set may or may not be closed.
        
        \item Neither.
        
        \item $A^c$ is closed, therefore $\overline{A^c} = A^c$. Then, $\overline{A}^c \cap \overline{A^c} = \overline{A}^c \cap A^c = (\overline{A} \cup A)^c$. But, $\overline{A} = A \cup L$ where $L$ are the limit points of $A$. Then, 
        $\overline{A}^c \cap \overline{A^c} = (A \cup L \cup A)^c = \overline{A}^c$, which must be open, by Theorem 3.2.13.
    \end{enumerate}
    
    \exc{3.2.9}
    \begin{enumerate}
        \item For the first equality, 
        \begin{gather*}
            x \in \left (\bigcup_{\lambda \in \Lambda} E_\lambda \right)^c \iff
             \neg(x \in \bigcup_{\lambda \in \Lambda} E_\lambda) \iff \\
             \neg(\exists \lambda \in \Lambda (x \in E_\lambda)) \iff \forall \lambda \in \Lambda (x \in E_\lambda^c) \iff x \in \bigcap_{\lambda \in \Lambda} E_\lambda^c.
        \end{gather*} For the second one,
        \begin{gather*}
            x \in \left( \bigcap_{\lambda \in \Lambda} E_\lambda \right)^c \iff 
            \neg (x \in \bigcap_{\lambda \in \Lambda} E_\lambda) \iff \\
            \neg(\forall \lambda \in \Lambda (x \in E_\lambda)) \iff 
            \exists \lambda \in \Lambda (x \in E_\lambda^c) \iff 
            x \in \bigcup_{\lambda \in \Lambda} E_\lambda^c.
        \end{gather*}
        
        \item For (i), let $\set{E_\lambda : \lambda \in \Lambda}$ be a finite collection of closed sets. Then, 
        \begin{equation*} 
            \left(\bigcup_{\lambda \in \Lambda} E_\lambda \right)^c =
            \bigcap_{\lambda \in \Lambda} E_\lambda^c,
        \end{equation*} but, by Theorem 3.2.13, each $E_\lambda^c$ is open, so a finite intersection of them is also open (Theorem 3.2.3), thus
        \begin{equation*}
            \bigcup_{\lambda \in \Lambda} E_\lambda
        \end{equation*} is closed, again by Theorem 3.2.13. A very similar proof follows for (ii).
    \end{enumerate}
    
    \exc{3.2.10}
    (i) Is not possible. Let $A \subseteq [0, 1]$ be countable. Then, we can form some sequence $(a_n)$ contained in $A$ such that no two $a_n$ are the same. Since $(a_n)$ is bounded, it must have a convergent subsequence $(b_n) \to b$, by the Bolzano-Weierstrass Theorem. If none of the $b_n'$s are equal to $b$, then $b$ is a limit point of $A$, by Theorem 3.2.5. Otherwise, there can only be a single $b_N$ which is equal to $b$, since no two $b_n'$s are the same. Hence, we can form a new sequence $(c_n)$ in the following way:
    \begin{equation*}
        c_n = \begin{cases}
        b_n & n \neq N \\
        b_{N+1} & n = N
        \end{cases}
    \end{equation*} which is contained in $A$, converges to $b$ and satisfies $c_n \neq b$ for all $n \in \N$. This means that $b$ is a limit point of $A$. Thus, every countable subset of $[0, 1]$ contains at least one limit point.
    
    (ii) Is possible. Consider the countable set $A = \set{x \in [0, 1]: x \in \Q}$. We need to show that $A$ has no isolated points, which is the same as showing that every point of $A$ is also a limit point of $A$. To do this, choose some $x \in A$ and \lep[l]. Then, by Theorem 1.4.3, we can find some rational $q \in [0, 1]$ not equal to $x$ such that $q \in V_\epsilon(x)$, therefore $x$ is a limit point of $A$.
    
    (iii) Is not possible. Let $A \subseteq \R$ and $B \subseteq A$ be the subset of all isolated points of $A$. Assume for contradiction that $B$ is uncountable. Then, given any $b \in B$, there must be some $V_{\epsilon_b}(b)$ that intersects $A$ only at $b$, since $b$ is an isolated point of $A$. But then, the collection of each on of those open disjoint neighborhoods would be uncountable, but this is a contradiction as shown in Exercise 1.5.6 (b). Therefore, $B$ is not uncountable.
    
    \exc{3.2.13}
    Let $A \subseteq \R$ be both open and closed. If $A = \emptyset$ then we are done, so assume $A$ is not empty. Now, choose some $a \in A$ and some $V_{\epsilon_0}(a) \subseteq A$. Consider the set $B = \set{b > a: (a, b] \subseteq A}$. $B$ is not empty, since $a + \epsilon_0/2$ is one of its elements. Now, assume for contradiction that $B$ is bounded above, and set $s = \sup{B}$. Also, notice that $(a, s-\epsilon] \subseteq A$ for any $\epsilon > 0$. This clearly implies that $(a, s) \subseteq A$. Assume for contradiction that $s \in A$. Then, $(a,s] \subseteq A$. Since $A$ is open, we can find some $V_{\epsilon_1}(s) \subseteq A$, in other words $(s-\epsilon_1, s+\epsilon_1) \subseteq A$. But this implies $(a,s+\epsilon_1/2] \subseteq A$. But this would mean $s+\epsilon_1/2$ is an element of $B$ which is greater than $s$, which is a contradiction. Thus, $s \notin A$. Next, we show that $s$ is a limit point of $A$. \lep. Since $s$ is the supremum of $B$, there is some $b \in B$ such that $s-\epsilon < b < s+\epsilon$. Since $B \subseteq A$, we have found an element $b$ of $A$ that intersects $V_\epsilon(s)$, and $b \neq s$ since $s$ is not in $A$. Thus, $s$ is a limit point of $A$. But $A$ is closed, so it must contain $s$, which is a contradiction. This means that $B$ is not bounded above, so every interval $[a, b] \subseteq A$. We could similarly construct a set 
    $C = \set{c < a : [c, a) \subseteq A}$ and use the same strategy to show that $C$ is not bounded below, which would mean every $[c, a] \subseteq A$, thus $A = \R$. 
    
    \exc{3.3.1}
    $K$ is nonempty and compact, hence bounded, so both $s := \sup K$ and $t := \inf K$ exist. Then for every $n \in \N$ there is some $k_n \in K$ such t hat $s - 1/n < k_n < s+1/n$, since $s$ is a least upper bound. Then, it is easy to see that $(k_n) \to s$. But, since $K$ is compact, there is some subsequence $(k_{n_a})$ contained in $K$ which converges to a limit $x \in K$. By Theorem 2.5.2, we know $(k_{n_a})$ converges to the same limit as $(k_n)$, therefore $s = x \in K$. A similar argument shows that $t \in K$.
    
    \exc{3.3.2}
    \begin{enumerate}
        \item Not compact. $(1, 2, 3, 4,\dots)$.
        
        \item Not compact. For every $n \in \N$, we can choose $q_n \in Q \cap [0, 1]$ such that $\sqrt{2}/2 < q < \sqrt{2}/2 + 1/(n+10)$. Then, $(q_n)$ clearly converges to $\sqrt{2}/2$, and so does every one of its subsequences. But $\sqrt{2}/2 \notin \Q$, so $\Q \cap [0, 1]$ is not compact.
        
        \item Compact.
        
        \item Not compact. We have already shown in Exercise 3.2.3 (d) that the set $\set{\sum_{k=1}^n 1/k^2 : n \in \N}$ does not contain its supremum, therefore it cannot be compact, by Exercise 3.3.1. The sequence $(a_n) = (1/n^2)$ is contained in $K$ and converges to $\sup K$.
        
        \item Compact.
    \end{enumerate}
    
    \exc{3.3.3}
    Let $(x_n)$ be a sequence contained in $K$. Since $K$ is bounded, so is $(x_n)$, therefore there is some subsequence $(x_{n_k})$ of $(x_n)$ which converges to a real number $x$, by the  Bolzano-Weierstrass Theorem. Since $K$ is closed, we must have $x \in K$ by Theorem 3.2.8.
    
    \exc{3.3.4}
    \begin{enumerate}
        \item Both. This is the intersection of two closed sets, so it must be closed. Also, the intersection of a bounded set with any other set is bounded, so $K \cap F$ is compact.
        
        \item Definitely compact. The complement of a bounded set is
        
        not bounded, so $A = F^c \cup K^c$ is not bounded, and neither is its closure, therefore $\overline{A}$ is closed but not compact.
        
        \item Neither. Let $K = \set{0,1,1/2,1/3,\dots}$ and $F = \set{0}$.
        
        $K\backslash F = \set{1,1/2,1/3,\dots}$ which is not closed, therefore also not compact.
        
        \item Both. $K$ is bounded, so its intersection with anything is also
        
        bounded, and the closure of any set is closed, therefore $\overline{K \cap F^c}$ is compact.
    \end{enumerate}
    
    \exc{3.3.5}
    \begin{enumerate}
        \item True. Since every compact set is closed and bounded, its arbitrary
        intersection is also closed (Theorem 3.2.14) and bounded, therefore compact.
        
        \item False. $[0,1] \cup [1,2] \cup [2,3] \dots$ is not bounded, therefore not compact.
        
        \item False. Let $A = (1,2)$ and $K = [0, 3]$. Then $A \cap K = (1, 2)$, which is not closed, therefore not compact.
        
        \item False. For each $n \in \N$ define 
        \begin{equation*}
            F_n = \bigcup_{k=n}^\infty [2k, 2k+1].
        \end{equation*} To see that every $F_n$ is closed, we use Theorem 3.2.8. Let $(a_n)$ be a Cauchy sequence contained in $F_n$. Then, there is some $N \in \N$ such that $\abs{a_p-a_q} < 1/2$ for all $p, q \geq N$, thus every term of the sequence is eventually in the same interval $[2k, 2k+1]$ for some $k \geq n$. Since $[2k, 2k+1]$ is closed, the limit of $(a_n)$ is in $[2k, 2k+1]$, therefore it is also in $F_n$, so $F_n$ is closed. Now, assume the intersection $\bigcap_{n=1}^\infty F_n \neq \emptyset$. Then, there is some $x \in \R$ such that $x \in F_n$ for all $n \in N$, in particular $x \in F_1$. Then, there must be some $k \geq 1$ such that $x \in [2k, 2k+1]$. But then, $x \notin F_{k+1}$, which is a contradiction. Hence, we can conclude that $\bigcap_{n=1}^\infty F_n = \emptyset$.
    \end{enumerate}
    
    \exc{3.3.6}
    \begin{enumerate}
        \item True for finite and compact sets (Exercise 3.3.1) but false for closed sets, for example, $\R$ has no maximum.
    \end{enumerate}
    
    \exc{3.3.9}
    \begin{enumerate}
        \item Since $I_0 \cap K = K$, it cannot be finitely covered by assumption. Assume that $I_n = [a_n, b_n]$ is already defined for some $n \in \N$ in such a way that $I_n \cap K$ cannot be finitely covered and consider the sets $L_n  := [a_n, (a_n + b_n)/2]$ and $R_n  := [(a_n + b_n)/2, b_n].$ If both $L_n \cap K$ and $R_n \cap K$ could be finitely covered, then the set consisting of the union of both these covers would clearly be a finite cover for $I_n \cap K$, a contradiction. Thus, define $I_{n+1}$ to be one of $L_n, R_n$ whose intersection with $K$ cannot be finitely covered. It is clear that $\lim \abs{I_n} = 0$.
        
        \item  Each $I_n$ contains some $x_n \in K$, otherwise their intersection with it would be empty, thus it could be finitely covered. Since $(\abs{I_n}) \to 0$, the sequence $(x_n)$ is Cauchy, so it must converge to some $x$. Then, we can use the fact that $K$ is closed to conclude that $x \in K$, by Theorem 3.2.8. Since $K \subseteq I_0$, it follows that $x \in I_0$. Also, since $I_n \subseteq I_0$ for each $n \in \N$, $x \in I_n$.
        
        \item Since $x \in O_{\lambda_0}$ and $O_{\lambda_0}$ is open, there is some $\epsilon_0 > 0$ such that $V_{\epsilon_0}(x) \subseteq O_{\lambda_0}$. Since $\lim \abs{I_n} = 0$, there must be some $N \in \N$ such that $\abs{I_N} < \epsilon_0/2$. But we've already shown that $x \in I_N$, so $I_N \subseteq V_{\epsilon_0}(x) \subseteq O_{\lambda_0}$, therefore $\set{O_{\lambda_0}}$ is a finite subcover for $K \cap I_N$, contradicting the assumption that none of the $K \cap I_n$'s can be finitely subcovered.
    \end{enumerate}
    
    \exc{3.4.6}
    Let $E \subseteq \R$ be connected. Assume $A,B \subseteq \R$ are disjoint nonempty sets that satisfy $E = A \cup B$. First, assume that $A$ contains a limit point of $x$ of $B$. Then, there is a sequence $(x_n)$ contained in $B$ that converges to $x$, a limit point of $B$ that is contained in $A$. Next, assume $A$ does not contain a limit point of $B$. Since $E$ is connected, $B$ must contain a limit point $x$ of $A$. In that case, there is a sequence $(x_n) \subseteq A$ which converges to $x$, which is in $B$.
    
    For the reverse implication assume that for all nonempty disjoint sets $A$ and $B$ satisfying $E = A \cup B$, there always exists a convergent sequence $(x_n) \to x$ with $(x_n)$ contained in one of $A$ or $B$, and $x$ an element of the other. Choose arbitrary nonempty disjoint sets $A$ and $B$ such that $E = A \cup B$. Assume that there is no sequence contained in $A$ whose limit exists and is in $B$. Then, there must be a sequence $(x_n) \subseteq B$ where $(x_n) \to x$ and $x \in A$. Since $A$ and $B$ are disjoint and $x \in A$, then $x \notin B$. Also, $(x_n)$ is contained in $B$ so none of the $x_n$ are equal to $x$, which $x$ is a limit point of $B$ and is an element of $A$. The argument is almost identical for the other case.
    
    \exc{4.2.1}
    \begin{enumerate}
        \item Let $(x_n) \to c$ and $(y_n) \to c$ be arbitrary sequences satisfying $x_n,y_n \neq c$. By Theorem 4.2.3, we must have $\lim{f(x_n)} = L$ and 
        $\lim{g(y_n)} = M$, which implies $\lim{f(x_n)} + \lim {g(y_n)} = L + M = \lim(f(x_n) + g(y_n))$, by the Algebraic Limit Theorem. By Theorem 4.2.3 again, this means $\lim_{x \to c} [f(x)+g(x)] = L + M$.
        
        \item \lep. Choose $\delta_1$ such that $\abs{f(x)-L} < \epsilon/2$ whenever $0 < \abs{x-c} < \delta_1$. Similarly, choose $\delta_2$ such that $\abs{g(x)-M} < \epsilon/2$ whenever $0 < \abs{x-c} < \delta_2$. Now, set $\delta = \min{\delta_1, \delta_2}$. Then, whenever $0 < \abs{x-c} < \delta$ we have
        \begin{equation*}
            \abs{f(x)+g(x)-L-M} \leq \abs{f(x)-L} + \abs{g(x)-M} <
            \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon,
        \end{equation*} so we can conclude that $\lim_{x \to c} [f(x) + g(x)] = L + M$.
        
        \item The sequential version of the argument is very similar to the one given in (a), so we show only the proof using Definition 4.2.1 directly.
        \begin{enumerate}
        \setcounter{enumiii}{2}
            \item \lep. Choose $\delta_1 > 0$ such that $\abs{g(x)-M} < 1$ whenever $0 < \abs{x-c} < \delta_1$. This implies $\abs{g(x)} < 1 + \abs{M}$ for the appropriate $x$. Now, choose $\delta_2 > 0$ and $\delta_3 > 0$ such that $\abs{g(x)-M} < \epsilon/(2 \abs{L})$ whenever $0 < \abs{x-c} < \delta_2$ and $\abs{f(x)-L} < \epsilon/(2(1+\abs{M}))$ when $0 < \abs{x-c}<\delta_3$. Now, set $\delta = \min{\delta_1, \delta_2, \delta_3}$. It follows that 
            \begin{align*}
                \abs{f(x)g(x) - L M} &= \abs{f(x)g(x) -L g(x) + L g(x)- L M} \\
                &\leq \abs{g(x)}\abs{f(x)-L} + \abs{L}\abs{g(x)-M} \\
                & < (1 + \abs{M}) \frac{\epsilon}{2(1+\abs{M})} + \abs{L} \frac{\epsilon}{2 \abs{L}} \\ &= \epsilon
            \end{align*} whenever $0 < \abs{x-c} < \delta$, therefore $\lim_{x \to c} [f(x)g(x)] = L M$.
            
            \item It is sufficient to show that $\lim_{x \to c} 1/g(x) = 1/M$. \lep. Choose $\delta_1 > 0$ such that $\abs{g(x)-M} < \abs{M}/2$ for all $0 < \abs{x-c}<\delta_1$. It follows that $\abs{g(x)} > \abs{M}/2 > 0$ for the appropriate $x$. Now, choose $\delta_2 > 0$ such that $\abs{g(x) - M} < \epsilon \abs{M}^2/2$ for $x$ in the expected interval. Setting $\delta := \min (\delta_1, \delta_2)$, it follows that 
            \begin{equation*}
                \abs{\frac{1}{g(x)}-\frac{1}{M}} = \abs{\frac{g(x)-M}{g(x)M}} <
                \frac{2 \abs{g(x)-M}}{\abs{M}^2} < \epsilon
            \end{equation*} whenever $0 < \abs{x-c}<\delta$.
        \end{enumerate}
    \end{enumerate}
    
    \exc{4.2.2}
    \begin{enumerate}
        \item $\delta = 1/5$.
        \item $\delta = 3$.
        \item $\delta = \pi - 3$.
        \item $\delta = \pi - 3$.
    \end{enumerate}
    
    \begin{shortlemma} \label{lem_rationalAproximations}
        Let $a \in \R$ and $(p_n/q_n) \to a$ be a sequence of rational numbers, satisfying the following conditions for each $n \in \N$:
        \begin{itemize}
            \item $p_n, q_n \in \Z$,
            \item $p_n/q_n \neq a$.
        \end{itemize} Then, the sequence $(1/q_n)$ converges to zero.
    \end{shortlemma}
    
    \begin{proof}
         Let $(p_n/q_n)$ be a sequence that follows the the required conditions. It is sufficient to show that $(q_n)$ is not bounded. Assume for contradiction that $(q_n)$ is bounded, in other words, there is some $M \in \R$ such that $\abs{q_n} \leq M$ for all $n \in \N$. Since $(p_n/q_n)$ is Cauchy, there is some $N \in \N$ such that $\abs{p_n/q_n-p_m/q_m} < 1/M^2$ for all $n,m \geq N$. Also, there must be some $N_1, N_2 \geq N$ such that $p_{N_1}/q_{N_1} \neq p_{N_2}/q_{N_2}$, if that were not the case it would mean that the sequence is eventually constant, which would contradict the fact that none of its terms are equal to $a$. Then, we have 
         \begin{equation*}
             \abs{\frac{p_{N_1}}{q_{N_1}}-\frac{p_{N_2}}{q_{N_2}}} = 
             \abs{\frac{p_{N_1} q_{N_2}-p_{N_2}q_{N_1}}{q_{N_1} q_{N_2}}}.
         \end{equation*} Notice that $p_{N_1} q_{N_2}-p_{N_2}q_{N_1}$ is an integer different than zero,
         
        \noindent since $p_{N_1}/q_{N_1} \neq p_{N_2}/q_{N_2}$, therefore its absolute value must be greater than or equal to one. Using the fact that $\abs{q_{N_1} q_{N_2}} \leq M^2$, 
         \begin{equation*}
             \abs{\frac{p_{N_1} q_{N_2}-p_{N_2}q_{N_1}}{q_{N_1} q_{N_2}}} \geq 
             \frac{1}{\abs{q_{N_1} q_{N_2}}} \geq \frac{1}{M^2}
         \end{equation*} which contradicts the fact that $\abs{p_{N_1}/q_{N_1}-p_{N_2}/q_{N_2}} < 1/M^2$. Thus, $(\abs{q_n})$ is not bounded, so its inverse must converge to zero.
    \end{proof}
    
    
    \exc{4.2.3}
    \begin{enumerate}
        \item $(x_n) = (1+1/n)$, $(y_n) = (1+(-1)^n/n)$, $(z_n) = (1+\sqrt{2}/n)$.
        
        \item $\lim(t(x_n)) = \lim(t(y_n)) = \lim(t(z_n)) = 0$.
        
        \item We claim that $\lim_{x \to 1} t(x) = 0$. \lep. If $\epsilon > 1$, then $\abs{t(x)} = t(x) < \epsilon$ for all $x$, so we can set $\delta$ to any positive real number. Otherwise, choose some arbitrary $a \in A = \set{x \in \R : t(x) \geq \epsilon}$. Notice that every $b \in A$ is rational, otherwise we would have $t(b) = 0 < \epsilon$. Assume $a$ is a limit point of $A$. Then, for every $n \in \N$ there is some $x_n \in A$ such that $x_n \in V_{1/n}(a)$ and $x_n \neq a$. Since $x_n \in \Q$, we can write it as $p_n/q_n$ for integers $p_n/q_n$ in the lowest terms, such that $q_n > 0$. Then, Lemma (\ref{lem_rationalAproximations}) guarantees that there is some $N \in \N$ such that $1/q_N < \epsilon$. But, $t(x_N) = t(p_N/q_N) = 1/q_N$ which should be greater than or equal to $\epsilon$, since $x_N \in A$. This contradiction means that $A$ has no limit points, only isolated points.
        
        Notice that $1 \in A$, since $t(1) = t(1/1) = 1 \geq 1$ (recall that we are assuming that $\epsilon \leq 1$). By our previous discussion, $1$ must be an isolated point of $A$. This means that there is some $V_\delta(1)$ which does not intersect $A$, except possibly at $1$. In other words, for every $x \in V_\delta(1)$ (except $x = 1$), we must have $t(x) = \abs{t(x)-0} < \epsilon$, which we can also write as $t(x) \in V_\epsilon(0)$. By definition 4.2.1B, we can conclude that $\lim_{x \to 1} t(x) = 0$, just as we conjectured.
    \end{enumerate}
    
    \exc{4.2.4}
    \begin{enumerate}
        \item $\delta = 8$.
        \item $\delta = 1$.
        \item $\delta = 1/9-1/10$.
    \end{enumerate}
    
    \exc{4.2.5}
    \begin{enumerate}
        \item \lep. Choose $\delta = \epsilon/3$. Then, $\abs{x-2} < \delta = \epsilon/3$ implies $\abs{3x-6} = \abs{(3x+4) - 10} < \epsilon$.
        
        \item \lep. Set $\delta = \min{1, \epsilon}$. Then, $\abs{x-0} < \delta$ implies $\abs{x} < 1$, therefore $\abs{x}^3 < \abs{x}^2 < \abs{x} < \delta$, which also implies $\abs{x^3 - 0} < \epsilon$.
        
        \item \lep. Choose $\delta = \min{1, \epsilon/6}$. Now, choose an arbitrary $x$ such that $0 < \abs{x-2} < \delta$. It follows that $\abs{x+3} < 6$. Then, $\abs{(x^2+x-1)-5} = \abs{x+3}\abs{x-2} < 6\abs{x-2} < \epsilon$.
        
        \item \lep. Choose $\delta = \min{1, 6\epsilon}$. Now, choose an arbitrary $x$ such that $0 < \abs{x-3} < \delta$. It follows that $\abs{3x} > 6$. Then,
        \begin{equation*}
            \abs{\frac{1}{x}-\frac{1}{3}} = \abs{\frac{x-3}{3x}} < \frac{\abs{x-3}}{6} < \epsilon.
        \end{equation*}
    \end{enumerate}
    
    \exc{4.2.6}
    \begin{enumerate}
        \item True, since $0 < \abs{x-c} < \delta' \implies 0 < \abs{x-c} < \delta$, for any $0 < \delta' < \delta$.
        
        \item False. For example, consider the function $f : \R \to \R$ that satisfies
        \begin{equation*}
            f(x) = \begin{cases}
            1 & x = 0 \\
            0 & x \neq 0.
            \end{cases}
        \end{equation*}Then, $f(0) = 1$, but $\lim_{x \to 0} f(x) = 0$.
        
        \item True, by several applications of the Algebraic Limit Theorem for Functional Limits.
        
        \item False. Consider the functions $f(x) = (x-a)/x$ and $g(x) = x/(x-a)$. Then, $\lim_{x \to a} f(x) = 0$, but $\lim_{x \to a} f(x)g(x) = 1$.
    \end{enumerate}
    
    \exc{4.2.7}
    \lep. Choose $\delta$ such that $\abs{g(x)} < \epsilon/M$ for an arbitrary $x$ satisfying $0 < \abs{x-c} < \delta$. Then, $\abs{f(x)g(x) - 0} \leq M\abs{g(x)} < \epsilon$.
    
    \exc{4.2.8}
    \begin{enumerate}
        \item The limit does not exist. Let $f(x) = \abs{x-2}/(x-2)$ and consider the sequences $(a_n) = (2+1/n)$ and $(b_n) = (2-1/n)$. Both of them converge to $2$ and none of their terms are equal to $2$. However, $\lim f(a_n) = 1$ but $\lim f(b_n) = -1$. By Corollary 4.2.5, we can conclude that the respective limit does not exist.
        
        \item We claim that the limit is $-1$. To see that, \lep[l]. Choose $\delta = 1/4$ and pick an arbitrary $x$ satisfying $0 < \abs{x-7/4} < \delta$. It follows that $x-2 < 0$, therefore $\abs{x-2}/(x-2) = -(x-2)/(x-2) = -1$. Then,
        \begin{equation*}
            \abs{\frac{\abs{x-2}}{x-2} - (-1)} = \abs{-1 + 1} = 0 < \epsilon.
        \end{equation*}
        
        \item The limit does not exist.  Let $f(x) = (-1)^{\floor{1/x}}$ and consider the sequences $(a_n) = (1/(2n))$ and $(b_n) = (1/(2n+1))$. Both of them converge to $0$ and none of their terms are equal to $0$. Notice that $\lim f(a_n) =
        \lim (-1)^{2n} = 1$, whereas $\lim f(b_n) = -1$. By Corollary 4.2.5, the limit does not exist.
        
        \item We claim that the limit is $0$. To see that, \lep[l]. Choose $\delta = \epsilon^3$. Then, for an arbitrary $x$ satisfying $0 < \abs{x} < \delta$, we can conclude that $\abs{\sqrt[3]{x}} < \epsilon$.
    \end{enumerate}
    
    \exc{4.2.9}
    \begin{enumerate}
        \item Let $M > 0$ be arbitrary. Make $\delta := 1/\sqrt{M}$. Then, for an arbitrary $x$ satisfying $0 < \abs{x} < \delta$, we can conclude that $x^2 <1/M$, which implies $1/x^2 > M$.
        
        \item \em{Definition:}\em \space $\lim_{x \to \infty} f(x) = L$ means that for all $\epsilon > 0$, we can find $N > 0$ such that $\abs{f(x) - L} < \epsilon$ whenever $x > N$.
        
        \lep. Choose $N$ such that $N > 1/\epsilon$. Then, for every $x > N$ we can conclude that, $\epsilon > 1/x = \abs{1/x-0}$.
        
        \item \em{Definition:}\em \space $\lim_{x \to \infty} f(x) = \infty$ means that for all $M > 0$, we can find $N > 0$ such that $f(x) > M$ whenever $x > N$.
        
        To see that $\lim_{x \to \infty} x = \infty$, let $M > 0$ be arbitrary and set $N := M$. Then, $x > N$ implies $x > M$.
    \end{enumerate}
    
    \exc{4.2.10}
    \begin{enumerate}
        \item \em{Definition:}\em \space $\lim_{x \to c^+} f(x) = L$ means that for all $\epsilon > 0$, we can find $\delta > 0$ such that whenever  $0 < x-c < \delta$ it follows that $\abs{f(x) - L} < \epsilon$.
        
        \noindent \em{Definition:}\em \space $\lim_{x \to c^-} f(x) = L$ means that for all $\epsilon > 0$, we can find $\delta > 0$ such that whenever  $0 < c-x < \delta$ it follows that $\abs{f(x) - L} < \epsilon$.
        
        \item Assume $\lim_{x \to c} f(x) = L$ and \lep[l]. By assumption, we can choose $\delta$ such that $\abs{f(x)-L} < \epsilon$ whenever $0 < \abs{x-c} < \delta$. But, $0 < \abs{x-c}<\delta$. But this implies that whenever $0 < x-c < \delta$, then $\abs{f(x)-L} < \epsilon$, which means $\lim_{x \to c^+} f(x) = L$. Similarly, we can conclude that whenever $0 < c-x < \delta$ then $\abs{f(x)-L} < \epsilon$, therefore $\lim_{x \to c^-} f(x) = L$.
        
        For the converse, assume both the right ant left-hand limits equal $L$. Now, let $\epsilon > 0$ be arbitrary. Choose $\delta_1$ such that $\abs{f(x)-L} < \epsilon$ whenever $0 < x-c < \delta_1$. Similarly, choose $\delta_2$ such that $\abs{f(x)-L} < \epsilon$ whenever $0 < c-x < \delta_2$. Setting $\delta := \min{\delta_1, \delta_2}$, it follows that $\abs{f(x)-L} < \epsilon$ whenever $0 < \abs{x-c} < \delta$.
    \end{enumerate}
    
    \exc{4.2.11}
    Let $(x_n) \to c$ be a sequence contained in $A$ such that none of its terms are equal to $c$. By the Sequential Criterion For Functional Limits, it follows that $\lim f(x_n) = \lim h(x_n) = L$. Since for every $n \in \N$ it we have $f(x_n) \leq g(x_n) \leq h(x_n)$, the Squeeze Theorem for Sequences (Exercise 2.3.3) lets us conclude that $\lim h(x_n) = L$. Applying the Sequential Criterion For Functional Limits once again, it follows that $\lim_{x \to c} h(x) = L$.
    
    \exc{4.3.1}
    \begin{enumerate}
        \item \lep. Set $\delta := \epsilon^3$. Then, $\abs{x} < \delta$ implies 
        $\abs{\sqrt[3]{x}-0} < \epsilon$.
        
        \item Assume $c > 0$. Choose $\delta = \min { 1, \epsilon ((c-1)^{2/3} + \sqrt[3]{c^2-c} + c^{2/3})}$. It is not hard to check that $x^{2/3} + \sqrt[3]{x c} + c^{2/3} > (c-1)^{2/3} + \sqrt[3]{c^2-c} + c^{2/3}$ whenever $\abs{x-c} < 1 \leq \delta$. Then, 
        \begin{align*}
           \abs{\sqrt[3]{x}-\sqrt[3]{c}} &= \abs{\sqrt[3]{x}-\sqrt[3]{c}} \abs{\frac{x^{2/3} + \sqrt[3]{x c} + c^{2/3}}{x^{2/3} + \sqrt[3]{x c} + c^{2/3}}} \\
            &= \frac{\abs{x-c}}{x^{2/3} + \sqrt[3]{x c} + c^{2/3}} \\
            &< \frac{\abs{x-c}}{(c-1)^{2/3} + \sqrt[3]{c^2-c} + c^{2/3}} < \epsilon
        \end{align*} whenever $\abs{x-c} < \delta$. Now, assume $c < 0$. Choose $\delta$ such that $\abs{\sqrt[3]{x}-\sqrt[3]{-c}} < \epsilon$ whenever $\abs{x-c} < \delta$. Using the fact that $\sqrt[3]{-y} = -\sqrt[3]{y}$ for any $y \in \R$, we can conclude that $\abs{\sqrt[3]{x}-\sqrt[3]{-c}} = \abs{(-\sqrt[3]{x}) - \sqrt[3]{c}} < \epsilon$. This means that the function $(-1) g(x)$ is continuous at $c$, but Theorem 4.3.4 allows us to conclude that $(-1) (-1) g(x) = g(x)$ must also be continuous at $c$.
    \end{enumerate}
    
    \exc{4.3.2}
    \begin{enumerate}
        \item $f(x) = 1$.
        
        \item $f(x) = x$.
        
        \item There is no such function. If a function $f$ is lesstinuous at $c$, then, by definition, $\abs{f(x)-f(c)} < \epsilon$ whenever $\abs{x-c} < \delta < \epsilon$. But this clearly implies that whenever $\abs{x-c} < \epsilon$ then $\abs{f(x)-f(c)} < \epsilon$, therefore $f$ is equaltinuous at $c$.
        
        \item \lep. First assume $f$ is lesstinuous at $c$. Then, there is some $\delta > 0$ such that $\abs{f(x)-f(c)} < \epsilon$ whenever $\abs{x-c} < \delta$. But this is exactly what we mean when saying $f$ is continuous at $c$.
        
        Now, assume $f$ is continuous at $c$. Then, there is some $\delta_1 > 0$ such that $\abs{f(x)-f(c)} < \epsilon$ whenever $\abs{x-c} < \delta_1$. Now, set $\delta := \min{\epsilon/2, \delta_1}$. Then, $\abs{f(x)-f(c)} < \epsilon$ whenever $\abs{x-c} < \delta$ and $\delta \leq \epsilon/2 < \epsilon$, therefore $f$ is lesstinuous at $c$.
        Thus, a function is continuous if and only if it is lesstinuous.
        
    \end{enumerate}
    
    \exc{4.3.3}
    \begin{enumerate}
        \item \lep. Use the continuity of $g$ at $f(c)$ to choose a $\delta_2 > 0$ such that $\abs{g(x)-g(f(c))} < \epsilon$ whenever $\abs{x-f(c)} < \delta_2$. Now, use the continuity of $f$ at $c$ to choose $\delta_1$ such that $\abs{f(x)-f(c)} < \delta_2$ for all $\abs{x-c} < \delta_1$. Choose an arbitrary $x$ such that $\abs{x-c} < \delta_1$. It follows that $\abs{f(x)-f(c)} < \delta_2$. This implies $\abs{g(f(x))-g(f(c))} < \epsilon$, as we wanted to show.
        
        \item Let $(x_n) \to c$ be a sequence contained in $A$. We need to show that $\lim g(f(x_n)) = g(f(c))$. Since $f$ is continuous at $c$, it follows that $\lim f(x_n) = f(c)$. Thus, $f(x_n)$ is a sequence contained in $B$ that converges to $f(c)$, which, by the continuity of $g$ at $f(c)$, implies $\lim g(f(x_n)) = c$, as we wanted to show.
    \end{enumerate}
    
    \exc{4.3.4}
    \begin{enumerate}
        \item Set $f(x) = 1$ and 
        \begin{equation*}
            g(x) = \begin{cases}
            1 & x = 1 \\
            0 & x \neq 1.
            \end{cases}
        \end{equation*} Then, $\lim_{x \to p} f(x) = 1 = q$ and $\lim_{x \to q} g(x) = 0 = r$, but 
        
        \noindent $\lim_{x \to p} g(f(x)) = g(1) = 1 \neq r$.
        
        \item Since $\R$ has no isolated points and $f$ is continuous, $\lim_{x \to p} f(x) = f(p)$. For the same reasons, $\lim_{x \to f(p)} g(x) = g(f(p)) = r$. By Theorem 4.3.9, $g(f(x))$ is continuous at $p$, so as long as $p$ is not an isolated point of the range of $f$, it follows that $\lim_{x \to p} g(f(x)) = g(f(p)) = r$. 
        
        \item As shown in the example we gave in (a), $f$ being continuous is not enough for the result to hold. Now, assume that $g$ is continuous. We can conclude that $\lim_{x \to q} g(x) = g(q) = r$. Then, to show that the result in (a) holds, it is sufficient to show that $\lim_{x \to p} g(f(x)) = g(q)$. To do that, \lep[l] \space and choose $\delta_1$ such that $\abs{g(x)-g(q)} < \epsilon$ whenever $\abs{x-q} < \delta_1$. Next, pick some $\delta > 0$ such that $\abs{f(x)-q} < \delta_1$ whenever $\abs{x-p} < \delta$. Now, choose an arbitrary $x$ such that $\abs{x-p} < \delta$. It follows that $\abs{f(x)-q} < \delta_1$, which means $\abs{g(f(x))-g(q)} < \epsilon$, therefore $\lim_{x \to p} g(f(x)) = g(q)$, as we wanted to show.
    \end{enumerate}
    
    \exc{4.3.5}
    \lep. Since $c$ is an isolated point of $A$, there is some $V_\delta (c)$ which only intersects $A$ at $c$. Thus, the only $x$ that satisfies $\abs{x-c} < \delta$ is $c$, so $\abs{f(x)-f(c)} = 0 < \epsilon$ whenever $\abs{x-c} < \delta$.
    
    \exc{4.3.6}
    \begin{enumerate}
        \item 
        \begin{align*}
            f(x) &= \begin{cases}
            2 & x = 0 \\ 
            1 & x \neq 0
            \end{cases} \\
            g(x) &= \begin{cases}
            1 & x = 0 \\ 
            2 & x \neq 0
            \end{cases}
        \end{align*}
        
        \item Not possible. By Theorem 4.3.4, $(f(x)+g(x))-f(x) = g(x)$ must be continuous at zero.
        
        \item $f(x) = 0$ and $g(x)= \floor{x}$.
        
        \item \begin{equation*}
            f(x) = \begin{cases}
            2 & x = 0 \\ 
            \frac{1}{2} & x \neq 0.
            \end{cases}
        \end{equation*}
        
        \item Not possible. Assume $f(x)^3$ is continuous at $0$. We've shown in Exercise $4.3.1$ that $g(x)=\sqrt[3]{x}$ is continuous at $0$, so Theorem 4.3.9 guarantees that $g(f(x)^3) = f(x)$ is continuous at $0$. The contrapositive of we just proved is that if $f$ is not continuous at $0$, then $f(x)^3$ is also not continuous at $0$.
    \end{enumerate}
    
    \exc{4.3.7}
    \begin{enumerate}
        \item Let $x \in \R$ be arbitrary. If $x \in \Q$, construct a sequence of $(x_n) \to x$ where every $x_n$ is irrational. This can be done since $\I$ is dense in $\R$. Then, $g(x_n) = 0$ for each $n \in \N$, therefore $\lim g(x_n) = 0 \neq g(x)$, since $g(x) = 1$. By Corollary 4.3.3, $g$ is not continuous at $x$. A very similar argument works if $x \notin \Q$.
        
        \item Let $x \in \Q$. It is clear that $t(x) > 0$. Now, construct a sequence $(x_n) \to x$ such that every $x_n$ is irrational. Then, $\lim t(x_n) = 0$, but $t(x) \neq 0$, so Corollary 4.3.3 guarantees that $t$ is not continuous at $x$.
        
        \item Let $x \notin \Q$ be arbitrary and let $(x_n) \to x$ be a sequence. We need to show that $\lim t(x_n) = t(x) = 0$. To do that, \lep[l] \space and assume for sake of contradiction that there are infinitely many $x_n$ such that $t(x_n) \geq \epsilon$. Then, we can form a subsequence $(x_{n_k})$ of $(x_n)$, which must also converge to $x$, consisting only of the terms in $(x_n)$ that satisfy $t(x_n) \geq \epsilon$. Since $\epsilon > 0$, none of the $x_{n_k}$ are irrational, which means they are all not equal to $x$. Then, we can write $x_{n_k} = p_k/q_k$ for each $k \in \N$, where $p_k, q_k$ are integers with $q_k > 0$. By Lemma (\ref{lem_rationalAproximations}), the sequence $(1/q_k)$ converges to zero. Thus, there is some $M \in \N$ such that $1/q_M < \epsilon$. But, by the construction of $(x_{n_k})$, $t(x_{n_M}) = t(p_M/q_M) = 1/q_M \geq \epsilon$, a contradiction.
        
        We can conclude that there are only finitely many $x_n$ for which $t(x_n) \geq \epsilon$. In other words, there is some $N \in \N$ such that $\abs{t(x_n)} = t(x_n) < \epsilon$ for all $n \geq N$, therefore $\lim t(x_n) = 0 = t(x)$, so $t$ is continuous at $x$.
    \end{enumerate}
    
    \exc{4.3.8}
    \begin{enumerate}
        \item Since $g$ is continuous, it follows that $\lim_{x \to 1^-} g(x) = g(1)$. \lep. Then, there is a $\delta > 0$ such that $\abs{g(1)-g(x_0)} < \epsilon$ for some $x_0$ where $0 < 1-x_0 < \delta$. Since $x_0 < 1$, we must have $g(x_0) \geq 0$. Then, $\abs{g(1)-g(x_0) } < \epsilon \implies -\epsilon \leq g(x_0)-\epsilon < g(1)$. Thus, we have shown that for any $\epsilon > 0$, it follows that $g(1) > 0 -\epsilon$, which must mean $g(1) \geq 0$.
        
        \item Choose some $t \in \R$. If $t \in \Q$, then $g(t) = 0$ by construction, so assume $t \notin \Q$. Since $g$ is continuous, we know that $\lim_{x \to t} g(x) = g(t)$, so our goal is to show that $g(t) = 0$. To do that, \lep[l] \space and choose $\delta > 0$ such that $\abs{g(x)-g(t)} < \epsilon$ whenever $\abs{x-t} < \delta$. Since $\Q$ is dense in $\R$, we can choose $q \in \Q$ such that $\abs{q-t} < \delta$. Then, $g(q) = 0$ by construction, so $\abs{g(q)-g(t)} = \abs{g(t)} < \epsilon$. Since $\abs{g(t)}$ is smaller than any positive number, we must have $g(t) = 0$.
        
        \item Since $g$ is continuous at $x_0$, it follows that $\lim_{x \to x_0} g(x) = g(x_0) > 0$. Then, we can find $\delta > 0$ such that $\abs{g(x)-g(x_0)} < g(x_0)$ whenever $0 < \abs{x-x_0} < \delta$. Notice that,  $\abs{g(x)-g(x_0)} < g(x_0) \iff$
        
        \noindent $0 < g(x) < 2g(x_0)$. Since there are uncountably many $x$ that satisfy $0 < \abs{x-x_0} < \delta$, the result follows.
    \end{enumerate}
    
    \exc{4.3.9}
    Let $x$ be a limit point of $K$. Then, there is a sequence $(x_n) \to x$ contained in $K$ such that none of the $x_n$ are equal to $x$. Since $h$ is continuous, Theorem 4.3.2 (iii) guarantees that $(h(x_n)) \to h(x)$. But, $h(x_n) = 0$ for every $n \in \N$, so $(h(x_n)) \to 0$. We can conclude that $h(x) = 0$, therefore $x \in K$, so $K$ is closed.
    
    \begin{shortlemma} \label{lem_evenOddContinuous}
        Let $f$ be an even function (i.e, $f(-x) = f(x)$ for all $x \in \R$) and $g$ be an odd function ($g(-x) = -g(x)$ for all $x \in \R$). Then, if $f$ is continuous at $c \in \R$ for all $c \geq 0$, then $f$ is continuous everywhere. Similarly, if $g$ is continuous at all $c \geq 0$ it is also continuous everywhere.
    \end{shortlemma}
    
    \begin{proof}
         Let $c \in \R$ be negative. Notice that the function $h(x)=-x$ is continuous at $c$ and $f$ is continuous at $h(c)=-c$. Then, by Theorem 4.3.9, $f \circ g$ is continuous at $c$. But, $f \circ g = f$, so $f$ is continuous everywhere. The proof is similar for $g$.
    \end{proof}
    
    \exc{4.3.10}
    \begin{enumerate}
        \item For each $n \in \N$ define $g_n(x) = \max{f_1(x), f_2(x), \dots, f_n(x)}$. We show that every $g_n$ is continuous by induction. Notice that $g_1 = f_1$, which is continuous by assumption. Assume inductively that $g_n$ is continuous. Then, $g_{n+1}(x) = \max{g_n(x),f_{n+1}(x)}$. By the formula given in the exercise, we can write 
        $$ g_{n+1}(x) = \frac{1}{2} (g_n(x)+f_{n+1}(x) + \abs{g_n(x)-f_{n+1}(x)}).$$ Since $g_n$, $\abs{x}$ and $f_{n+1}$ are all continuous, Theorem 4.3.4 and 4.3.9 guarantee that $g_{n+1}$ is also continuous.
        
        \item First, we show that $f_n$ is continuous everywhere for every $n \in \N$. Choose arbitrary $c \in \R$ and $n \in \N$. Since $f$ is even, it is sufficient to show that $f$ is continuous at every nonnegative real, by Lemma (\ref{lem_evenOddContinuous}). First, assume $0 < c < 1/n$. Then, $f_n(c) = n c$. \lep. Let $\delta = \min{n \epsilon, c/2}$. Now, choose an arbitrary $x$ such that $\abs{x-c} < \delta$. It follows that $x > c/2 > 0$, so $\abs{x}=x$. Then, $\abs{f_n(x)-f_n(c)} = \abs{n x-n c} = n \abs{x-c} < \epsilon$. Thus, $f_n$ is continuous at $c$. If $c = 1/n$, it is easy to verify with left and right limits that $f$ is continuous at $c$, so assume $c > 1/n$. Then, $\delta = c-1/n$ works for any $\epsilon > 0$.
        
        To compute $h(x)$, first assume $\abs{x} = 0$. Then, $\abs{0} < 1/n$ for all $n \in \N$, so $f_n(0) = n 0 = 0$. Thus, $h(0) = 0$. Now, let $\abs{x} > 0$. Choose $N \in \N$ such that $N \geq 1/\abs{x}$. Then, $f_N(x) = 1$, and, since $1$ none of the $f_n$ are ever greater than $1$, it follows that $h(x) = 1$. Thus, 
        \begin{equation*}
            h(x) = \begin{cases}
            0 & x = 0 \\ 
            1 & x \neq 0.
            \end{cases}
        \end{equation*} Therefore $h$ is not continuous at $0$.
    \end{enumerate}
    
    \exc{4.3.11}
    \begin{enumerate}
        \item Let $a \in \R$ be arbitrary and \lep[l]. Set $\delta = \epsilon/c$. Now, for any $x$ such that $\abs{x-a} < \delta$ if follows that $\abs{f(x)-f(a)} \leq c \abs{x-a} < \epsilon$.
        
        \item First, we show that $\abs{y_{n+m}-y_n} \leq c^{n-1}\abs{y_{m+1}-y_1}$. We fix $m \in \N$ and induct on $n \in \N$. The base case is $\abs{y_{m+1}-y_1} \leq \abs{y_m+1-y_1}$, which is trivially true. Now, assume inductively that $\abs{y_{n+m}-y_n} \leq c^{n-1}\abs{y_{m+1}-y_1}$. Then, 
        \begin{equation*}
            \abs{y_{n+m+1} - y_{n+1}} = \abs{f(y_{n+m})-f(y_n)} \leq 
            c \abs{y_n+m-y_n} \leq c^n \abs{y_{m+1}-y_1},
        \end{equation*} as we wanted to show. Next, we show that $\abs{y_{m+1} - y_1} \leq \abs{y_2-y_1} (1-c^m)/(1-c)$ for all $m \in \N$, again using induction. The base case is trivial, so assume the inductive hypothesis. Then, 
        \begin{align*}
            &\abs{y_{m+2}-y_1} \leq \abs{y_{m+2}-y_{m+1}} + \abs{y_{m+1}-y_1} \leq\\
            &c^m \abs{y_2-y_1} + \abs{y_2-y_1} \frac{1-c^m}{1-c} =
            \abs{y_2-y_1} \frac{1-c^{m+1}}{1-c}.
        \end{align*} Combining both results, we can conclude that 
        \begin{equation*}
            \abs{y_{n+m}-y_n} \leq c^{n-1} \abs{y_2-y_1} \frac{1-c^m}{1-c}.
        \end{equation*} Since $\abs{c} < 1$, it is easy to see that the right hand side of the previous inequation gets arbitrarily small as $n$ gets large. In other words, for any $\epsilon > 0$ we can choose $N \in \N$ such that 
        \begin{equation*}
            \abs{y_{n+m}-y_n} \leq c^{n-1} \abs{y_2-y_1} \frac{1-c^m}{1-c} \leq \epsilon
        \end{equation*} for all $n \geq N$ and all $m \in \N$. This shows that $(y_n)$ is Cauchy.
        
        \item Since $(y_n) \to y$, $\lim f(y_n) = y$, by the sequential criterion for continuity. Also, $\lim y_{n+1} = \lim y_n = y$, so we can take the limit on both sides of the equation $y_{n+1} = f(y_n)$ to get $y = f(y)$.
        
        Now assume there is some $x \in \R$ such that $f(x) = x$. Then, $\abs{f(x)-f(y)} = \abs{x-y} \leq c \abs{x-y}$. Since $c < 1$, this can only happen if $\abs{x-y} = 0$, in other words, $x = y$.
        
        \item Choose some arbitrary $x_1 \in \R$. Then, consider the sequence $(x_n)$ defined $x_{n+1} = f(x_n)$. We have already shown that this sequence converges to some real number $x$, and that $f(x) = x$, but, as shown in the previous item, this must mean $x = y$. Thus, $(x_n)$ converges to $y$.
    \end{enumerate} 
    
    \exc{4.3.12}
    First, we show that $g(x) \in C_x = \set{\abs{x-a} : a \in F}$ for all $x \in \R$. To do that, it is sufficient to show that $C_x$ is closed, since nonempty closed sets that are bounded below contain their infimum and $C_x$ is bounded below by $0$. Let $x \in \R$ be arbitrary and let $c$ be a limit point of $C_x$. Then, for every $n \in \N$ there is some $c_n \in C_x$ such that $c_n \in V_{1/n}(c)$ and $c_n \neq c$. Since $c_n \in C_x$, we can write it as $c_n = \abs{a_n - x}$ for some $a_n \in F$. Then, infinitely many $a_n$ are greater than or equal to $x$ or infinitely many of them are less than $x$. In the second case, form a subsequence $(a_{n_k})$ consisting of all the $a_n$ which are less than $x$. Then, $x - a_{n_k} \in V_{1/n_k}(c)$, which implies $a_{n_k} \in V_{1/n_k}(x-c)$. Since $x-a_n$ was never equal to $c$, it follows that none of the $a_{n_k}$ are equal to $x-c$, so $x-c$ is a limit point of $F$, therefore $x-c \in F$, since $F$ is closed. But, $c = \abs{x-(x-c)}$, which means $c \in C_x$. The argument is nearly identical when infinitely many of the $a_n$ are greater than or equal to $x$. We can conclude that $C_x$ is closed, so it must contain its infimum.
    
    Now, let $c \in \R$ be arbitrary and consider a sequence $(x_n) \to c$ where none of its terms are $c$. \lep. We can choose $N \in \N$ such that $\abs{x_n-c} < \epsilon$ whenever $n \geq N$. Since $g(x_n) \in \set{\abs{x_n-a} : a \in F}$, we can write it as $g(x_n) = \abs{x_n-a_n}$ for some $a_n \in F$. Similarly, we can write $g(c) = \abs{c-a_0}$ for some $a_0 \in F$. Then, using the fact that $g(x_n)$ is lower bound, we can conclude that $\abs{x_n-a_n} \leq \abs{x_n-a_0} \leq \abs{x_n-c}+\abs{c-a_0} < \epsilon + g(c)$. Similarly, $\abs{c-a_0} \leq \abs{c-a_n} < \epsilon + g(x_n)$, which implies $g(c)-\epsilon < g(x_n) < g(c) + \epsilon$ whenever $n \geq N$. This means that $g(x_n) \to g(c)$, so $g$ is continuous at $c$.
    
    Let $g(x) = \abs{x-a}$ for some $a \in F$ and $x \notin F$. Then, $g(x)$ is only $0$ when $x = a$, but $x \notin F$, so $x \neq a$ and $g(x) \neq 0$.
    
    \exc{4.3.13}
    \begin{enumerate}
        \item $f(0+0) = f(0) + f(0) = 2f(0) = f(0)$, therefore $f(0) = 0$. Also, $0 = f(0) = f(x-x) = f(x) + f(-x)$, so $f(-x) = -f(x)$.
        
        \item We prove $f(n) = k f(n)$ for every $n \in \N$ by induction. The base case is $f(1) = 1 k = f(1)$, so assume $f(n) = k n$. Then, $f(n+1) = f(n) + f(1) = k n + k = (k+1) n$. Now, let $z \in \Z$ be arbitrary. If $z > 0$ then $z \in \N$ so $f(z) = k z$. If $z = 0$ then $f(z) = 0 = 0 k$. Otherwise, $-z \in \N$ so $f(-z) = -k z = -f(z)$, so $f(z) = k z$. 
        
        Now, let $n \in \N$ be arbitrary. Then, $f(1) = k = f(\sum_{k=1}^n 1/n) = \sum_{k=1}^n f(1/n) = n f(1/n)$, so $f(1/n) = k/n$ for all $n \in \N$. This result is easily extended to the integers, since $f(-z)=-f(z)$. Using the same method, we can show that $f(n x) = n f(x)$ for all $n \in \N$ and all $x \in \R$. Now, let $r \in \Q$ be arbitrary. Then, we can write it as $r = a/b$ where $a,b \in \Z$. Then, $f(r)=f(a/b)= a f(1/b) = a k /b = k r$.
        
        \item Let $c \in \R$ and $(x_n) \to c$ be arbitrary. Then, $(x_n-c) \to 0$, which means, since $f$ is continuous at zero, that $\lim f(x_n -c) = f(0) = 0$. Using the additive property of $f$, we can conclude that $\lim f(x_n-c) = \lim f(x_n) - f(c) = 0$, which implies $\lim f(x_n) = f(c)$, so $f$ is continuous at $c$.
        
        Let $x \in \I$ be arbitrary. Since $\Q$ is dense in $\R$, we can form a sequence $(x_n) \to x$ where every $x_n \in \Q$. Then, $\lim f(x_n) = \lim k x_n = k x$ and since $f$ is continuous everywhere, we can conclude that $f(x) = \lim f(x_n) = k x$.
    \end{enumerate}
    
    \item \begin{shorttheorem}
        Let $K \subseteq \R$ be compact and $f: K \to \R$ be continuous. Then, $f(K) := \set{y \in \R : \exists x \in K (y = f(x))}$ is also compact.
    \end{shorttheorem}
    
    \begin{proof}
         Let $(y_n)$ be a sequence contained in $f(K)$. For each $y_n$, we can choose some $x_n$ such that $f(x_n) = y_n$, thus forming the  sequence $(x_n)$. Since $K$ is compact, $(x_n)$ must have a convergent subsequence, say $(x_{n_k})$, which converges to some $x \in K$. By the sequential criterion for continuity, the sequence $(f(x_{n_k})) = (y_{n_k})$ converges to $f(x)$, which clearly is an element of $f(K)$. Since every sequence contained in $f(K)$ has a convergent subsequence whose limit is also in $f(K)$, it follows that $f(K)$ is compact.
    \end{proof}
    
    \exc{4.4.1}
    \begin{enumerate}
        \item $g(x)=x$ is continuous on $\R$ so, by the Algebraic Continuity Theorem, $h(x)=x x = x^2$ is also continuous on $\R$. Applying the theorem one more time we get that $f(x) = h(x) x = x^3$ is continuous on $\R$.
        
        \item Consider the sequences $(x_n) = (n+1/n^2)$ and $(y_n) = (n)$. Then, $(\abs{x_n-y_n}) = (1/n^2) \to 0$. But $\abs{f(x_n)-f(y_n)} = 3 + 3/n^3 + 1/n^6 \geq 3$, so $x^3$ is not uniformly continuous on $\R$.
        
        \item Let $A \subseteq \R$ be bounded by $M > 0$. \lep. Set $\delta := \epsilon/(3M^2)$ and choose arbitrary $x,y \in A$ such that $\abs{x-y} < \delta$. Notice that $\abs{x^2 + x y + y^2} \leq 3M^2$. Then, 
        \begin{equation*}
            \abs{x^3-y^3}=\abs{x-y}\abs{x^2+ x y + y^2} \leq \abs{x-y} 3M^2 < \epsilon.
        \end{equation*}
    \end{enumerate}
    
    \exc{4.4.2}
    \begin{enumerate}
       \item Consider the sequences $(1/(n+1))$ and $(1/(n+2))$ contained in $(0, 1)$. It is easy to see that $(\abs{1/(n+1)-1/(n+2)}) \to 0$. Also, $\abs{1/(1/(n+1)) - 1/(1/(n+2))} = 1 \geq 1$ for all $n \in \N$, so, by Theorem 4.4.5, $1/x$ is not uniformly continuous on $(0, 1)$.
        
    \item Yes. \lep. Set $\delta := \epsilon$ and choose arbitrary $x,y \in (0, 1)$ such that $\abs{x-y} < \epsilon$. Notice that
    \begin{equation*}
        \frac{1}{\sqrt{x^2+1} + \sqrt{y^2+1}} \leq 1/2
    \end{equation*} and $\abs{x+y} \leq 2$. It follows that 
    \begin{equation*}
        \frac{\abs{x+y}}{\sqrt{x^2+1} + \sqrt{y^2+1}} \leq 1.
    \end{equation*} Then, 
    \begin{align*}
        &\abs{\sqrt{x^2+1}-\sqrt{y^2+1}} = 
        \abs{\sqrt{x^2+1}-\sqrt{y^2+1}} \frac{\abs{\sqrt{x^2+1}+\sqrt{y^2+1}}}{\abs{\sqrt{x^2+1}+\sqrt{y^2+1}}} =\\
        &\frac{\abs{x^2-y^2}}{\abs{\sqrt{x^2+1}+\sqrt{y^2+1}}} =
        \frac{\abs{x+y}}{\abs{\sqrt{x^2+1}+\sqrt{y^2+1}}} \abs{x-y} \leq \abs{x-y} < \epsilon.
    \end{align*}
    
    \item Since $x, \sin{x}$ and $1/x$ are continuous on $\R \backslash 0$,  $x \sin{1/x}$ is continuous on $(0, 1]$. Also, we have shown in Example 4.3.6 that 
    \begin{equation*}
        g(x) = \begin{cases}
        x \sin{1/x} & x \neq 0 \\ 
        0 & x = 0
        \end{cases}
    \end{equation*} is continuous at $0$, so $g$ is continuous on $[0, 1]$. Since $[0, 1]$ is compact, $g$ is uniformly continuous on $[0, 1]$, which implies it is uniformly continuous on $(0, 1)$. But on this interval, $g(x) = x \sin{1/x}$, so $x \sin{1/x}$ is uniformly continuous on $(0, 1)$.
    \end{enumerate}
    
    \exc{4.4.3}
    First, we show that $1/x^2$ is uniformly continuous on $[1, \infty)$. \lep. Set $\delta = \epsilon/2$ and choose arbitrary $x, y \in [1, \infty)$ such that $\abs{x-y} < \delta = \epsilon/2$. Notice that 
    \begin{equation*}
        \frac{\abs{x+y}}{x^2 y^2} = \frac{1}{x y^2} + \frac{1}{x^2 y} \leq 2.
    \end{equation*} Then, 
    \begin{equation*}
        \abs{\frac{1}{x^2}-\frac{1}{y^2}} = \frac{\abs{x^2-y^2}}{x^2 y^2} =
        \abs{x-y} \frac{\abs{x+y}}{x^2 y^2} \leq 2 \abs{x-y} < \epsilon.
    \end{equation*}
    
    Now, consider the sequences $(x_n) = (1/\sqrt{n+1})$ and $(y_n) = (1/\sqrt{n+2})$. Clearly, $(\abs{x_n-y_n}) \to 0$ and both sequences are contained in $(0, 1]$. But $\abs{f(x_n)-f(y_n)} = \abs{(n+1)-(n+2)} = 1 \geq 1$, so Theorem 4.4.5 asserts that $f$ is not uniformly continuous on $(0, 1]$.
    
    \exc{4.4.4}
    \begin{enumerate}
        \item True. Since $f(x) > 0$ on $[a, b]$, $1/f$ is defined on all of $[a, b]$. Also, $f$ is continuous on this interval, so Theorem 4.3.4 guarantees that $1/f$ is also continuous on $[a, b]$. Since $[a, b]$ is compact, $1/f$ must attain a minimum and a maximum on the interval (Theorem 4.4.2), therefore $1/f$ is bounded on $[a, b]$.
        
        \item True. Assume that $f(A)$ is not bounded. Then, there is an $x_1 \in A$ satisfying $\abs{f(x_1)} \geq 1$. In general, for every $n \in \N$ there must be an $x_{n+1} \in A$ such that $\abs{f(x_{n+1})} \geq \abs{f(x_n)} + 1$. Since $(x_n) \subseteq A$, it must be bounded, so there is some convergent subsequence $(x_{n_k})$. Now, consider the sequence $(y_k)$ defined by$y_k = x_{n_{k+1}}$ for each $k \in \N$. Clearly, $(\abs{x_k-y_k}) \to 0$. Then, $\abs{f(y_k)} = \abs{f(x_{n_{k+1}})} \geq \abs{f(x_{n_k})} + 1$ and it follows that $\abs{f(y_k)-f(x_{n_k})} \geq 1$, so $f$ cannot be uniformly continuous, by Theorem 4.4.4. This is a contradiction, so $f(A)$ is bounded.
        
        \item Not true. Consider the function $f : \R \to \R$ defined by
        \begin{equation*}
            f(x) = \begin{cases}
            1 & x \geq 0 \\ 
            -1 & x < 0
            \end{cases}
        \end{equation*} Regardless of $K$, $f(K)$ will be $\set{-1}$ or $\set{1}$ or $\emptyset$ or $\set{1, -1}$. Each of these sets is bounded and contain no limit points, therefore they are compact. However, $f$ is obviously discontinuous at $0$.
        \end{enumerate}
        
    \exc{4.4.5}
    Let $g : (a,c) \to \R$ be uniformly continuous on $(a, b]$ and $[b, c)$. To see that $g$ is uniformly continuous on $(a, c)$, \lep[l]. Since $g$ is uniformly continuous on $(a, b]$, we can choose a $\delta_1 > 0$ such that $\abs{f(x)-f(b)} < \epsilon/2$ as long as $\abs{x-b} < \delta_1$. Similarly, we we can choose $\delta_2 > 0$ such that $\abs{f(y)-f(b)} < \epsilon/2$ if $\abs{y-b} < \delta_2$. We can also find $\delta_3, \delta_4 >0$ that satisfy the uniform continuity properties when $x,y \in (a,b]$ and $x,y \in [b,c)$ respectively. Now, set $\delta = \min{\delta_1, \delta_2, \delta_3, \delta_4}$. Next let $x,y \in (a, c)$ with $\abs{x-y} < \delta$ be arbitrary. The cases $x,y \in (a, b]$ and $x,y \in [b, c)$ are trivial, so assume $x \in (a, b]$ and $y \in [b, c)$. It is easy to see that $\abs{x-b} < \delta \leq \delta_1$ and $\abs{y-b} < \delta \leq \delta_2$. Then, it follows that $\abs{f(x)-f(b)} < \epsilon/2$ and $\abs{f(y)-f(b)} < \epsilon/2$. Summing the inequalities and applying the triangle inequality once again, we get $\abs{f(x)-f(y)} < \epsilon$, so $g$ is uniformly continuous on $(a, c)$.
    
    \exc{4.4.6}
    \begin{enumerate}
        \item $f(x) = 1/x$ and $(x_n) = 1/n$.
        
        \item Impossible. Let $f : (0, 1) \to \R$ be uniformly continuous and $(x_n)$ be a Cauchy sequence. \lep. Then, we can find $\delta > 0$ such that $\abs{f(x)-f(y)} < \epsilon$ whenever $\abs{x-y} < \delta$. Since $(x_n)$ is Cauchy, we can choose $N \in \N$ such that $\abs{x_n-x_m} < \delta$ whenever $n, m \geq N$. Then, we can conclude that $\abs{f(x_n)-f(x_m)} < \epsilon$ for all $n,m \geq N$, so $f((x_n))$ is Cauchy.
        
        \item Impossible. Let $f: [0, \infty) \to \R$ be continuous and $(x_n) \to x$ be a Cauchy sequence. Since $[0, \infty)$ is closed, $x \in [0, \infty)$. Thus, by the sequential characterization of continuity, we must have $\lim f(x_n) = f(x)$, so $(f(x_n))$ converges, therefore it is Cauchy.
    \end{enumerate} 
    
    \exc{4.4.7}
    First, notice that $f$ is continuous on $[0, 1]$, so Theorem 4.4.7 guarantees that $f$ is also uniformly continuous on this interval. To see that $f$ is uniformly continuous on $[1, \infty)$, \lep[l] and set $\delta = 2 \epsilon$. Keeping in mind that $\abs{\sqrt{x}+\sqrt{y}} = \sqrt{x}+\sqrt{y} \geq 2$,
    \begin{equation*}
        \abs{\sqrt{x}-\sqrt{y}} = \abs{\sqrt{x}-\sqrt{y}} \abs{\frac{\sqrt{x}+\sqrt{y}}{\sqrt{x}+\sqrt{y}}} = 
        \frac{\abs{x-y}}{\abs{\sqrt{x}+\sqrt{y}}} \leq \frac{\abs{x-y}}{2} < \epsilon.
    \end{equation*} A slight modification of Exercise 4.4.5 shows that the uniform continuity of $f$ on $[0, 1]$ and $[1, \infty)$ implies $f$ is uniformly continuous on $[0, \infty)$.
    
    \exc{4.4.8}
    \begin{enumerate}
        \item Impossible. We know that $f$ is continuous and $[0, 1]$ is compact so $f([0, 1])$ must also be compact (Theorem 4.4.1). Since $(0, 1)$ is open, it is not closed (the only sets that are both open and closed are $\emptyset$ an $\R$) therefore not compact, so we cannot have $f([0, 1]) = (0, 1)$.
        
        \item \begin{equation*}
            \frac{\sin{2 \pi x} + 1}{2}
        \end{equation*}
        
        \item \begin{equation*}
            \frac{(1-x) \sin{1/x}+1}{2}
        \end{equation*}
    \end{enumerate}
    
    \exc{4.4.9}
    \begin{enumerate}
        \item \lep. Set $\delta := \epsilon/M$ and let $x,y \in A$ such that $\abs{x-y} < \delta$ be arbitrary. Then, 
        \begin{equation*}
            \abs{\frac{f(x)-f(y)}{x-y}} \leq M \implies
            \abs{f(x)-f(y)} \leq \abs{x-y} M < \epsilon.
        \end{equation*}
        
        \item No. Consider the function $f : [0, \infty) \to \R$ defined by $f(x)=\sqrt{x}$, which is uniformly continuous by Exercise 4.4.7 and let $M > 0$ be arbitrary. Set $y := 0$ and $x = 1/(4M^2)$. Then 
        \begin{equation*}
            \abs{\frac{f(x)-f(y)}{x-y}} = \frac{\sqrt{x}}{x} = 
            \frac{1}{\sqrt{x}} = \frac{1}{1/(2M)} = 2M > M. 
        \end{equation*} Thus, $f$ is uniformly continuous on $[0, \infty)$ but not Lipschitz on that interval.
    \end{enumerate}
    
    \exc{4.4.10}
    \begin{enumerate}
        \item \lep. Choose $\delta_1 > 0$ such that $\abs{f(x)-f(y)} < \epsilon/2$ when $\abs{x-y} < \delta_1$. Similarly, choose $\delta_2 > 0$ such that $\abs{g(x)-g(y)} < \epsilon/2$ when $\abs{x-y} < \delta_2$. Set $\delta = \min{\delta_1, \delta_2}$ and let choose arbitrary $x,y \in A$ such that $\abs{x-y} < \delta$. Then,
        \begin{equation*}
            \abs{(f(x)+g(x)) - (f(y)+g(y))} \leq \abs{f(x)-f(y)} + \abs{g(x)-g(y)}
            < \epsilon.
        \end{equation*}
        
        \item Set $A := \R$ and $f(x)=g(x) = x$. Both $f$ and $g$ are uniformly continuous on $\R$, but $f(x)g(x) = x^2$ is not.  
        
        \item Set $A := (0, 1)$, $f(x) := 1$ and $g(x) := x$. Then, $f(x)/g(x) = 1/x$, which is not uniformly continuous on $A$ by Exercise 4.4.2 (a). 
        
        \item Set $A := (1, \infty)$, $f(x) := 1/x$ and $g(x) := x-1$. It is easy to verify that both $f$ and $g$ are uniformly continuous on $A$. However, $f(g(x)) = 1/(x-1)$ is not bounded at $A$, so it cannot be uniformly continuous, by Exercise 4.4.4 (b).
        
        \item \lep. Choose $\delta_1 > 0$ such that $\abs{f(x)-f(y)} < \epsilon$ when $\abs{x-y} < \delta_1$. Next, choose $\delta > 0$ such that $\abs{g(x)-g(y)} < \delta_1$ whenever $\abs{x-y} < \delta$. Now let $x,y \in A$ be such that $\abs{x-y} < \delta$. It follows that $\abs{g(x)-g(y)} < \delta_1$. This implies $\abs{f(g(x))-f(g(y))} < \epsilon$, so $f \circ g$ is uniformly continuous on $A$.
    \end{enumerate}
    
    \exc{4.4.11}
    Assume $g$ is continuous and that $O \subseteq \R$ is open. If $g^-1(O)$ is empty then it is open, so assume $g^-1(O) \neq \emptyset$. Then, we can choose some arbitrary $c \in g^-1(O)$, and we need to show that there is some $\delta > 0$ such that $V_{\delta}(c) \subseteq g^{-}1(O)$. By the definition of $g^-1(O)$, it follows that $g(c) \in O$. Since $O$ is open, there is some $\epsilon > 0$ such that $V_\epsilon(g(c)) \subseteq O$. Using the continuity of $g$, we can find $\delta > 0$ such that $g(x) \in V_\epsilon(g(c))$ for all $x \in V_\delta(c)$. Now consider the set $g(V_\delta(c))$. All of its elements are also elements of $V_\epsilon(g(c))$, which is a subset of $O$. This means that $g(V_\delta(c)) \subseteq O$. Then, is easy to verify that $g(V_\delta(c)) \subseteq O \implies V_\delta(c) \subseteq g^{-1}(O)$, which is what we wanted to show.
    
    For the reverse implication we assume that $g^{-1}(O)$ is open whenever $O \subseteq \R$ is open and seek to show that $g$ is continuous on $\R$. Let $c \in \R$ and $\epsilon > 0$ be arbitrary. The set $g^{-1}(V_\epsilon(g(c)))$ must be open, since $V_\epsilon(g(c))$ is open. Notice that $c \in g^{-1}(V_\epsilon(g(c)))$, which means there is a $\delta > 0$ such that $V_\delta(c) \subseteq g^{-1}(V_\epsilon(g(c)))$. Then, $x \in V_\delta(c) \implies x \in g^{-1}(V_\epsilon(g(c))) \implies g(x) \in V_\epsilon(g(c))$ so we can conclude that $g$ is continuous by Theorem 4.3.2 (ii).
    
    \exc{4.4.12}
    \begin{enumerate}
        \item False. $\oldsin^{-1}([-1,1]) = \R$.
        
        \item False, as seen in (a).
        
        \item False, as seen in (a). 
        
        \item True. Assume $F$ is closed. Then, $F^c$ is open, so $f^{-1}(F^c)=(f^{-1}(F))^c$ must be open, by the Topological Characterization of Continuity. This implies $f^{-1}(F)$ is closed.
    \end{enumerate}
    
    \exc{4.4.13}
    \begin{enumerate}
        \item We proved this in Exercise 4.4.6 (b) when $A = (0, 1)$, but the proof itself only assumed $A$ is a subset of the reals, so it also works here.
        
        \item Assume $g: (a, b) \to \R$ is uniformly continuous. Let $(x_n) \subseteq (a, b)$ be a Cauchy sequence which converges to $a$. By (a),  $g(x_n)$ is also Cauchy, so it converges, and the same argument applies for a sequence that converges to $b$. Thus, we are justified in defining a new function $f: [a, b] \to \R$ such that
        \begin{equation*}
            f(x) = 
            \begin{cases}
            g(x) & x \in (a, b) \\ 
            \lim_{x \to a} g(x) & x = a \\
            \lim_{x \to b} g(x) & x = b.
            \end{cases}
        \end{equation*} To see that $f$, the extended function of $g$, is uniformly continuous on its domain, \lep[l]. Choose $\delta_1 > 0$ such that $\abs{g(x)-g(y)} < \epsilon$ whenever $\abs{x-y} < \delta_1$ for $x,y \in (a, b)$. Next, choose $\delta_2, \delta_3 > 0$ such that $\abs{g(x)-f(a)} < \epsilon$ when $\abs{x-a} < \delta_2$ and $\abs{g(x)-f(b)} < \epsilon$ when $\abs{x-b} < \delta_3$. Now set $\delta = \min{\delta_1, \delta_2, \delta_3, (b-a)/2}$. Now let $x,y \in [a, b]$ be arbitrary. We need to consider four cases: $x,y \in (a, b)$ (1), $x = a, y \in (a, b)$ (2), $x = b, y \in (a, b)$ (3) and $x = a, y = b$ (4). It follows immediately from our definition of $\delta$ that, for cases (1), (2) and (3), we have $\abs{x-y} < \delta \implies \abs{f(x)-f(y)} < \epsilon$. For case (4), notice that $\abs{x-y}=b-a$, which is never less than $\delta$, since $\delta \leq (b-a)/2 < b-a$, so the desired implication is vacuously true. This shows that $f$ is uniformly continuous on $[a, b]$, which of course also means that $f$ is continuous on $[a, b]$, as desired.
        
        For the reverse implication, notice that $[a, b]$ is compact, so the continuity of $g$ at $[a, b]$ implies $g$ is uniformly continuous at $[a, b]$, and certainly on $(a, b)$ as well.
    \end{enumerate}
    
    \exc{4.5.1}
    Let $f : [a, b] \to \R$ be continuous. Since $[a, b]$ is an interval, it is also connected so $f([a, b])$ must also be connected, by Theorem 4.5.2. Now, assume $L$ is a real number such that $f(a) < L < f(b)$.  Since $f(a), f(b) \in f([a, b])$, the connectedness of $f([a, b])$ implies $L \in f([a, b])$, which means there is some $c \in [a, b]$ such that $f(c) = L$. Notice that $c$ cannot be $a$ or $b$, since $f(a) \neq f(c) \neq f(b)$. The argument is similar if $f(a) > L > f(b)$.
    
    \exc{4.5.2}
    \begin{enumerate}
        \item $f:(0, 2 \pi) \to [-1, 1]$ where $f(x) = \sin{x}$.
        
        \item If the closed interval is bounded then it is also compact, so $f$ will have a compact range, thus not open. If the closed interval is allowed to be unbounded, then $f: \R \to (0, 1)$ defined as
        \begin{equation*}
            f(x) = \frac{2^x}{2^x+1}
        \end{equation*} is an example.
        
        \item Let $f: (0, 2) \to [1, \infty)$ be defined as 
        \begin{equation*}
            f(x) = 
            \begin{cases}
            \frac{1}{x} & 0 < x \leq 1 \\ 
            x & 1 < x < 2.
            \end{cases}
        \end{equation*}
        
        \item Not possible, $\R$ is connected and $\Q$ is not, but continuous functions map connected sets to connected sets.
    \end{enumerate}
    
    \exc{4.5.3}
    Let $f: A \to \R$ be increasing on $[a, b] \subseteq A$ and assume it satisfies the intermediate value property on $[a, b]$. Let $c \in [a, b]$ be arbitrary. First, we consider the case where $c \in (a, b)$ and $f(a) < f(c) < f(b)$. \lep. We will assume without loss of generality that $\epsilon$ is small enough that $f(a) < f(c) - \epsilon/2 < f(c) < f(c) + \epsilon/2 < f(b)$. Since $f$ satisfies the intermediate value property on $[a, b]$, there is some $y_\epsilon$ and $y_{-\epsilon}$ so that $f(y_\epsilon) = f(c) + \epsilon/2$ and $f(y_{-\epsilon}) = f(c)-\epsilon/2$. Since $f$ is increasing and $f(y_\epsilon) > f(y_{-\epsilon})$, we can conclude that $y_\epsilon > y_{-\epsilon}$. For the same reason, we can deduce that $y_\epsilon > c > y_{-\epsilon}$. Set $\delta = \min{y_\epsilon-c, c-y_{-\epsilon}} > 0$. It is easy to verify that $\abs{x-c} < \delta$ implies $y_\epsilon > x > y_{-\epsilon}$, which means $f(c)+\epsilon/2 \geq f(x) \geq f(c)-\epsilon/2$, which finally implies $\abs{f(x)-f(c)} \leq \epsilon/2 < \epsilon$, so $f$ is continuous at $c$. The other cases can be verified through simple adjustments of the same argument.
    
    \exc{4.5.4}
    Assume $F$ is not empty. Then, there we can choose some $c \in F$ where $c \in A$ and $g(c) = g(y)$ for some $y \in A$ that is not $c$, and we can also assume that $y > c$. If $g$ is constant between $y$ and $c$, then it is clear that $F$ is uncountable, so assume there is some $x$ between $c$ and $y$ such that $g(x) \neq g(c)$, in particular, we will assume without loosing generality that $g(x) < g(c)=g(y)$. Then, there are uncountably many $L$ such that $g(x) < L < g(c)$, and we fix one of them. By the Intermediate Value theorem, there is some $t_1 \in (c, x)$ such that $g(t_1) = L$. Similarly, we can use the fact that $g(x) < L < g(c) = g(y)$ to choose $t_2 \in (x, y)$ such that $g(t_2) = L$. Since $t_1 \neq t_2$ and $g(t_1) = g(t_2) = L$, we can conclude that $t_1 \in F$. Thus, every one of the uncountably many points between $g(x)$ and $g(c)$ have a correspondent point in $F$, so $F$ is uncountable.
    
    \exc{4.5.5}
    \begin{enumerate}
        \item First, lets show why $f(c)$ cannot be greater than $0$. To do that, assume $f(c) > 0$. Since $f$ is continuous at $c$, we can find $\delta > 0$ such that $\abs{f(x)-f(c)} < f(c)$. Since $\delta > 0$, there is some $t \in K$ such that $c + \delta > t > c-\delta$, which implies $\abs{t-c} < \delta$, thus $\abs{f(t)-f(c)} < f(c)$. Expanding the previous inequality, we can conclude that $f(t) > 0$, but this contradicts the fact that $t \in K$. 
        
        Next, assume $f(c) < 0$. Then, we can find $\delta > 0$ such that $\abs{f(x)-f(c)} < -f(c)$ whenever $\abs{x-c} < \delta$. Consider the number $d = c + \min{\delta/2, b-c}$, which is greater than $c$, since $b \neq c$, but less than or equal to $b$. By one of the previous inequalities, we can conclude that $f(d) < 0$, which, together with the fact that $a < d \leq b$, implies that $d \in K$. This means $c \geq c + \min{\delta/2, b-c}$, which is a contradiction. Thus, we must have $f(c) = 0$.
        
        Now consider a continuous function $g : [a, b] \to \R$ and let $L$ be a real number satisfying $g(a) < L < g(b)$. Then, $g(a) - L < 0 < g(b)-L$, so if we consider the function $f : [a, b] \to \R$ defined by $f(x) = g(x)-L$, we get that $f(a) < 0 < f(b)$. By what was proven on the previous paragraphs, we can find $c \in [a, b]$ such that $f(c) = 0$. This implies $g(c) = L$, as we wanted to show.
        
        \item Assume inductively that we have defined the interval $I_k = [a_k, b_k]$ where $f(a_k) < 0$ and $f(b_k) \geq 0$ and consider the midpoint $z_k := (a_k+b_k)/2$. If $f(z_k) \geq 0$, set $I_{k+1} := [a_k, z_k]$, otherwise set $I_{k+1} := [z_k, b_k]$. In either case, $f$ is negative at the left endpoint and nonnegative at the right. Then, for each $k \in \N$, $I_k$ is nonempty and $I_k \supseteq I_{k+1}$, so there is some $c \in \bigcap_{n=1}^\infty I_n$.
        
        Using the fact that the length of $I_k$ converges to $0$, it is easy to see that the sequence $(a_k)$ consisting of the left endpoints of each $I_k$ and the sequence $(b_k)$ consisting of the right endpoints of each $I_k$ both converge to $c$. Since $f(a_k) < 0$ and $f(b_k) \geq 0$ for every $k \in \N$, we must have $\lim f(a_k) \leq 0$ and $\lim f(b_k) \geq 0$.  Also, $c \in I_0 = [a, b]$ so $f$ is continuous at $x$, which means $\lim f(a_k) = \lim f(b_k) = f(c)$. Thus, $0 \leq f(c) \leq 0$, so we can conclude that $f(c) = 0$, as we wanted to show.
    \end{enumerate}
    
    \exc{4.5.6}
    \begin{enumerate}
        \item Consider the continuous function $g : [0, 1/2] \to \R$ where $g(x) = f(x + 1/2) - f(x)$. Then, $g(0) = f(1/2) - f(0)$ and $g(1/2) = f(1)-f(1/2) = f(0)- f(1/2)$, which means $g(1/2) = -g(0)$. If $g(1/2) = 0$, then so is $g(0)$, and we have $g(0) = f(1/2)-f(0) = 0$, thus $f(1/2) = f(0)$ as we wanted to show.  Otherwise, one of $g(1/2)$ or $g(0)$ will be negative and the other one positive. By the Intermediate Value Theorem, there is some $c \in (0, 1/2)$ such that $g(c) = 0$, which means $f(c+1/2) = f(c)$, and $\abs{(c+1/2)-c} = 1/2$, as we wanted to show.
        
        \item Let $n \in \N$ be arbitrary. Define $g_n : [0, 1-1/n]$ where $g_n(x) = f(x+1/n) - f(x)$ and notice that $g_n$ is continuous. Consider the sum
        \begin{align*}
            &\sum_{i=0}^{n-1} g_n\left (\frac{i}{n} \right) = 
            \sum_{i=0}^{n-1} f\left(\frac{i+1}{n} \right)-f\left( \frac{i}{n} \right) =
            \sum_{i=1}^{n} f\left(\frac{i}{n} \right) - \sum_{i=0}^{n-1} f\left(\frac{i}{n} \right) =\\
            & -f(0) + \sum_{i=0}^n f\paren{\frac{i}{n}} -
            \paren{\paren{\sum_{i=0}^n f\paren{\frac{i}{n}}} - f(1)} = f(1)-f(0)=0.
        \end{align*} If there is some $0 \leq i \leq n-1$ such that $g_n(i/n) = 0$, then $f((i+1)/n) = f(i/n)$ and $\abs{((i+1)/n) - (i/n)} = 1/n$, so we would be done. Therefore, we can assume that none of the $g_n(i/n)$ are $0$. If they were all positive, then their sum would be also positive. Similarly, if every $g_n(i/n)$ was negative, their sum would be negative, not $0$. So there is some $0 \leq i,j \leq n-1$ where $g_n(i/n) > 0$ and $g_n(j/n) < 0$. Then, by the Intermediate Value Theorem, there is some $c$ between $i/n$ and $j/n$ such that $g(c) = 0$. Again, this means that $f(c+1/n) = f(c)$ and $\abs{(c+1/n)-c} = 1/n$, so we are done.
        
        \item Consider the function $f: [0, 1] \to \R$ where $f(x) = (x-1/10)^2$ and define $g: [0, 3/5] \to \R$ as $g(x) = f(x+2/5)-f(x) = 8x/10 + 8/100$. Clearly, $g$ could only be $0$ if $x = -1/10$, but that cannot happen as $-1/10 \notin [0, 3/5]$. Thus, there is no $x \in [0, 3/5]$ such that $f(x+2/5) = f(x)$, which means that it is impossible to find two points $x, y \in [0, 1]$ with distance $2/5$ from each other where $f(x)=f(y)$.
    \end{enumerate}
    
    
    \exc{4.5.7}
    Consider the continuous function $g : [0, 1] \to \R$ defined by $g(x) = f(x)-x$. Assume that $f(x) > x$ for all $x \in [0, 1]$. Then, $f(1) > 1$, but this contradicts the assumption that the range of $f$ is contained in $[0, 1]$, so there must be some $x_0 \in [0, 1]$ such that $f(x_0) \leq x_0$. Similarly, there must be some $x_1 \in [0, 1]$ such that $f(x_1) \geq x_1$. If $f(x_0) = x_0$ or $f(x_1) = x_1$, then we would have found a fixed point for $f$, so assume $f(x_0) \neq x_0$ and $f(x_1) \neq x_1$. Then, $f(x_0) - x_0 = g(x_0) < 0$ and $g(x_1) > 0$. By the Intermediate Value Theorem, there must be some $c \in [0, 1]$ such that $g(c) = 0 = f(c)-c$, thus $f(c) = c$ and $c$ is a fixed point of $f$, as we wanted to show.
    
    \newpage
    \begin{shortlemma} \label{lem_continuousInjectiveIncreases}
        Let $f : [a, b] \to \R$ be a continuous and one-to-one function and $f^{-1} : f([a, b]) \to [a, b]$ be its inverse. Then, $f$ and $f^{-1}$ are both strictly increasing on $[a, b]$ or both strictly decreasing on $[a, b]$.
    \end{shortlemma}
    
    \begin{proof}
         Assume that $f(b) > f(a)$. Choose arbitrary $x,y \in (a, b)$ such that $y > x$ and assume for sake of contradiction that $f(y) \leq f(x)$. This means $f(x) > f(y)$, since $f$ is injective. Then, there exists a real number $L_1$ satisfying $f(y) < L_1 < f(x)$, where $f(c_1) = L_1$ for some $c_1 \in (x, y)$, by the Intermediate Value Theorem. First, consider the case where $f(y) < f(x) < f(b)$. Then, $f(y) < L_1 < f(x) < f(b)$, so there is some $c_2 \in (y, b)$ such that $f(c_2) = L_1 = f(c_1)$, but $(y, b)$ and $(x,y)$ are disjoint, so $c_1 \neq c_2$ which contradicts the assumption that $f$ is injective.
         
         Next, consider the case where $f(x) > f(b)$ and choose some real number $L_1$ satisfying $f(a) < f(b) < L_1 < f(x)$. Then, there is some $c_1 \in (a, b)$ such that $f(c_1) = L_1$. Similarly, there is some $c_2 \in (x, b)$ where $f(c_2) = L_1 = f(c_1)$, which again contradicts the assumption that $f$ is injective. The only remaining cases are $x=a$ or $y=b$, but those will lead to the same contradiction by very similar arguments. Thus, we can conclude that $f(y) > f(x)$ for arbitrary $x, y \in [a, b]$ satisfying $x > y$, so $f$ is strictly increasing on $[a, b]$. Assuming $f(b) < f(a)$ would lead to a nearly identical argument concluding that $f$ is strictly decreasing on the relevant interval.
         
         Assume $f$ is increasing and let $x,y \in f([a, b])$ satisfy $y > x$. We can find unique $t_1, t_2 \in [a, b]$ such that $f(t_1) = x$ and $f(t_2) = y$. Then, $f(t_2) > f(t_1)$, which can only happen if $t_2 > t_1$, since $f$ increases. But $t_1 = f^{-1}(x)$ and $t_2 = f^{-1}(y)$, so $f^{-1}(y) > f^{-1}(x)$, thus $f^{-1}$ is strictly increasing. A similar argument follows if $f$ is strictly decreasing.
    \end{proof}
    
    \exc{4.5.8}
    Choose some $c \in f([a, b])$ and \lep[l]. By Lemma \ref{lem_continuousInjectiveIncreases}, $f$ and $f^{-1}$ are both either strictly increasing or strictly decreasing, so lets consider the first case. Since $c$ is in the range of $f$, there is some $t \in [a, b]$ such that $f(t) = c$. We will assume $t \in (a, b)$ and, without loss of generality, that $a <t-\epsilon<t+\epsilon < b$. Notice that $t + \epsilon > t$, so $f(t+\epsilon) > c$ and similarly $f(t+\epsilon) > c > f(t-\epsilon)$. Then, set $\delta := \min{f(t+\epsilon)-c, c-f(t-\epsilon)}$ and let $y \in f([a, b])$ be such that $\abs{y-c} < \delta$. It follows that $f(t-\epsilon) < y < f(t+\epsilon)$. Since $f^{-1}$ is increasing, we can conclude that $f^{-1}(f(t-\epsilon)) < f^{-1}(y) < f^{-1}(f(t+\epsilon))$, in other words, $t-\epsilon < f^{-1}(y) < t+\epsilon$. From this inequality and the fact that $t = f^{-1}(c)$, it follows that $\abs{f^{-1}(y)-f^{-1}(c)} < \epsilon$, as we wanted to show. The case where $t = a$ or $t = b$ are almost unchanged, and the same goes if $f$ and $f^{-1}$ are both strictly decreasing.
    
    \exc{5.2.1}
    Let $f$ and $g$ defined on an interval $A$ and assume they are both differentiable at $c \in A$. Then,
    \begin{equation*}
        \frac{f(x)+g(x) - (f(c)+g(c))}{x-c} = \frac{f(x)-f(c)}{x-c} + \frac{g(x)-g(c)}{x-c}
    \end{equation*} therefore 
    \begin{equation*}
        \lim_{x \to c} \frac{f(x)+g(x) - (f(c)+g(c))}{x-c} = f'(c) + g'(c)
    \end{equation*} by the functional-limit version of the Algebraic Limit Theorem.
    
    Now let $k \in \R$ be arbitrary. Then 
    \begin{equation*}
        \frac{k f(x) - k f(c)}{x-c} = k \frac{f(x)-f(c)}{x-c},
    \end{equation*} thus 
    \begin{equation*}
        \lim_{x \to c} \frac{k f(x) - k f(c)}{x-c} = k f'(c).
    \end{equation*}
    
    \exc{5.2.2}
    \begin{enumerate}
        \item 
        \begin{align*}
          f(x) &=  \begin{cases}
                2 & x = 0 \\
                1 & x \neq 0
            \end{cases} \\
          g(x) &= \begin{cases}
              \frac{1}{2} & x = 0 \\
              1 & x \neq 0.
          \end{cases}
        \end{align*}
        
        \item $g(x) = x^2$ and 
        \begin{equation*}
            f(x) = 
            \begin{cases}
                1/x & x \neq 0 \\ 
                0 & x = 0.
            \end{cases}
        \end{equation*}
        
        \item Impossible. Assume $g$ and $f + g$ are differentiable at $0$. Then, by Theorem 5.2.4, $(f+g) - g = f$ is also differentiable at $0$.
        
        \item \begin{equation*}
            f(x) = 
            \begin{cases}
                x^2 & x \in \I \\ 
                0 & x \in \Q.
            \end{cases}
        \end{equation*}
    \end{enumerate}
    
    \exc{5.2.3}
    \begin{enumerate}
        \item $h$ is not continuous at $0$, therefore it is also not differentiable at $0$. So, let $c \in \R$ be arbitrary and assume $c \neq 0$. Then, 
        \begin{equation*}
            \frac{1/x-1/c}{x-c} = \frac{c-x}{c x(x-c)} = \frac{-1}{c x}.
        \end{equation*} Therefore 
        \begin{equation*}
            \lim_{x \to c} \frac{1/x-1/c}{x-c} = \lim_{x \to c} \frac{-1}{c x} = \frac{-1}{c^2}.
        \end{equation*}
        
        \item Notice that $f/g = f \cdot (h \circ g)$. Applying Theorem 5.2.4 (iii) and the Chain Rule, we have that 
        \begin{align*}
            &(f/g)'(c) = (f \cdot (h \circ g))'(c) = f(c) (h \circ g)'(c) + f'(c) h(g(c))=\\
            &f(c) h'(g(c)) g'(c) + f'(c) h(g(c)) = -g'(c)f(c)/g(c)^2 + f'(c)/g(c) = \\
            & \frac{g(c) f'(c) - f(c) g'(c)}{[g(c)]^2}.
        \end{align*}
        
        \item Write $f(x)/g(x) - f(c)/g(c)$ as $f(x)/g(x) - f(c)/g(x) + f(c)/g(x) f(c)/g(c)$ and after algebraic manipulation we can rewrite it as
        \begin{equation*}
            \frac{f(x)-f(c)}{g(x) (x-c)} + f(c) \frac{g(c) - g(x)}{g(c) g(x) (x-c)}.
        \end{equation*} Applying the functional-limit version of the Algebraic Limit Theorem and using the fact that $f$ and $g$ are continuous at $c$ and $g(c) \neq 0$ we get the desired result.
    \end{enumerate}
    
    \exc{5.2.4}
    \begin{enumerate}
        \item Assume $h$ is differentiable at $a \in A$. Define $l: A \to \R$ as 
        \begin{equation*}
            l(x) = \begin{cases}
                \frac{h(x)-h(a)}{x-a} & x \neq a \\ 
                h'(a) & x = a.
            \end{cases}
        \end{equation*} Clearly, $l$ is continuous at $a$. Now let $x \in A$ be arbitrary. If $x = a$, then $l(a)(a-a) = 0 = h(a) - h(a)$. Otherwise, $l(x) (x-a) = h(x)-h(a)$, as we wanted to show. 
        
        For the reverse implication assume that $l : A \to \R$ is continuous at $a \in A$ and satisfies $h(x)-h(a) = l(x) (x-a)$ for all $x \in A$. Then, if $x \neq a$, it follows that $l(x) = (h(x)-h(a)) / (x-a)$. Since $l$ is continuous at $a$, 
        \begin{equation*}
            \lim_{x \to a} l(x) = l(a) = h'(a),
        \end{equation*} so $h$ is differentiable at $a$.
        
        \item Let $f$ and $g$ be functions that satisfy the assumptions of Theorem 5.2.5. Since $g$ is differentiable at $f(c)$, there is a function $l : B \to \R$ that is continuous at $f(c)$ and satisfies $l(x) (x-f(c)) = g(x)- g(f(c))$ for all $x \in B$, keeping in mind that $f(A) \subseteq B$. Thus, for any arbitrary $t \in A$, it follows that $l(f(t)) (f(t)-f(c)) = g(f(t)) - g(f(c))$. If $t \neq c$, we can divide both sides of the previous equation by $t-c$ and take the limit as $t$ goes to $c$. Since $f$ is continuous at $c$ and $l$ is continuous at $f(c)$, it follows that $l(f(c)) f'(c) = (g \circ f)'(c) = g'(f(c)) f'(c)$.
    \end{enumerate}
    
    \exc{5.2.5}
    \begin{enumerate}
        \item $f_a$ is continuous at $0$ for every positive value of $a$.
        
        \item Since $f_a$ is not continuous when $a \leq 0$, assume $a > 0$. Consider the right limit 
        \begin{equation} \label{eq_5.2.5.1}
            \lim_{x \to 0^+} \frac{f(x)}{x} = \lim_{x \to 0^+} x^{a-1}.
        \end{equation} Clearly, if $a > 1$ then the limit in (\ref{eq_5.2.5.1}) is $0$, if $a = 1$ the limit is $1$, otherwise the limit does not exist. The left limit of the same function is clearly always $0$, so $f$ is differentiable at $0$ whenever $a > 1$.
        
        The derivative function is necessarily continuous when $a > 1$. Using the power rule, which was not yet proved when $a \notin \N$, we can conclude that when $a > 1$ then
        \begin{equation} \label{eq_5.2.5.2}
            f'_a(x) = \begin{cases}
                a x^{a-1} & x > 0 \\ 
                0 & x \leq 0
            \end{cases}
        \end{equation} which is clearly continuous.
        
        \item By (b), we already know that $a$ must at least be greater than one. In that case, $f'_a(x)$ is given by (\ref{eq_5.2.5.2}). The left limit of $f_a(x)'/x$ at $0$ is clearly $0$, so if we want $f_a'$ to be differentiable at $0$ then the right limit must also be zero, and that happens only when $a > 2$.
    \end{enumerate}
    
    \exc{5.2.6}
    \begin{enumerate}
        \item Assume $g$ is differentiable at $c$ and let $f(x) = c + x$. Then,
        \begin{equation*}
            g'(c) = \lim_{x \to f(0)} \frac{g(x)-g(c)}{x-c} = \lim_{x \to 0} \frac{g(f(x))-g(c)}{f(x)-c} = \lim_{h \to 0}\frac{g(c+h) - g(c)}{h}.
        \end{equation*}
        
        \item Making the substitution $-h = u$, we can conclude that 
        \begin{equation*}
           g'(c) = \lim_{h \to 0}\frac{g(c+h)-g(c)}{h} = \lim_{u \to 0} \frac{g(c)-g(c-u)}{u}.
        \end{equation*} Then, 
        \begin{align*}
            &g'(c) + g'(c) = \lim_{u \to 0} \frac{g(c)-g(c-u)}{u} + \lim_{h \to 0}\frac{g(c+h) - g(c)}{h} =  \\ &\lim_{h \to 0} \frac{g(c+h)-g(c-h)}{2h}.
        \end{align*} Notice that $A$ has to be open so that for small enough $h$, $g(c+h)$ and $g(c-h)$ are both defined for every $c \in A$.
    \end{enumerate}
    
    \exc{5.2.7}
    \begin{enumerate}
        \item $a = 3/2$.
        
        \item $a = 2.5$.
        
        \item $a = 4.5$.
    \end{enumerate}
    
    \exc{5.2.8}
    \begin{enumerate}
        \item \lep. Set $\delta := \epsilon$. Then, 
        \begin{equation*}
            \abs{\frac{x^2-y^2}{x-y}-2y} = \abs{x-y} < \epsilon
        \end{equation*} whenever $0 < \abs{x-y} < \delta$, so $f$ is uniformly differentiable on $\R$. On the other hand, assume there is some $\delta > 0$ such that 
        
        \noindent $\abs{(g(x)-g(y))/(x-y)-g'(y)} < \epsilon$ for all $x,y \in A$ satisfying $0 < \abs{x-y} < \delta$. Then,
        \begin{equation*}
           \abs{ \frac{g(x)-g(y)}{x-y}-g'(y)} = \abs{x^2+x y-2y^2} = 
           \abs{x-y} \abs{x+2y} < \epsilon.
        \end{equation*} Now let  $x := 2\epsilon/(3\delta) + \delta/3$ and $y := x+\delta/2$. Then, $\abs{x-y}\abs{x+2y} = \delta/2 \cdot 2\epsilon/\delta = \epsilon$. But $0 < \abs{x-y} = \delta/2 < \delta$, so we must have $\abs{x-y} \abs{x+2y} = \epsilon < \epsilon$, a contradiction. Thus, $g$ is not uniformly differentiable on $\R$.
        
        \item Let $f: A \to \R$ be uniformly differentiable on $A$ and choose an arbitrary $c \in A$. \lep. Since $f$ is differentiable at $c$, there is some $\delta_1 > 0$ such that 
        \begin{equation*}
            \abs{\frac{f(c)-f(x)}{c-x} - f'(x)} < \epsilon/2
        \end{equation*} whenever $0 < \abs{x-c} < \delta_1$. Also, since $f$ is uniformly differentiable at $A$, there is some $\delta_1 > 0$ such that
        \begin{equation*}
            \abs{\frac{f(c)-f(x)}{c-x} - f'(c)} < \epsilon/2
        \end{equation*} whenever $0 < \abs{x-c} < \delta_2$. Now set $\delta := \min{\delta_1, \delta_2}$. Then,
        \begin{equation*}
            \abs{f'(x)-f'(c)} \leq \abs{f'(x)-\frac{f(c)-f(x)}{c-x}} + 
            \abs{\frac{f(c)-f(x)}{c-x} - f'(c)} < \epsilon
        \end{equation*} for all $x$ satisfying $0 < \abs{x-c} < \delta$, therefore $f'$ is continuous on $A$.
        
        \item No. Consider the function $g_2$ in the introduction to Chapter 5. $g_2$ is differentiable on $[0, 1]$ but not uniformly differentiable on the same interval.
    \end{enumerate}
    
    \exc{5.2.9}
    \begin{enumerate}
        \item True. Let $f$ be differentiable on an interval $[a, b]$ and assume $f(a) \neq f(b)$. Since the irrationals are dense in $\R$, there is some $\alpha$ between $f(a)$ and $f(b)$ that is irrational. By theorem 5.2.7, there must be some $c \in (a, b)$ such that $f'(c) = \alpha$.
        
        \item Not true. Consider the function $g: [0, 1] \to \R$ defined by
        \begin{equation*}
            g(x) = \begin{cases}
                x^2 \sin{1/x} + x & x \neq 0 \\
                0 & x = 0.
            \end{cases}
        \end{equation*} $g$ is differentiable everywhere and $g'(0) = 1$. Now let $\delta > 0$ be arbitrary and choose $n \in \N$ such that $n > 1/(2 \delta \pi)$. Then, $1/(2n\pi) \in V_{\delta}(0)$, but $g'(1/(2n\pi)) = 0$. Thus, every $\delta$-neighborhood around $0$ has a point $a$ where $g'(a)$ is not greater than $0$. 
        
        \item 
    \end{enumerate}
    
    \exc{5.2.10}
    First, notice that 
        \begin{equation*}
            \lim_{x \to 0} \frac{g(x)-g(0)}{x-0} = \lim_{x \to 0} 1/2 + x \sin{1/x} = 1/2 = g'(0),
        \end{equation*} and the Algebraic Limit Theorem for Derivatives can be shown that $g$ is differentiable at $c \neq 0$. Now let $(a, b)$ be an interval satisfying $a < 0$ and $b > 0$. Choose $n \in \N$ such that $n > 1/(2\pi b) + 1/4$. It follows that 
        \begin{equation*}
            \frac{1}{(2n-1/2)\pi} < \frac{1}{(2n-1/2)\pi} < b.
        \end{equation*} But it is easy to verify that 
        \begin{equation*}
            g \paren{\frac{1}{(2n-1/2)\pi}} > g \paren{\frac{1}{(2n-1/2)\pi}},
        \end{equation*} so $g$ is not increasing on any interval containing $0$.
        
    
    \begin{shortlemma} \label{lem_5.2.11.1}
        Let $f : A \to \R$ be differentiable on $(a, b)$ where $(a, b) \subseteq A$. Then, $f$ is increasing on $(a, b)$ if and only if $f'(x) \geq 0$ for all $x \in (a, b)$. Similarly, $f$ is decreasing on $(a, b)$ if and only if $f'(x) \leq 0$ for all $x \in (a, b)$.
    \end{shortlemma}
    \begin{proof}
         First assume that $f$ is increasing on $(a, b)$. Given any $c \in (a, b)$, construct a sequence $(x_n)$ contained in $(a,b)$ such that $(x_n) \to c$ and $x_n > c$ for all $n \in \N$. Then, every term in the sequence \begin{equation*}
             (y_n) = \paren{\frac{f(x_n)-f(c)}{x_n-c}}
         \end{equation*}
         is nonnegative, since $f$ is increasing. Since $f$ is differentiable at $c$, $(y_n)$ converges to $f'(c)$, thus $f'(c) \geq 0$.
         
         For the converse direction assume that the derivative is nonnegative on all of $(a, b)$ and choose $x_1, x_2 \in (a, b)$ such that $x_2 > x_1$. Assume for contradiction that $f(x_1) > f(x_2)$. Then 
         \begin{equation*}
             \frac{f(x_2)-f(x_1)}{x_2-x_1} < 0,
         \end{equation*} so there the Mean Value Theorem gives us some $c \in (x_1, x_2)$ such that $f'(c) < 0$, establishing our contradiction. Both arguments are similar when $f$ is decreasing.
    \end{proof}
    
    \exc{5.2.11}
    \begin{enumerate}
        \item We know that 
        \begin{equation} \label{eq_5.2.11.1}
            \lim_{x \to a^+} \frac{g(x)-g(a)}{x-a} = g'(a) < 0.
        \end{equation} Now assume for contradiction that for every $x \in (a, b)$ we have $g(x) \geq g(a)$. Then, both the numerator and denominator in (\ref{eq_5.2.11.1}) are always nonnegative, so, by the Order Limit Theorem, $g'(a) \geq 0$, which is a contradiction. Thus, there is some $x \in (a, b)$ such that $g(a) > g(x)$. A similar argument can be used to show that there is a $y \in (a, b)$ such that $g(y) < g(b)$.
        
        \item Since $g$ is differentiable on $[a, b]$, it is also continuous on that interval. Also, $[a, b]$ is compact, so $g$ has a minimum and maximum value in $[a, b]$. If both extreme values are at $\set{a, b}$, then $g$ is constant and the result follows. Thus, we can assume that there is some $c \in (a, b)$ that is either a maximum value of $g$ or a minimum value, so $g'(c) = 0$, again by Theorem 5.2.6. Since $g'(x) = f'(x) - \alpha$ for all $x \in [a, b]$, then $g'(c) = f'(c) - \alpha = 0$, thus $f'(c) = \alpha$, as we wanted to show.
    \end{enumerate}
    
    \exc{5.2.12}
    Let $y \in f([a, b])$ be arbitrary. Then, there is some unique $x \in [a, b]$ such that $f(x) = y$. Since $f'(x) \neq 0$,
    \begin{equation*}
       \frac{1}{f'(x)} = \lim_{h \to x} \frac{h-x}{f(h)-f(x)} = \lim_{h \to f^{-1}(y)} \frac{h-x}{f(h)-f(x)}.
    \end{equation*} Now we can use the fact that the composition of continuous functions is continuous to conclude that 
    \begin{equation*}
        \lim_{h \to f^{-1}(y)} \frac{h-x}{f(h)-f(x)} = \lim_{h \to y} \frac{f^{-1}(h)-x}{h-y} = (f^{-1})'(y), 
    \end{equation*} as we wanted to show.
    
    \exc{5.3.1}
    \begin{enumerate}
        \item Since $f'$ is continuous on the compact set $[a, b]$, it is also uniformly continuous on $[a, b]$. Then, by Exercise 4.4.4(b), $f'([a, b])$ must be bounded, so we can choose some $M \in \R$ such that $\abs{f'(z)} \leq M$ for all $z \in [a, b]$. Now, choose $x,y \in [a, b]$ and assume that $x < y$. Then, by the Mean Value Theorem, there is some $c \in (x, y)$ such that 
        \begin{equation*}
            \abs{\frac{f(x)-f(y)}{x-y}} = \abs{f'(c)} \leq M,
        \end{equation*} so $f$ is Lipschitz on $[a, b]$.
        
        \item Since $f'$ is continuous on the compact set $[a, b]$, it must attain a maximum value at some $c_1 \in [a, b]$ and a minimum value at $c_2 \in [a, b]$. Set $\alpha := \max{\abs{f'(c_1)}, \abs{f'(c_2)}, 0.5}$ and notice that $\alpha < 1$ by assumption . Now, consider some $x, y \in [a, b]$ with $y > x$. By the Mean Value Theorem, there is some $p \in (a, b)$ such that 
        \begin{equation*}
            \abs{\frac{f(x)-f(y)}{x-y}} = \abs{f'(p)} \leq \alpha.
        \end{equation*} Since $1 > \alpha > 0$, $f$ is contractive on $[a, b]$.
    \end{enumerate}
    
    \exc{5.3.2}
    Assume $f$ is not one-to-one on $A$. Then, there is some $x,y \in A$ with $y > x$ such that $f(x) = f(y)$. By Rolle's Theorem, there is some $c \in (x, y)$ such that $f'(c) = 0$. So, if the derivative of $f$ is never $0$ on $A$, then $f$ has to be one-to-one.
    
    $f: [0, 1] \to \R$ defined by $f(x) = x^2$ is a counterexample to the converse statement.
    
    \exc{5.3.3}
    \begin{enumerate}
        \item Consider the function $g:[0,3] \to \R$ defined by $g(x) = h(x)-x$. Then, $g(0) = 1$ and $g(3) = -1$. By the Intermediate Value Theorem, there is some $d \in [0, 3]$ such that $g(d) = 0 = h(d)-d$, thus $h(d) = d$ as we wanted to show.
        
        \item Since 
        \begin{equation*}
            \frac{h(1)-h(0)}{1-0} = 1
        \end{equation*} and 
        \begin{equation*}
            \frac{h(3)-h(1)}{3-1} = 0,
        \end{equation*} there must be points $a,b \in (0, 3)$ such that $h'(a) = 0$ and $h'(b) = 1$. Then, by Darboux's Theorem, there is some $c \in [0, 3]$ such that $h'(c) = 1/3$.
        
        \item This happens for exactly the same reason as in $(b)$, since $0 < 1/4 < 1$.
    \end{enumerate}
    
    \newpage
    \exc{5.3.4}
    \begin{enumerate}
        \item Since $f$ is differentiable on $A$ it is also continuous on $A$, so $\lim f(x_n) = f(\lim x_n) = f(0) = 0$. Concerning the derivative, since $(x_n) \to 0$ and none of the $x_n$ are zero then 
        \begin{equation*}
            \lim \frac{f(x_n)-f(0)}{x_n-0} = 0 = \lim_{x \to 0} \frac{f(x)-f(0)}{x} = f'(0).
        \end{equation*}
        
        \item 
  
    \end{enumerate}
    
    \exc{5.3.5}
    \begin{enumerate}
        \item Applying the Mean Value Theorem on the interval $[a, b]$ and the function $h$, we can conclude after some algebra that there is some $c \in (a, b)$ such that $h'(c) = 0$. But, using the Algebraic Limit Theorem for Derivatives, we get that $h'(c) = (f(b)-f(a))g'(c) - (g(b)-g(a))g'(c) = 0$, therefore $(f(b)-f(a))g'(c) = (g(b)-g(a))g'(c)$.
        
        \item Consider the parametric curve given by $x = f(t)$ and $y = g(t)$. The derivative of $y$ with respect to $x$ is $g'(t)/f'(t)$, and the slope from the point $x = f(a)$ and $y = g(a)$ to $x = f(b)$ and $y = g(b)$ is $(g(b)-g(a))/(f(b)-f(a))$. So the Generalized Mean Value Theorem asserts the same thing as the normal Mean Value Theorem, but for parametric curves.
    \end{enumerate}
    
    \exc{5.3.6}
    
    Throughout this exercise we will assume $M \neq 0$, since the case $M = 0$ is trivial.
    \begin{enumerate}
        \item Assume for contradiction that there is some $d \in [0, a]$ such that $\abs{g(d)} > Md$. Notice that $d$ cannot be $0$, otherwise we would have $0 > 0$. Then, we can say that $\abs{g(d)/d} > M$. $g(d)$ cannot be 0, since $M \geq 0$, so assume first that $g(d)$ is positive. Then, we have $(g(d)-g(0))/(x-0) = M + \epsilon$ for some $\epsilon > 0$. By the Mean Value Theorem, there is some $c \in (0, d)$ such that $g'(c) = M + \epsilon > M$, which contradicts the assumption that $\abs{g'(x)} \leq M$ for all $x \in [0, a]$. The case where $g(d) < 0$ is similar. 
        
        \item Define the function $f:[0, a] \to \R$ by $f(x) = M x^2/2$. Then, by (a), we know that $\abs{h'(x)} \leq f'(x)$ for all $x \in [0, a]$. Assume for contradiction that there is some $d \in [0, a]$ such that $\abs{h(d)} > f(d)$. It is easy to verify that $d \neq 0$, which also implies $f(d) \neq 0$. Assuming that $h(d) > 0$, we can conclude that $h(d) = f(d) (1+ \epsilon)$ for some $\epsilon > 0$. By the Generalized Mean Value Theorem, it follows that there is some $c \in (0, d)$ such that $h'(c)f(d)=f'(c)h(d)=f'(c)f(d)(1+\epsilon)$, and it follows that $h'(c)=f'(c) (1+\epsilon) > f'(c)$, but $c \in (0, a]$ so we can also conclude that $\abs{h'(c)} \leq f'(c) $, which is a contradiction. The case where $h(d) < 0$ is similar.
        
        \item Let $h:[0, a] \to \R$ be differentiable three times, $h''(0) = h'(0) = h(0) = 0$ and $\abs{h'''(x)} \leq M$ for all $x \in [0, a]$. We claim that $\abs{h(x)} \leq M^3/6$ for all $x \in [0, a]$.
        
        The proof is almost identical to (b). Define the function $f:[0, a] \to \R$ by $f(x) = M x^3/6$. Then, by (b), we know that $\abs{h'(x)} \leq f'(x)$ for all $x \in [0, a]$. Assume for contradiction that there is some $d \in [0, a]$ such that $\abs{h(d)} > f(d)$. It is easy to verify that $d \neq 0$, which also implies $f(d) \neq 0$. Assuming that $h(d) > 0$, we can conclude that $h(d) = f(d) (1+ \epsilon)$ for some $\epsilon > 0$. By the Generalized Mean Value Theorem, it follows that there is some $c \in (0, d)$ such that $h'(c)f(d)=f'(c)h(d)=f'(c)f(d)(1+\epsilon)$, and it follows that $h'(c)=f'(c) (1+\epsilon) > f'(c)$, but $c \in (0, a]$ so we can also conclude that $\abs{h'(c)} \leq f'(c) $, which is a contradiction. The case where $h(d) < 0$ is similar.
        
        It is easy to show by strong induction that if $h:[0, a] \to \R$ is differentiable $n$ times and $h^{(n-1)} = h^{(n-2)} = \dots = h(0) = 0$ and $\abs{h^{(n)}} \leq M$ for all $x \in [0, a]$ then $\abs{h(x)} \leq M x^n/n!$ (here $h^{(n)}$ means the $n$th derivative of $h$).
    \end{enumerate}
    
    \exc{5.3.7}
    Assume $f$ has at least two fixed points on the interval $A$ where it is differentiable. Then, there are $x, y \in A$ with $y > x$ such that $f(y) = y$ and $f(x) = x$. Then, applying the mean value theorem to the interval $[x, y]$, there is some $c \in (x, y)$ such that
    \begin{equation*}
        \frac{f(y)-f(x)}{y-x} = \frac{y-x}{y-x} = 1 = f'(c),
    \end{equation*} so the derivative of $f$ must be one in at least one point of $A$.
    
    \exc{5.3.8}
    \lep. Choose $\delta > 0$ such that $\abs{f'(x)-L} < \epsilon$ whenever $0 < \abs{x} < \delta$ and choose some arbitrary $x$ satisfying $0 < \abs{x} < \delta$. By the Mean Value Theorem, we can find some $c \in (0, x)$ such that 
    \begin{equation*}
        f'(c) = \frac{f(x)-f(0)}{x-0}.
    \end{equation*} Since $0 < \abs{c} < \abs{x} < \delta$, it follows that 
    \begin{equation*}
        \abs{\frac{f(x)-f(0)}{x-0} - L} = \abs{f'(c)-L} < \epsilon,
    \end{equation*} therefore 
    \begin{equation*}
        \lim_{x \to 0} \frac{f(x)-f(0)}{x-0} = L = f'(0).
    \end{equation*}
    
    \exc{5.3.9}
    Since $f'$ and $g'$ are continuous at $a$ and $g'(a) \neq 0$, $f'/g'$ is also continuous at $a$. Thus, $\lim_{x \to a} f'(x)/g'(x) = f'(a)/g'(a)$. 
    Notice that 
    \begin{equation*}
       \frac{f(x)}{g(x)} = \frac{\frac{f(x)-f(a)}{x-a}}{\frac{g(x)-g(a)}{x-a}}
    \end{equation*} and we can use the fact that $g'(a) \neq 0$ again to conclude that 
    \begin{equation*}
        \lim_{x \to a}\frac{\frac{f(x)-f(a)}{x-a}}{\frac{g(x)-g(a)}{x-a}} = 
        \frac{\lim_{x \to a} \frac{f(x)-f(a)}{x-a}}{\lim_{x \to a}\frac{g(x)-g(a)}{x-a}} = \frac{f'(a)}{g'(a)},
    \end{equation*} as we wanted to show.
    
    \exc{5.3.10}
    It is easy to compute that $\lim_{x \to 0} f(x) = 0$, $\lim_{x \to 0} g(x) = 0$ and $\lim_{x \to 0} f(x)/g(x) = 0$. After some algebra, we can also see that 
    \begin{equation*}
        \frac{f'(x)}{g'(x)} = \frac{2x \sin{1/x^4}-4\cos{1/x^4}/x + x^3 \sin{1/x^4}}{2}.
    \end{equation*} Assuming that the limit of $f'(x)/g'(x)$ exists, we can use the algebraic limit theorem to conclude that 
    \begin{equation*}
        \lim_{x \to 0} \frac{f'(x)}{g'(x)} = -2 \lim_{x \to 0} \frac{\cos{1/x^4}}{x},
    \end{equation*} but it is easy to see that this limit does not exist, by considering the sequence $(1/\sqrt[4]{2 n \pi})$. The sequence converges to $0$ but applying $\cos{1/x^4}/x$ to its terms yields a divergent sequence. This means that the limit does not exist, therefore $\lim_{x \to 0} f'(x)/g'(x)$ also does not exist.
    
    This is surprising, as we normally expect the limit of the ratio of the derivatives of two functions to be the same as the limit of the ratio of the functions themselves due to Theorem 5.3.6. However, this does not contradict said theorem, since it only asserts that that is the case when the limit of $f'(x)/g'(x)$ exists, which does not happen in this example.
    
    \exc{5.3.11}
    \begin{enumerate}
        \item Assume $\lim_{x \to a} f'(x)/g'(x) = L$ and \lep[l]. Choose $\delta > 0$ such that $\abs{f'(x)/g'(x) - L} < \epsilon$ whenever $0 < \abs{x-a} < \delta$ and choose some arbitrary $x$ satisfying $0 < \abs{x-a} < \delta$. Then, we can use the Generalized Mean Value Theorem together with the fact that $g(y) \neq 0$ for all $y \neq a$ to conclude that 
        \begin{equation*}
            \abs{\frac{f(x)}{g(x)} - L} = \abs{\frac{f(x)-f(a)}{g(x)-g(a)}} = \abs{\frac{f'(c)}{g'(c)} - L}
        \end{equation*} for some $c$ strictly between $a$ and $x$. This means that $0 < \abs{c-a} < \delta$, so we can conclude that 
        \begin{equation*}
           \abs{\frac{f(x)}{g(x)} - L} = \abs{\frac{f'(c)}{g'(c)} - L} < \epsilon,
        \end{equation*} therefore $\lim_{x \to a} f(x)/g(x) = L$.
        
        \item Yes. Let $M > 0$ be arbitrary. Choose $\delta > 0$ such that $f'(x)/g'(x) \geq M$ whenever $0 < \abs{x-a} < \delta$ and let $x$ satisfy $0 < \abs{x-a} < \delta$. For the same reason as before, there is some $c$ strictly between $x$ and $a$ such that 
        \begin{equation*}
            \frac{f(x)}{g(x)} = \frac{f'(c)}{g'(c)} \geq M,
        \end{equation*} thus 
        \begin{equation*}
            \lim_{x \to a} \frac{f(x)}{g(x)} = \infty.
        \end{equation*}
    \end{enumerate}
    
    \exc{5.4.2}
    Consider the obviously converging series 
    \begin{equation*}
        \sum_{n=0}^\infty \frac{2}{2^n}.
    \end{equation*} Since $h(y)  \leq 1$ for all $y \in \R$, each term of the previous series is strictly greater than the absolute value of the terms in our original series, we can use the Comparison Test to conclude that 
    \begin{equation*}
        \sum_{n=0}^\infty \abs{\frac{h(2^n x)}{2^n}}
    \end{equation*} converges. Then, by the Absolute Convergence Test, $g(x)$ converges.
    
    \exc{5.4.3}
    One of the Theorems of Chapter 4 is that the composition of continuous functions is continuous, so $h(2^n x)$ is continuous for all $n \in \N \cup \set{0}$. The Algebraic Limit Theorem for Continuity then implies that the finite sum is also continuous.
    
    \exc{5.4.4}
    \begin{enumerate}
        \item $g$ is continuous and the set $[0, 2]$ is compact, so $g$ must have both a minimum and a maximum in this interval.
        
        \item 
    \end{enumerate}
    
    \exc{5.4.5}
    It is easy to check that for any even number $k \in \N$, $h(k) = 0$. Thus,
    \begin{equation*}
       g(x_m) = \sum_{n=0}^\infty \frac{h(2^{n-m})}{2^n} = \sum_{n=0}^m \frac{h(2^{n-m})}{2^n} = \sum_{n=0}^m 2^{-m}.
    \end{equation*} Now,
    \begin{equation*}
        \frac{g(x_m)-g(0)}{x_m-0} = \frac{g(x_m)}{x_m} = \sum_{n=0}^m 2^m 2^{-m} = m+1.
    \end{equation*}
    
    \exc{5.4.6}
    Define $x_{m n} = 1/2^m + 1/2^n$ for every $m,n \in \N$. Notice that each $1 \geq x_{m n} > 0$ for each $m,n \in \N$. Then, assuming $m > n$, we can use the fact that $h(2k + x) = h(x) \geq 0$ for every $k \in \Z$ and every $x \in \R$ to get
    \begin{align*}
        &g(x_{m n}) = \sum_{i = 0}^\infty \frac{h(2^{i-n}+2^{i-m})}{2^i} = 
        \sum_{i = 0}^n \frac{h(2^{i-n}+2^{i-m})}{2^i} + \sum_{i=n+1}^\infty \frac{h(2^{i-m})}{2^i} = \\
        & n (2^{-m}+2^{-n})+2^{-n}-2^{-m} + \frac{m-n}{2^m}.
    \end{align*} Also, it is not hard to see that $g(2^{-n}) = 2^{-n}(n+1)$, so 
    \begin{equation*}
        \frac{g(x_{m n})-g(2^-n)}{x_{m n} - 2^{-n}} = m-1
    \end{equation*} which gets arbitrarily large as $m$ increases. Since $\lim_{m \to \infty} (x_{m n}) = 2^{-n}$, this shows that $g$ is not differentiable at $2^{-n}$.

    \exc{6.2.1}
    \begin{enumerate}
        \item Let $x > 0$ be arbitrary. Then, $\lim (f_n(x)) = \lim (x/(1/n+x^2)) = 1/x$.
        
        \item No. Assume for contradiction that $(f_n)$ converges uniformly to $1/x$ and set $\epsilon := 1$. Fix $N \in \N$ such that $\abs{f_N(x)-f(x)} < 1$ for all $x \in (0, \infty)$. Setting $x := 1/(N+1)$,
        \begin{align*}
            &\abs{f_N(x)-f(x)} = \frac{1}{x+N x^3} < 1 \implies \\
            & x + N x^3 > 1,
        \end{align*} but $x + N x^3 < x + N x = x (N+1) = 1$, which is a contradiction.
        
        \item No, for the same reason as in (b).
        
        \item Yes. \lep \space and choose $N \in \N$ such that $N > 1/\epsilon$. Then, it follows that $x + n x^3 > 1 + n > 1/\epsilon$ for all $n \geq N$. Setting $f(x) = 1/x$,
        \begin{equation*}
            \abs{f_n(x)-f(x)} = \frac{1}{x+n x^3} < \epsilon
        \end{equation*} for all $n \geq \N$ and all $x \in (1, \infty)$.
    \end{enumerate}
    
    \exc{6.2.2}
    \begin{enumerate}
        \item Fix some $n \in \N$, \lep[l] and set $\delta := 1/n$. If $x \neq 1/m$ for some $m \in \N$ with $m \leq n$, then $\abs{f(x)} = 0 < \epsilon$ so assume otherwise. Since we want to show that $f_n$ is continuous at $0$, we can also assume that $\abs{x}=1/m < \delta = 1/n$. This means $m > n$, a contradiction. Therefore, there is no $x \in \R$ satisfying both $\abs{x} < \delta$ and $x = 1/m$ for some $m \in \N$ with $m \leq n$. Thus, for every satisfying $\abs{x} < \delta$, $\abs{f_n(x) - f_n(0)} = 0 < \epsilon$, so $f_n$ is continuous at $0$. 
        
        It is easy to verify that $f: \R \to \set{0, 1}$ is such that
        \begin{equation*}
            f(x) = \begin{cases}
                1 & x = \frac{1}{m} \text{ for some } m \in \N \\
                0 & \text{otherwise}.
            \end{cases}
        \end{equation*}
        Assume for contradiction that $f_n \to f$ uniformly on $\R$. Set $\epsilon := 1/2$ and choose $N \in \N$ such that $\abs{f_n(x)-f(x)} < \epsilon$ for all $x \in \R$ and all $n \geq \N$. Then, setting $x = 1/(N+1)$, we have that $\abs{f_N(x)-f(x)} = \abs{0-1} = 1 < 1/2$, a contradiction.
        
        $f$ is discontinuous at $0$, since the sequence $(x_m) = (1/m)$ converges to $0$ and $(f(x_m)) \to 1$, but we can also construct a sequence $(y_m)$ of irrational numbers that converges to $0$, and in that case $(f(y_m)) \to 0$.
        
        The Continuous Limit Theorem does not apply here, since the convergence is not uniform.
        
        \item Each $g_n$ is continuous at $0$, as we can always find $\delta$-neighborhoods small enough not to include any of the $1/1, 1/2 \dots 1/n$.
        
        It is easy to see that $g_n \to g$ pointwise, where $g: \R \to \set{0, 1}$ is defined by 
        \begin{equation*}
            g(x) = \begin{cases}
                x & x = \frac{1}{m} \text{ for some } m \in \N \\ 
                0 & \text{otherwise}.
            \end{cases}
        \end{equation*}
        In contrast to (a), this convergence is uniform. To see that, \lep[l]. Choose $N \in \N$ such that $N > 1/\epsilon$ and let $x \in \R$ and $n \geq \N$ be arbitrary. If $x \neq 1/m$ for some $m \in \N$, then $\abs{g_n(x)-g(x)} = 0 < \epsilon$. So, assume $x = 1/m$ for some $m \in \N$. If $m \leq n$, then $\abs{g_n(x) - g(x)} = \abs{1/m-1/m} = 0 < \epsilon$, so assume $m > n$. Then, $\abs{g_n(x) - g(x)} = g(x) = 1/m$. Since $m > n \geq N > 1/\epsilon$, it follows that $1/m < \epsilon$, so $g_n$ converges to $g$ uniformly.
        
        Since each $g_n$ is continuous at $0$ and $g_n \to g$ uniformly, the Continuous Limit Theorem guarantees that $g$ is continuous at $0$. To verify this in a different way, \lep[l] and set $\delta := \epsilon$. Then, $\abs{g(x)-g(0)} = g(x)$ is different than zero only when $x = 1/m$ for some $m \in \N$, so assume that is the case. Then, if $\abs{x} = x < \delta = \epsilon$, if follows that $g(x)=x < \epsilon$, so $g$ is continuous at $0$, as expected.
        
        \item For the same reasons as before, each $h_n$ is continuous at $0$. It is easy to see that $h_n \to h$ pointwise, where $h: \R \to \set{0, 1}$ is defined by 
        \begin{equation*}
            h(x) = \begin{cases}
                x & x = \frac{1}{m} \text{ for some } m \in \N \\ 
                0 & \text{otherwise}.
            \end{cases}
        \end{equation*}
        Assume for contradiction that this convergence is uniform. Set $\epsilon = 1/2$ and choose $N \in \N$ such that $\abs{h_n(1/m)-h(1/m)} = \abs{h_n(1/m)-1/m}$
        
        \noindent $< \epsilon$ for all $m \in \N$ and all $n \geq N$. Then, $\abs{h_{N+1}(1/(N+1))-1/(N+1)}=1-1/(N+1) < 1/2$ which implies $N < 1$. This is a contradiction, since $N \in \N$.
        
        Since $h = g$ ($g$ is introduce in (b)), $h$ is continuous at $0$. This does not contradict the Continuous Limit Theorem, it just shows that its converse is not necessarily true.
    \end{enumerate}
    
    \exc{6.2.3}
    \begin{enumerate}
        \item $g_n \to g$, where $g:[0, \infty) \to \R$ is such that
        \begin{equation*}
            g(x) = \begin{cases}
                0 & x > 1 \\ 
                x & 0 \leq x \leq 1
            \end{cases}
        \end{equation*} and $h_n \to h$ where $h:[0, \infty)$ is defined by 
        \begin{equation*}
            h(x) = \begin{cases}
                1 & x \neq 0 \\
                0 & x = 0.
            \end{cases}
        \end{equation*}
        
        \item Each $g_n$ is clearly continuous at $1$, but $g(1)=1$ and $\lim_{x \to 1^+} g(x) = 0$, so $g$ is not continuous at $0$. Similarly, each $h_n$ is continuous at $0$, but $h$ clearly is not. If the convergence was uniform in either of those cases, the continuity of the terms in the sequence would imply the continuity of the limiting function, by the Continuous Limit Theorem.
        
        \item Modify each of the functions so that their domain is now $[2, \infty)$. Then, $g(x) = 0$, $h(x) = 1$ and $h_n(x) = 1$ for all $x \in  [2, \infty)$ and all $n \in N$. It is obvious that $h_n \to h$ uniformly, so we focus on $g$. \lep and choose $N > \max{(2-\epsilon)/\epsilon, 1}$. Fix some arbitrary $n \geq \N$ and $x \in [2 \infty)$. It is easy to check that $x/(1+x^n) \leq 2/(1+2^n)$. Then, $2^n > n > (2-\epsilon)/\epsilon$. After some algebra, $\abs{x/(1+x^n)} < 2/(2^n+1) < \epsilon$. Therefore, $g_n \to g$ uniformly.
    \end{enumerate}
    
    \exc{6.2.4}
    
    Let $A \subseteq \R$ be open and $f: A \to \R$ be uniformly differentiable on $A$. \lep and choose $\delta > 0$ such that 
    \begin{equation*}
        \abs{\frac{f(x)-f(y)}{x-y} - f'(y)} < \epsilon
    \end{equation*} whenever $0 < \abs{x-y} < \delta$, for all $x,y \in A$. For each $n \in \N$ and each $y \in A$, define $f_n:A \to \R$ as
    \begin{equation*}
        f_n(y) = \frac{f(y+ \frac{\delta}{2n}) - f(y)}{(y+\frac{\delta}{2n}) - y}.
    \end{equation*} Since $f$ is continuous at $A$, each $f_n$ is also continuous at $A$. Set $N := 1$ and choose some natural $n$ such that $n \geq N$. Then,
    \begin{equation*}
        \abs{f_n(y)-f'(y)} < \epsilon,
    \end{equation*} so $f_n \to f'$ uniformly. By the Continuous Limit Theorem, $f'$ is continuous on $A$.

    \exc{6.2.5}
    
    The forward direction follows from a straightforward application of the triangle inequality. 
    
    For the converse direction, we begin by letting $x \in A$ be arbitrary with the intent to define what $f(x)$ will be, where $f: A \to \R$. By assumption, the sequence $(f_n(x))$ is Cauchy, so it converges to some $L \in \R$. Then, we define $f(x) := L$. $f$ is well defined, since every $a \in A$ is assigned a value $L \in \R$, and this value is unique since the limit of sequences is unique.
    
    To see that $f_n \to f$ uniformly, \lep[l].
    
    \exc{6.2.6}
    \begin{enumerate}
        \item $f_n(x) = x^n$ is continuous on $[0, 1]$, therefore $f_n$ is uniformly continuous on this interval. However, $f_n \to f$, which is not even continuous on $[0, 1]$.
        
        Let $f_n \to f$ uniformly on $A \subseteq \R$, and assume each $f_n$ is uniformly continuous. To argue that $f$ is uniformly continuous, \lep[l]. Use the uniform convergence of $f_n$ to $f$ to choose $N \in \N$ such that $\abs{f_N(z)-f(z)} \leq \epsilon/3$. Since each $f_n$ is uniformly continuous, we can choose $\delta > 0$ such that $\abs{f_N(x)-f_N(y)} < \epsilon/3$ for all $\abs{x-y} < \delta$. Applying the triangle inequality twice,
        \begin{equation*}
            \abs{f(x)-f(y)} \leq \abs{f(x)-f_N(x)} + \abs{f_N(x)-f_N(y)} + \abs{f_N(y)-f(y)} < \epsilon
        \end{equation*} whenever $\abs{x-y} < \delta$, so $f$ is uniformly continuous on $A$.
        
        \item $f_n(x) = n x/(1+n x^2)$ is bounded on $(0, 1)$, but $f_n \to 1/x$, which is not bounded on the same interval.
        
        Let $f_n \to f$ uniformly on a set $A \subseteq \R$ and assume each $f_n$ is bounded. Assume for contradiction that $f$ is not bounded. Since $f_n \to f$ uniformly, we can choose $N \in \N$ such that $\abs{f(x)-f_N(x)} < 1$ for all $x \in A$. By assumption, there is some $M \in \R$ such that $\abs{f_N(x)} \leq M$ for all $x \in A$, and there is some $x_0 \in A$ such that $\abs{f(x_0)} > M + 1$. Then,
        \begin{equation*}
            \abs{f(x_0)-f_N(x_0)} < 1 \implies \abs{f(x_0)} < f_N(x_0) + 1 \leq M + 1,
        \end{equation*} which contradicts the assumption that $\abs{f(x_0)} > M + 1$. Thus, $f$ must be bounded.
        
        \item For each $n \in \N$, let $f_n: \R \to \R$ be defined by
        \begin{equation*}
            f_n(x) = \begin{cases}
                x & x = \frac{1}{m} \text{ for some $m \in \N$ with $m \leq n$} \\ 0 & \text{ otherwise}.
            \end{cases}
        \end{equation*} It is easy to verify that $f_n \to f$ uniformly, were
        \begin{equation*}
            f(x) = \begin{cases}
                x & x = \frac{1}{m} \text{ for some $m \in \N$} \\ 
                0 & \text{ otherwise}.
            \end{cases}
        \end{equation*} Clearly, each $f_n$ has $n$ discontinuities, whereas $f$ has a countably infinite number of discontinuities.
        
        \item If only pointwise convergence is assumed, $f_n(x) = x^n$ defined on $[0, 1]$ is a counterexample. Each $f_n$ has fewer than one discontinuity, but they converge to a function with one discontinuity.
    \end{enumerate}
    
    \exc{6.2.7}
    
    \lep. Since $f$ is uniformly continuous on $\R$, we can choose $\delta > 0$ such that $\abs{f(x)-f(y)} < \epsilon$ whenever $\abs{x-y} < \delta$. Choose $N \in \N$ such that $N > 1/\delta$. Fix some $n \geq N$, and notice that $\abs{f_n(x)-f(x)} = \abs{f(x+1/n)-f(x)}$. Since $1/n < \delta$, it follows that $\abs{f(x+1/n)-f(x)} < \epsilon$ for all $x \in \R$, so $f_n \to f$ uniformly.
    
    To see that the result does not hold if $f$ is not uniformly continuous, let $f: \R \to \R$ be defined as $f(x) = x^2$. Assume for contradiction that there is some $N \in \N$ such that $\abs{f(x+1/n)-f(x)} < 1$ for all $n \neq N$ and all $x \in \R$. Setting $x := N/2 - 1/(2N)$, $\abs{f(x+1/N)-f(x)}=2x/N + 1/N^2 = 1 < 1$, a contradiction.
    
    \exc{6.2.8}
    
    \lep. Since $g_n \to g$ uniformly, $g$ is continuous on $K$, by the Continuous Limit Theorem. Also, since $K$ is compact and $g(x) \neq 0$ on $K$, there is some $M > 0$ such that $\abs{g(x)} > M$ for all $x \in K$. Using the fact that $g_n \to f$ uniformly, choose $N \in \N$ such that $\abs{g_n(x)-g(x)} < \min{M/2, \epsilon M^2/2}$ for all $n \geq N$ and all $x \in K$. Notice that for the appropriate $n$ and $x$, it follows that $\abs{g(x)-g_n(x)} < M/2$, which implies $\abs{g_n(x)} > M/2$. We can conclude that $\abs{g_n(x) g(x)} > M^2/2$ for all $n \geq N$ and all $x \in K$. Thus,
    \begin{equation*}
        \abs{\frac{1}{g_n(x)}-\frac{1}{g(x)}} = \frac{\abs{g_n(x)-g(x)}}{\abs{g(x) g_n(x)}} < \frac{2 \abs{g_n(x)-g(x)}}{M^2} < \epsilon,
    \end{equation*} so $1/g_n \to 1/g$ uniformly on $K$.
    
    \exc{6.2.9}
    \begin{enumerate}
        \item \lep. Choose $N_1 \in \N$ such that $\abs{f(x)-f_n(x)} < \epsilon/2$ for all $n \geq N_1$. Similarly, choose $N_2$ such that $\abs{g(x)-g_n(x)} < \epsilon/2$ for all $n \geq N_1$. Then,
        \begin{align*}
            &\abs{(f(x)+g(x)) - (f_n(x)+g_n(x))} \leq \\
            &\abs{f(x)-f_n(x)} + \abs{g(x)-g_n(x)} < \epsilon/2 + \epsilon/2 = \epsilon
        \end{align*} for all $n \geq \max{N_1, N_2}$.
        
        \item $f_n(x) = g_n(x) = x + 1/n$.
        
        \item \lep. Choose $N_1 \in \N$ such that $\abs{g_n(x)-g(x)} < \min{M, \epsilon/(2M)}$ for all $n \geq N_1$. It is not hard to verify that $\abs{g_n(x)} \leq 2M$ for all $n \geq N_1$. Next, choose $N_2$ such that $\abs{f(x)-f_n(x)} < \epsilon/(4M)$ for all $n \geq N_2$. Setting $N := \max{N_1, N_2}$,
        \begin{align*}
            &\abs{f_n(x)g_n(x)-f(x)g(x)} = \\ &\abs{f_n(x)g_n(x) - f(x)g_n(x) + f(x)g_n(x)-f(x)g(x)} \leq \\ 
            &\abs{g_n(x)} \abs{f(x)-f_n(x)} + \abs{f(x)} \abs{g(x)-g_n(x)} \leq \\ & 2M \abs{f(x)-f_n(x)}  + M\abs{g(x)-g_n(x)} < 2M \frac{\epsilon}{4M} + M \frac{\epsilon}{2M} = \epsilon
        \end{align*} for all $n \geq N$. Thus, $f_n g_n \to f g$ uniformly.
    \end{enumerate}
    
    \exc{7.2.1}
    Assume for contradiction that $L(f, P) > U(f)$ for some partition $P$. By Lemma 7.2.4, $L(f, P) \leq U(f, P')$ for every $P' \in \mathcal{P}$, so $L(f, P)$ is a lower bound for $U(f)$, which contradicts the assumption that it is greater than $U(f)$. Thus, $U(f)$ is an upper bound for $\set{L(f, P): P \in \mathcal{P}}$, so it must be greater than or equal to its supremum, namely $L(f)$.
    
    \exc{7.2.3}
    \begin{enumerate}
        \item Assume first that $f$ is integrable on $[a, b]$. Then, by Theorem 7.2.8, for each $n \in \N$ we can find a partition $P_n$ such that $0 \leq U(f, P_n) - L(f, P_n) < 1/n$. Then it is easy to see that $(U(f, P_n) - L(f, P_n)) \to 0$. To show that $\int_a^b f = \lim L(f, P_n)$, notice that for any $n$,
        \begin{equation*}
            U(f, P_n) - L(f, P_n) \geq U(f) - L(f, P_n) \geq L(f) - L(f, P_n) \geq 0.
        \end{equation*} But then for any given $\epsilon > 0$ we can find $N$ such that $\epsilon > U(f, P_n) - L(f, P_n) \geq L(f) - L(f, P_n) \geq 0$ for all $n \geq N$. This shows that $\lim \paren{L(f) - L(f, P_n)} = 0$, thus $\lim L(f, P_n) = L(f) = \int_a^b f = \lim U(f, P_n)$.
        
        \item It is not hard to show that $L(f, P_n) = \frac{1-1/n}{2}$ and $U(f, P_n) = \frac{1+1/n}{2}$. Then $\lim L(f, P_n) = \lim U(f, P_n) = 1/2$, which shows that $\int_0^1 x = \frac{1}{2}$.
    \end{enumerate}
    
    \exc{7.2.4} Since $L(g, P) = U(g, P)$, it is trivial that $(U(g, P) - L(g, P)) \to 0$, so $g$ is integrable by the sequential criterion for integrability and $\int_a^b g = U(g, P)$.
    
\end{enumerate}

I don't know a lot about this, but the functions you need for the semantics of intuitionistic logic really are more complicated. For example, they can look like
$\mathbb{N} \to \{U : U\text{ is open in } \mathbb{R}\}$, so that we assing each propositional variable an open subset of $\mathbb{R}$. Then a formula is valid if and only if 
it gets assigned to all $\mathbb{R}$ by any valuation. What is cool about this is that there is also a completeness theorem for intuitionistic 
logic under this semantics, though if I understand it correctly in this setting it is not equivalente to the statement that every 
consistent set of sentences has a model (and apparently it can be proven intuitionistically).

\end{document}